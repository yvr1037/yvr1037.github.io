<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>notes of huggingface transformer</title>
      <link href="/year/11/25/notes-of-huggingface-transformer/"/>
      <url>/year/11/25/notes-of-huggingface-transformer/</url>
      
        <content type="html"><![CDATA[<h4 id="Perface"><a href="#Perface" class="headerlink" title="Perface"></a>Perface</h4><p>HuggingFace-Transformers手册是开源公司HuggingFace开发的涵盖很多模型的框架。</p><p>Transformers(前身是称为pytorch Transformers和pytorch pretrained bert)为自然语言理解(NLU)和自然语言生成(NLG)提供了最先进的通用架构(bert,GPT-2,RoBERTTa,XLM,DistileBert,XLNet,CTRL…..),其中超过32个100多种语言的预训练模型并同时支持Tensorflow 2.0和Pytorch两大深度学习框架.</p><p>The library was designed with two strong goals in mind:</p><ul><li><p>Be as easy and fast to use as possible:</p><blockquote><ul><li>We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions, just three standard classes required to use each model: <a href="https://huggingface.co/transformers/main_classes/configuration.html">configuration</a>, <a href="https://huggingface.co/transformers/main_classes/model.html">models</a> and <a href="https://huggingface.co/transformers/main_classes/tokenizer.html">tokenizer</a>.</li><li>All of these classes can be initialized in a simple and unified way from pretrained instances by using a common <code>from_pretrained()</code> instantiation method which will take care of downloading (if needed), caching and loading the related class instance and associated data (configurations’ hyper-parameters, tokenizers’ vocabulary, and models’ weights) from a pretrained checkpoint provided on <a href="https://huggingface.co/models">Hugging Face Hub</a> or your own saved checkpoint.</li><li>On top of those three base classes, the library provides two APIs: <a href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline"><code>pipeline()</code></a> for quickly using a model (plus its associated tokenizer and configuration) on a given task and <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer()</code></a>/<a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TFTrainer"><code>TFTrainer()</code></a> to quickly train or fine-tune a given model.</li><li>As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to extend/build-upon the library, just use regular Python/PyTorch/TensorFlow/Keras modules and inherit from the base classes of the library to reuse functionalities like model loading/saving.</li></ul></blockquote></li><li><p>Provide state-of-the-art models with performances as close as possible to the original models:</p><blockquote><ul><li>We provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture.</li><li>The code is usually as close to the original code base as possible which means some PyTorch code may be not as <em>pytorchic</em> as it could be as a result of being converted TensorFlow code and vice versa.</li></ul></blockquote></li></ul><p>这是<a href="https://huggingface.co/transformers/philosophy.html%E5%AE%98%E7%BD%91%E7%BB%99%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A%EF%BC%9A">https://huggingface.co/transformers/philosophy.html官网给出的解释：</a></p><ul><li>架构<ul><li>使用每个模型都需要三个标准类:<strong>configuration</strong>,<strong>models</strong>,<strong>tokenizer</strong>.model用于指定使用的模型,例如model为bert，那么相应的网络结构是bert的网络结构；configuration是模型具体的架构配置，例如可以配置多头的数量等等,这里配置需要注意的地方就是，如果自定义配置不改变核心网络结构的则仍旧可以使用预训练模型权重，如果配置涉及到核心结构的修改，例如前馈网络的隐层神经元的个数，则无法使用预训练模型权重，这个时候transformers会默认你要重新自己预训练一个模型从而随机初始化整个模型的权重，这是是一种半灵活性的设计.</li><li>所有这些类都可以使用通用的from_pretrained()实例化方法，以简单统一的方式从受过训练的实例中初始化，该方法将负责下载（如果需要），缓存和加载相关的类实例以及相关的数据(config的的超参数，tokenizer生成器的词汇表和模型的权重)在 <a href="https://link.zhihu.com/?target=https://huggingface.co/models">Hugging Face Hub</a> 上提供的预先训练的检查点或您自己保存的检查点</li><li>在这三个基本类的基础上，该库提供了两个API：<ul><li><a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/main_classes/pipelines.html%23transformers.pipeline">pipeline()</a>用于在给定任务上快速使用模型（及其关联的tokenizer和configuration）和 </li><li>Trainer或者<a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/main_classes/trainer.html%23transformers.TFTrainer">TF</a>trainer 快速训练或微调给定模型</li></ul></li></ul></li></ul><p>因此<strong>Transformers</strong>不是神经网络构建模块化的模块工具箱。如果要扩展/构建库，只需使用常规的Python / PyTorch / TensorFlow / Keras模块并从库的基类继承即可重用模型加载/保存之类的功能。</p><p>现有的预训练模型整体上都属于下面的五个类别：</p><h5 id="Decoders-or-autoregressive-models"><a href="#Decoders-or-autoregressive-models" class="headerlink" title="Decoders or autoregressive models"></a>Decoders or autoregressive models</h5><p>自回归模型在经典语言建模任务上进行了预训练：猜测下一个已读完所有先前token的token。它们对应于transformer模型的解码器部分，并且在整个句子的顶部使用了一个掩码，以便注意头只能看到文本中的之前内容，而不能看到其后的内容。尽管可以对这些模型进行微调并在许多任务上取得出色的结果，但其最自然的应用是文本生成。此类模型的典型例子是GPT</p><h5 id="Encoders-or-autoencoding-models"><a href="#Encoders-or-autoencoding-models" class="headerlink" title="Encoders or autoencoding models"></a>Encoders or autoencoding models</h5><p>通过以某种方式破坏输入token并尝试重建原始句子来对自编码模型进行预训练。从某种意义上说，它们与transformer中的的编码器相对应，因为它们无需任何掩码即可访问完整的输入。这些模型通常建立整个句子的双向表示。可以对它们进行微调并在许多任务（例如文本生成）上取得出色的结果，但是它们最自然的应用是文本分类或token分类（比如词性标注）。此类模型的典型例子是BERT</p><p>自动回归模型和自动编码模型之间的唯一区别在于模型的预训练方式。因此，相同的体系结构既可以用于自动回归模型，也可以用于自动编码模型.</p><h5 id="Sequence-to-Sequence-models"><a href="#Sequence-to-Sequence-models" class="headerlink" title="Sequence-to-Sequence models"></a>Sequence-to-Sequence models</h5><p>序列到序列模型将transformers的编码器和解码器同时用于翻译任务或通过将其他任务转换为序列到序列问题来训练得到的。可以将它们微调来适应许多任务（这里应该是说把sequence to sequence的预训练模型的encoder或者decoder单独抽取出来，然后用法就和上面两种模型的用法一致），但最自然的应用是翻译，摘要和问题解答。T5是一个典型的例子.</p><h5 id="Multimodal-models"><a href="#Multimodal-models" class="headerlink" title="Multimodal models"></a>Multimodal models</h5><p>多模态模型将文本输入与其他类型的输入（例如图像）混合在一起，并且更特定于给定任务.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211125193439861.png" alt="image-20211125193439861"></p><p>这种模型没有提供任何预训练权重只是定义了模型的结构.</p><h5 id="Retrieval-based-models"><a href="#Retrieval-based-models" class="headerlink" title="Retrieval-based models"></a>Retrieval-based models</h5><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211125193530708.png" alt="image-20211125193530708"></p><h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>  The most basic object in the 🤗 Transformers library is the <code>pipeline()</code> function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:</p><p>  There are three main steps involved when you pass some text to a pipeline:</p><ol><li>The text is preprocessed into a format the model can understand.</li><li>The preprocessed inputs are passde to the model .</li><li>The predictions of the model are post-processed,so you can make sense of them.</li></ol><p>Some of the currently <strong>available pipelines</strong> are:</p><ul><li>feature-extraction(get the vector representation of a text)</li><li>file-mask</li><li>ner(named entity recogniton)</li><li>question-answering</li><li>sentiment-analysis</li><li>summarization</li><li>text-generation</li><li>translation</li><li>zero-shot-classification</li></ul><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><h5 id="Transformer-history"><a href="#Transformer-history" class="headerlink" title="Transformer history"></a>Transformer history</h5><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126210322407.png" alt="image-20211126210322407"></p><p>The <a href="https://arxiv.org/abs/1706.03762">Transformer architecture</a> was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:</p><ul><li><strong>June 2018</strong>: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results</li><li><strong>October 2018</strong>: <a href="https://arxiv.org/abs/1810.04805">BERT</a>, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)</li><li><strong>February 2019</strong>: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns</li><li><strong>October 2019</strong>: <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance</li><li><strong>October 2019</strong>: <a href="https://arxiv.org/abs/1910.13461">BART</a> and <a href="https://arxiv.org/abs/1910.10683">T5</a>, two large pretrained models using the same architecture as the original Transformer model (the first to do so)</li><li><strong>May 2020</strong>, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called <em>zero-shot learning</em>)</li></ul><p>This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:</p><ul><li>GPT-like (also called <em>auto-regressive</em> Transformer models)</li><li>BERT-like (also called <em>auto-encoding</em> Transformer models)</li><li>BART/T5-like (also called <em>sequence-to-sequence</em> Transformer models)</li></ul><p>We will dive into these families in more depth later on.</p><h5 id="Transformers-are-language-models"><a href="#Transformers-are-language-models" class="headerlink" title="Transformers are language models"></a>Transformers are language models</h5><p>All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as <em>language models</em>. This means they have been trained on large amounts of raw text in a self-supervised fashion. <strong>Self-supervised</strong> learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!</p><p>所有上述提到的模型都已经被训练成了对应的语言模型。这也就是说这些模型以自我监督的方式接受了大量原始文本的训练。自监督学习是一种训练类型，目标是根据模型的输入自动计算的。也就是说不需要人类手动标记数据。</p><hr><p>This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called <em><strong>transfer learning</strong></em>. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task</p><p>这种类型的模型对其所训练的语言有统计理解但对于特定的实际任务不是很有用。因此通用的预训练模型都会经历一个称为<strong>迁移学习</strong>的过程。在这个过程中，模型在给定的任务上以有监督的方式进行微调——即使用人工标注的数据标签。</p><hr><p>An example of a task is predicting the next word in a sentence having read the <em>n</em> previous words. This is called *<strong>causal language modeling</strong> because the output depends on the past and present inputs, but not the future ones.  </p><p>任务的一个实例就是预测已经阅读的前n个单词的句子的下一个单词。这也被称为<strong>因果语言建模</strong>，因为输出取决于过去和现在的输入而不是未来的输入。</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126212440273.png" alt="image-20211126212440273"></p><p>Another example is <em>masked language modeling</em>, in which the model predicts a masked word in the sentence.</p><p>另一个例子是掩码语言建模，其中模型预测句子的掩码词。</p><p><img src="C:\Users\VrShadow\Desktop\work\IS_Lab\Web\image-20211126212540879.png" alt="image-20211126212540879"></p><h5 id="Transformer-are-big-models"><a href="#Transformer-are-big-models" class="headerlink" title="Transformer are big models"></a>Transformer are big models</h5><p>Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.</p><p>除了一些特殊(如 DistilBERT)外，实现更好性能的一般策略是增加模型的大小以及预训练的数据量。</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126213252541.png" alt="image-20211126213252541"></p><p>Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126213702715.png" alt="image-20211126213702715"></p><h5 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h5><p><em>Pretraining</em> is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.</p><p>预训练是从头开始训练模型的行为：权重随机初始化,训练在没有任何先验知识的情况下开始.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126222209255.png" alt="image-20211126222209255"></p><p>This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.</p><p><em>Fine-tuning</em>, on the other hand, is the training done <strong>after</strong> a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.</p><p>微调是在模型预训练后进行的训练。要进行微调，首先需要获得一个预训练的语言模型然后使用特定于任务的数据集进行额外的训练。</p><ul><li>The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).</li><li>Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.</li><li>For the same reason, the amount of time and resources needed to get good results are much lower</li></ul><p>For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is “transferred,” hence the term <em>transfer learning</em>.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126222820143.png" alt="image-20211126222820143"></p><p>Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.</p><p>微调模型具有更低的时间,数据,经济和环境成本。迭代不同的微调方案也更快更容易，因为训练比完全预训练的约束更少。</p><p>This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it.</p><h5 id="General-Transformer-Architecture"><a href="#General-Transformer-Architecture" class="headerlink" title="General Transformer Architecture"></a>General Transformer Architecture</h5><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126223610672.png" alt="image-20211126223610672"></p><p>The transformer is based on the attention mechanism.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126223727115.png" alt="image-20211126223727115"></p><p>The combination of the two parts is known as an encoder-decoder or a sequence-to-sequence transformer.</p><p>The model is primarily composed of two blocks:</p><ul><li><strong>Encoder (left)</strong>: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.</li></ul><p>编码器接受输入并构建它的表示(其特征)，这意味着模型经过优化以从输入中获取理解。</p><ul><li><strong>Decoder (right)</strong>: The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.</li></ul><p>解码器使用编码器的表示(特征)和其他输入生成目标序列，这意味着模型针对生成输出进行优化。</p><p>Each of these parts can be used independently, depending on the task:</p><ul><li><p><strong>Encoder-only models</strong>: Good for tasks that require understanding of the input, such as <strong>sentence classification and named entity recognition.</strong></p><p>适用于<strong>需要理解输入的任务，例如句子分类和命名实体识别。</strong></p></li><li><p><strong>Decoder-only models</strong>: Good for generative tasks such as <strong>text generation</strong>.</p><p>适用于<strong>生成任务，例如文本生成</strong></p></li><li><p><strong>Encoder-decoder models</strong> or <strong>sequence-to-sequence models</strong>: Good for generative tasks that require an input, such as <strong>translation or summarization</strong>.</p><p>适用于<strong>需要输入的生成任务，例如翻译或者摘要。</strong> </p></li></ul><h5 id="Atention-layers"><a href="#Atention-layers" class="headerlink" title="Atention layers"></a>Atention layers</h5><p>A key feature of Transformer models is that they are built with special layers called <em>attention layers</em>. In fact, the title of the paper introducing the Transformer architecture was <a href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a>! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.</p><p>Transformer模型的关键点就是他们由称为注意力层的特殊层构建而成。事实上，提出Transformer架构的论文是”Attention is all your need”。后面会详细探究Attenton layer的细节。</p><p>To put this into context, consider the task of translating text from English to French. Given the input “You like this course”, a translation model will need to also attend to the adjacent word “You” to get the proper translation for the word “like”, because in French the verb “like” is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating “this” the model will also need to pay attention to the word “course”, because “this” translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of “this”. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.</p><h5 id="The-original-architecture"><a href="#The-original-architecture" class="headerlink" title="The original architecture"></a>The original architecture</h5><p>The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language</p><p>Transformer架构最初是为了翻译而设计的,在训练期间，编码器接受某种语言的输入句子，而解码器接受所需目标语言的相同句子。</p><ul><li><p>In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence).</p><p>在编码器中，注意力层可以使用句子中的所有单词(给定单词的翻译可以依赖于句子中它之后和之前的内容)</p></li><li><p>The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.</p><p>解码器是按照顺序工作，只能关注已经翻译的句子中单词。比如，当我们预测翻译目标中的前三个单词时，我们将他们提供给解码器然后解码器使用编码器的所有输入尝试预测第四个单词。</p></li></ul><p>Note that the the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.</p><p>注意，解码器的第一个注意层关注解码器所有过去的输入，但第二个注意层使用编码器的输出。因此，它可以访问整个输入句子以最好的预测当前的单词，这是很有用的因为不同的语言有不同的语法规则，把单词放在不同的顺序或者句子后面提供的上下文可能有助于确定一个给定单词的最佳翻译。</p><p>The <em>attention mask</em> can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences.</p><p><strong>attention mask</strong>也可以运用在编码/解码中，防止模型注意到某些特殊的单词</p><h5 id="Architecture-amp-amp-Checkpoints"><a href="#Architecture-amp-amp-Checkpoints" class="headerlink" title="Architecture &amp;&amp; Checkpoints"></a>Architecture &amp;&amp; Checkpoints</h5><p>As we dive into Transformer models in this course, you’ll see mentions of <em>architectures</em> and <em>checkpoints</em> as well as <em>models</em>. These terms all have slightly different meanings:</p><ul><li><strong>Architecture</strong>: This is the skeleton of the model — the definition of each layer and each operation that happens within the model.</li><li><strong>Checkpoints</strong>: These are the weights that will be loaded in a given architecture.</li><li><strong>Model</strong>: This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify <em>architecture</em> or <em>checkpoint</em> when it matters to reduce ambiguity.</li></ul><p>For example, BERT is an architecture while <code>bert-base-cased</code>, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the <code>bert-base-cased</code> model.”</p><h4 id="Encoder-models"><a href="#Encoder-models" class="headerlink" title="Encoder models"></a>Encoder models</h4><p>Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having “bi-directional” attention, and are often called <em>auto-encoding models</em>.</p><p>Encoder model只使用the transformer model的encoder部分。在每个阶段，attention layers都可以访问初始句子的所有词。这些模型同v行被描述为具有”bi-directional”，通常被称为自编码模型。</p><p>The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.</p><p>这些模型的预训练通常围绕某种方式破坏给定的句子(例如，通过屏蔽其中的随机词)，并让模型查找或重构初始句子。</p><p>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.</p><p>编码器模型最适合需要理解完整句子的任务，例如sentence classification,ner(命名实体识别)(以及更加一般的单词分类)和eqa提取式回答.</p><h5 id="Representatives-of-this-family-of-models-include"><a href="#Representatives-of-this-family-of-models-include" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul><li><a href="https://huggingface.co/transformers/model_doc/albert.html">ALBERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/bert.html">BERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/electra.html">ELECTRA</a></li><li><a href="https://huggingface.co/transformers/model_doc/roberta.html">RoBERTa</a></li></ul><h4 id="Decoder-models"><a href="#Decoder-models" class="headerlink" title="Decoder models"></a>Decoder models</h4><p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em>auto-regressive models</em>.</p><p>解码器模型仅使用Transformer模型的解码器。在每个阶段，对于给定的单词，注意力层只能访问位于句子之前的单词。这些模型通常被称为自回归模型。</p><p>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p><p>解码器模型的预训练通常围绕预测句子中的下一个单词。</p><p>These models are best suited for tasks involving text generation.</p><p>这些模型最适合设计文本生成的任务.</p><h5 id="Representatives-of-this-family-of-models-include-1"><a href="#Representatives-of-this-family-of-models-include-1" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul><li><a href="https://huggingface.co/transformers/model_doc/ctrl.html">CTRL</a></li><li><a href="https://huggingface.co/transformers/model_doc/gpt.html">GPT</a></li><li><a href="https://huggingface.co/transformers/model_doc/gpt2.html">GPT-2</a></li><li><a href="https://huggingface.co/transformers/model_doc/transformerxl.html">Transformer XL</a></li></ul><h4 id="Seq-to-Seq-models"><a href="#Seq-to-Seq-models" class="headerlink" title="Seq-to-Seq models"></a>Seq-to-Seq models</h4><p>Encoder-decoder models (also called <em>sequence-to-sequence models</em>) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.</p><p>encoder-decoder models(也称为SeqtoSeq models)使用Transformer体系结构的所有部分。在每个阶段，编码器的注意力机制可以访问初始句子的每个单词,而解码器的注意力层只能访问输入中某个单词前面的单词。</p><p>The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, <a href="https://huggingface.co/t5-base">T5</a> is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.</p><p>这些模型的预训练可以使用编码器/解码器模型的目标来完成,但通常涉及一些更复杂的东西。例如，T5是通过一个掩码特殊词取代随机文本跨度(可以包含多个单词)进行预训练的，然后目标是预测这个掩码词取代的文本。</p><p>Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.</p><p>Seq-to-Seq模型最适合根据给定的输入生成新句子的任务，比如摘要，翻译或者生成式问题回答。</p><h5 id="Representatives-of-this-family-of-models-include-2"><a href="#Representatives-of-this-family-of-models-include-2" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul><li><a href="https://huggingface.co/transformers/model_doc/bart.html">BART</a></li><li><a href="https://huggingface.co/transformers/model_doc/mbart.html">mBART</a></li><li><a href="https://huggingface.co/transformers/model_doc/marian.html">Marian</a></li><li><a href="https://huggingface.co/transformers/model_doc/t5.html">T5</a></li></ul><h4 id="Bias-and-limitations"><a href="#Bias-and-limitations" class="headerlink" title="Bias and limitations"></a>Bias and limitations</h4><p>If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.</p><p>如果打算在production中使用一个预先训练的模型或者经过微调的版本，请注意，尽管这些模型是最强大的工具但是他们也有局限性。其中最大的问题是是为了能够对大量的数据进行训练，研究人员经常搜集他们能够找到的所有内容，并且从互联网上可获得的信息中挑选出最好的和最差的。</p><p>Example：<strong>pipeline:fill-mask model:bert-base-uncased</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipelineunmasker <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"fill-mask"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">)</span>result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">"This man works as a [MASK]."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">"token_str"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">"This woman works as a [MASK]."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">"token_str"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">Some weights of the model checkpoint at bert<span class="token operator">-</span>base<span class="token operator">-</span>uncased were <span class="token operator">not</span> used when initializing BertForMaskedLM<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'cls.seq_relationship.bias'</span><span class="token punctuation">,</span> <span class="token string">'cls.seq_relationship.weight'</span><span class="token punctuation">]</span><span class="token operator">-</span> This IS expected <span class="token keyword">if</span> you are initializing BertForMaskedLM <span class="token keyword">from</span> the checkpoint of a model trained on another task <span class="token operator">or</span> <span class="token keyword">with</span> another architecture <span class="token punctuation">(</span>e<span class="token punctuation">.</span>g<span class="token punctuation">.</span> initializing a BertForSequenceClassification model <span class="token keyword">from</span> a BertForPreTraining model<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token operator">-</span> This IS NOT expected <span class="token keyword">if</span> you are initializing BertForMaskedLM <span class="token keyword">from</span> the checkpoint of a model that you expect to be exactly identical <span class="token punctuation">(</span>initializing a BertForSequenceClassification model <span class="token keyword">from</span> a BertForSequenceClassification model<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">[</span><span class="token string">'carpenter'</span><span class="token punctuation">,</span> <span class="token string">'lawyer'</span><span class="token punctuation">,</span> <span class="token string">'farmer'</span><span class="token punctuation">,</span> <span class="token string">'businessman'</span><span class="token punctuation">,</span> <span class="token string">'doctor'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'nurse'</span><span class="token punctuation">,</span> <span class="token string">'maid'</span><span class="token punctuation">,</span> <span class="token string">'teacher'</span><span class="token punctuation">,</span> <span class="token string">'waitress'</span><span class="token punctuation">,</span> <span class="token string">'prostitute'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender — and yes, prostitute ended up in the top 5 possibilities the model associates with “woman” and “work.” This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it’s trained on the <a href="https://huggingface.co/datasets/wikipedia">English Wikipedia</a> and <a href="https://huggingface.co/datasets/bookcorpus">BookCorpus</a> datasets).</p><p>可以通过例子发现,当需要填写这两句话中被屏蔽的单词时,模型只给出了一个不分性别的答案并按照‘man’和‘work’相关联，‘woman’和‘work’相关联可能性最大的前5种可能性中。</p><p>When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won’t make this intrinsic bias disappear.</p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>In this chapter, you saw how to approach different NLP tasks using the high-level <code>pipeline()</code> function from 🤗 Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser.</p><p>We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:</p><p>关键点在于，可以使用完整的transformer架构也可以使用编码器和解码器，具体却决于你要解决的task特点，下表进行简单总结：</p><table><thead><tr><th>Model</th><th>Examples</th><th>Tasks</th></tr></thead><tbody><tr><td>Encoder</td><td>ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa</td><td>Sentence classification, named entity recognition, extractive question answering</td></tr><tr><td>Decoder</td><td>CTRL, GPT, GPT-2, Transformer XL</td><td>Text generation</td></tr><tr><td>Encoder-decoder</td><td>BART, T5, Marian, mBART</td><td>Summarization, translation, generative question answering</td></tr></tbody></table><h4 id="Using-Transformers"><a href="#Using-Transformers" class="headerlink" title="Using Transformers"></a>Using Transformers</h4><ul><li><strong>Ease of use</strong>: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.</li><li><strong>Flexibility</strong>: At their core, all models are simple PyTorch <code>nn.Module</code> or TensorFlow <code>tf.keras.Model</code> classes and can be handled like any other models in their respective machine learning (ML) frameworks.</li><li><strong>Simplicity</strong>: Hardly any abstractions are made across the library. The “All in one file” is a core concept: a model’s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.</li></ul>]]></content>
      
      
      <categories>
          
          <category> dialogue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dialogue_system</title>
      <link href="/year/11/21/Dialogue-system/"/>
      <url>/year/11/21/Dialogue-system/</url>
      
        <content type="html"><![CDATA[<p>NLP领域比较传统和核心的task有很多</p><p>下面先介绍Chinese NLP的基本任务:</p><h4 id="Co-reference-Resolution"><a href="#Co-reference-Resolution" class="headerlink" title="Co-reference Resolution"></a>Co-reference Resolution</h4><p>Background</p><hr><p>​    Co-reference identifies pieces of text and links them with other pieces of text that refer to the same thing. Sometimes pieces of text have zero-length, where an overt pronoun or noun is omitted.</p><p>Example</p><hr><p>input:</p><pre><code>我的姐姐给我她的狗。很喜欢.</code></pre><p>output</p><pre><code>[我]0的[姐姐]1给[我]0[她]1的[狗]2。[]0很喜欢[]2.</code></pre><h6 id="Standard-Metrics"><a href="#Standard-Metrics" class="headerlink" title="Standard Metrics"></a>Standard Metrics</h6><p>Average of F1-scores returned by these three precison/recall metrics:</p><ul><li>MUC</li><li>B-cubed</li><li>Entity-based CEAF</li><li>BLANC</li><li>Link-Based Entity-Aware metric(LEA)</li></ul><h4 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a>Sentiment Analysis</h4><p>Background</p><hr><p>Sentiment Analysis detects identifies and extracts subjective information from text.<br>情感分析检测识别并从文本中提取主观信息.</p><hr><p>Example</p><hr><p>inputs:</p><pre><code>总的感觉这台机器还不错，实用的有：阴阳历显示，时间与日期快速转换, 记事本等。</code></pre><p>Output:</p><pre><code>Positive</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN</title>
      <link href="/year/11/06/Faster%20R-CNN/"/>
      <url>/year/11/06/Faster%20R-CNN/</url>
      
        <content type="html"><![CDATA[<h5 id="Perface"><a href="#Perface" class="headerlink" title="Perface:"></a>Perface:</h5><p>在🦌同学的感染下，笔者最近也学习了目标检测方向的相关内容，看的第一篇论文是<a href="https://arxiv.org/abs/1504.08083#">Faster R-CNN：Towards Rel-Time Objection Dection with Region Proposal Networks</a>，里面涉及到很多前置模型需要了解结构，在这里分享一点笔记</p><h5 id="目标检测背景"><a href="#目标检测背景" class="headerlink" title="目标检测背景"></a>目标检测背景</h5><p>目标检测是很多计算机视觉人物的基础，目前主流的目标检测的算法主要基于深度学习模型可以分为两大类</p><ol><li>one-stage检测算法,这种算法直接产生物体的类别概率和坐标位置,不需要直接产生候选区域.比如说YOLO和SSD</li><li>two-stage检测算法,这是将检测问题划分为两个阶段,首先是产生候选区域,然后对候选区域分类;典型算法是R-CNN系列,faster rcnn就是基于<strong>region proposal</strong>(候选区域)</li></ol><h5 id="backbone-network"><a href="#backbone-network" class="headerlink" title="backbone network"></a>backbone network</h5><p><strong>Faster R-CNN</strong>使用的主干网络是VGG-16,在论文中称主干网络时<strong>backbone network</strong>,主干网络就是用来<strong>feature extraction</strong>,当然这个不是一成不变的,可以替换,比如现在也同样流行使用<strong>Resnet</strong>,再如<strong>CornerNet</strong>算法中使用的backbone network是Hourglass Network.<br>关于VGG-16可以参考<a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/vgg.html">VGG介绍</a>,16的含义是含有参数有16层,分别是13个卷积层+3个全连接层</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027195132710.png" alt="image-20211027195132710" style="zoom:65%;"><p>图来自网络</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027195339689.png" alt="image-20211027195339689" style="zoom:60%;"><h5 id="Faster-R-CNN算法步骤"><a href="#Faster-R-CNN算法步骤" class="headerlink" title="Faster R-CNN算法步骤"></a>Faster R-CNN算法步骤</h5><p>这部分是为了理解Faster R-CNN,总体描述下算法的整个过程以便后期做细节分析</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027195754031.png" alt="image-20211027195754031" style="zoom:50%;"><p>大致流程是:将整张图片输入CNN层,得到feature map,卷积特征输入到**RPN(Region Proposal Network)**得到候选框的特征信息,对候选框中提取的特征使用分类器判别是否属于一个特定类别,对于属于某一特征的候选框用回归器进一步调整其位置.</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027200749657.png" alt="image-20211027200749657" style="zoom:50%;"><p>Faster R-CNN可以看作RPN和Fast R-CNN模型的结合,即Faster R-CNN = RPN + Fast R-CNN.下面介绍每一步骤的输入输出的细节.</p><ul><li>首先通过预训练模型训练得到Conv layers(这个conv layer实际上就是VGG-16)能够接收整张图片并提取特征图feature maps,这个feature map是在conv层之后获得的特征.</li><li>feature map被共享之后用于后续的RPN和Rol池化层<ul><li>BPN层:BPN网络用于生成region proposals.该层通过softmax判断anchors属于前景(foreground)还是背景(background),再利用边框回归修正anchors,获得精确的proposals </li><li>RoI Pooling层:该层收集输入的feature map和proposals综合这些信息提取proposal feature map,进入到后面可利用全连接操作层进行目标识别和定位</li></ul></li><li>最后的classifier会将Roi Pooling层形成固定大小的feature map进行全连接操作,利用softmax进行具体类别的分类,同时利用L1 loss完成bounding box regression回归操作获得物体的准确位置</li></ul><h5 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h5><h6 id="1-RPN"><a href="#1-RPN" class="headerlink" title="1.RPN"></a>1.RPN</h6><p>之前的R-CNN和Fast R-CNN都是采用可选择性搜索(SS)来产生候选框的,但是这种方法特别耗时;Faster R-CNN最大的亮点是抛弃SS,采用RPN生成候选框.<br><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027204057233.png" alt="image-20211027204057233" style="zoom:67%;"></p><p>说明:</p><ol><li>Conv feature map:VGG-16网络最后一个卷积层输出的feature map</li><li>Sliding window:滑动窗口实际上就是3*3的卷积核,滑窗只要选取所有可能的区域并没有额外的作用</li><li>K anchor boxes:在每个sliding window的点上初始化的参考区域(论文中k=9)就是9个矩形框</li><li>Intermediate layer:中间层，256-d是中间层的维度(论文中谁用ZF网络就是256维,VGG就是512维)</li><li>Cls layer:分类层,预测proposal的anchor对应的proposal的(x,y,w,h)</li><li>2k scores:2k个分数(18个)</li><li>Reg layer:回归层,判断该proposal是前景还是背景</li><li>4k coordinates:4k坐标(36个)</li></ol><ul><li>RPN的输入是卷积特征图,输出是图片生成的proposals,RPN通过一个滑动窗口连接在最后一个卷积层的feature map上,生成一个长度256的全连接特征</li><li>这个全连接层特征分别送入两个全连接层一个是分类层,用于分类检测;一个是回归层,用于回归;对于每个滑动窗口位置一般设置k(论文中k=9)个不同大小或者比例的anchors这意味着每个滑窗覆盖的位置就会预测9哥候选区域<br><strong>分类层</strong>:每个anchor输出两个预测值:anchor是背景(background,非object)的score和anchor是前景(foreground,object)的score<br><strong>回归层</strong>:输出4k(4*9=36)个坐标值表示每个候选区域的位置(x,y,w,h)</li></ul><p>也就是说我么是通过这些特征图应用滑动窗口加anchor机制进行目标区域判定和分类的,这里的滑窗加anchor机制功能类似于fast rcnn的selective search生成proposals的作用,而我们是通过RPN生成proposals.RPN就是一个卷积层 + relu +左右两个层(cls layer和reg layer)的小型网络</p><h6 id="2-anchor"><a href="#2-anchor" class="headerlink" title="2.anchor"></a>2.anchor</h6><p>论文内容:The k proposals are parameterized relative to k reference boxes, which we call anchors;可以理解为锚点位于之前说的3 * 3的滑窗中心处,就是因为有多个anchor.这9个anchor是作者设置的,论文中scale=[128,256,512],长宽比[1:1,1:2,2:1]有9种；自己可以根据目标的特点做出不同的设计;对于一幅 w * h的feature map一共有w * h * k个锚点.</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027213420072.png" alt="image-20211027213420072" style="zoom:50%;"><h6 id="3-VGG提取特征"><a href="#3-VGG提取特征" class="headerlink" title="3.VGG提取特征"></a>3.VGG提取特征</h6><p>VGG的网络流程图:</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027213725540.png" alt="image-20211027213725540" style="zoom:67%;"><p>每个卷积层利用前面网络信息生成抽象描述:<br>第一层学习边缘edges信息；<br>第二层:学习边缘edges中图案patterns以学习更加复杂的形状信息；最终得到卷积特征图其空间维度(分辨率)比原图小了很多但更深；<br>特征图的width和height由于卷积层间的池化层而降低,而depth由于卷积层学习的filters数量而增加.</p><h6 id="4-ROI-pooling"><a href="#4-ROI-pooling" class="headerlink" title="4.ROI pooling"></a>4.ROI pooling</h6><p>ROI就是region of interest指的是感兴趣区域;如果是原图，roi就是目标，如果是featuremap，roi就是特征图像目标的特征了，roi在这里就是经过RPN网络得到的，总之就是一个框。pooling就是池化。所以ROI Pooling就是Pooling的一种，只是是针对于Rois的pooling操作而已。RPN 处理后，可以得到一堆没有 class score 的 object proposals.待处理问题为：如何利用这些proposals分类.Roi pooling层的过程就是为了将不同输入尺寸的feature map（ROI）抠出来，然后resize到统一的大小.</p><p>ROI pooling层的输入:</p><ol><li>特征图features map(这个特征图就是cnn卷积出来以后用于共享的那个特征图)</li><li>roi信息:(就是RPN网络的输出,一个表示所有ROI的N*5矩阵,N表示ROI的数目;第一列表示图像index,其余四列表示其余的左上角和右下角坐标,坐标信息是对应原图中的绝对坐标)</li></ol><p>ROI pooling层的过程:</p><p>首先将RPN中得到的原图中roi信息映射到feature map上按原图与featuremap的比例缩小roi坐标就行了），然后经过最大池化，池化到固定大小w×h。但这个pooling不是一般的Pooling，而是将区域等分，然后取每一小块的最大值，最后才能得到固定尺寸的roi。</p><p>也就是：</p><p>根据输入的image，将Roi映射到feature map对应的位置；<br>将映射后的区域划分为相同大小的sections（sections数量和输出的维度相同）；<br>对每个section进行max pooling操作；<br>ROI pooling层的输出：</p><p>结果是，由一组大小各异的矩形，我们快速获取到具有固定大小的相应特征图。值得注意的是，RoI pooling 输出的维度实际上并不取决于输入特征图的大小，也不取决于区域提案的大小。这完全取决于我们将区域分成几部分。也就是，batch个roi矩阵，每一个roi矩阵为：通道数xWxH,也就是从selective search得到batch个roi，然后映射为固定大小。</p><h6 id="5-NMS"><a href="#5-NMS" class="headerlink" title="5.NMS"></a>5.NMS</h6><p>NMS（Non Maximum Suppression，非极大值抑制）用于后期的物体冗余边界框去除，因为目标检测最终一个目标只需要一个框，所以要把多余的框干掉，留下最准确的那个。</p><p>NMS的输入：</p><p>检测到的Boxes(同一个物体可能被检测到很多Boxes，每个box均有分类score)</p><p>NMS的输出：</p><p>最优的Box.</p><h6 id="6-FC-layer"><a href="#6-FC-layer" class="headerlink" title="6.FC layer"></a>6.FC layer</h6><p>经过roi pooling层之后，batch_size=300, proposal feature map的大小是7×7,512-d,对特征图进行全连接，参照下图，最后同样利用Softmax Loss和L1 Loss完成分类和定位。</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027220232527.png" alt="image-20211027220232527"></p><p>通过全连接层与softmax计算每个region proposal具体属于哪个类别（如人，马，车等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个region proposal的位置偏移量bbox_pred，用于回归获得更加精确的目标检测框</p><p>即从PoI Pooling获取到7x7大小的proposal feature maps后，通过全连接主要做了：</p><p>通过全连接和softmax对region proposals进行具体类别的分类；</p><p>再次对region proposals进行bounding box regression，获取更高精度的rectangle box。</p><h5 id="主要部分"><a href="#主要部分" class="headerlink" title="主要部分"></a>主要部分</h5><p><strong>Faster</strong> <strong>RCNN</strong>其实可以分为四部分主要内容</p><h6 id="1-Conv-Layer"><a href="#1-Conv-Layer" class="headerlink" title="1.Conv Layer"></a>1.Conv Layer</h6><p>作为一种CNN目标检测方法,Faster RCNN首先使用一组基础的cnn+relu+pooling层提取image的feature map,这个feature map被共享用用于后续RPN层和全连接层</p><h6 id="2-Region-Proposal-NetWorks"><a href="#2-Region-Proposal-NetWorks" class="headerlink" title="2.Region Proposal NetWorks"></a>2.Region Proposal NetWorks</h6><p>RPN网络用于生成region proposals,该层通过softmax判断anchors属于positive还是negative,再利用bounding</p><p>box regression修正anchors获得精确的proposals</p><h6 id="3-Roi-Pooling"><a href="#3-Roi-Pooling" class="headerlink" title="3.Roi Pooling"></a>3.Roi Pooling</h6><p>该层手机输入的feature map和proposals,综合这些信息之后提取proposals,综合这些信息提取proposals feature maps送入后续全连接层判定目标类别</p><h6 id="4-Classfication"><a href="#4-Classfication" class="headerlink" title="4.Classfication"></a>4.Classfication</h6><p>利用proposals feature map计算proposals的类别同时再次bounding box regression获得检测框最终的精确位置</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211028212022996.png" alt="image-20211028212022996" style="zoom:67%;"><p>上图展示了python版本中的VGG16模型中的faster rcnn的网络结构可以清晰的看到该网络对于一幅任意大小的P*Q的图像:</p><ul><li>首先固定至大小M×N然后将M×N图像送入网络;</li><li>而Conv layer</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> RCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dian2021夏令营</title>
      <link href="/year/11/06/002/"/>
      <url>/year/11/06/002/</url>
      
        <content type="html"><![CDATA[<p>报名参加夏令营起初是想花时间研究AI机器学习领域的经典算法，然后做了dian的一个<strong>lab</strong></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dian </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> CNN简易框架搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SeqToSeq_Translation(Attention)</title>
      <link href="/year/09/27/seq2seq_translation_tutorial/"/>
      <url>/year/09/27/seq2seq_translation_tutorial/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</p><hr><p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p><p>This is the third and final tutorial on doing “NLP From Scratch”, where we<br>write our own classes and functions to preprocess the data to do our NLP<br>modeling tasks. We hope after you complete this tutorial that you’ll proceed to<br>learn how <code>torchtext</code> can handle much of this preprocessing for you in the<br>three tutorials immediately following this one.</p><p>In this project we will be teaching a neural network to translate from<br>French to English.</p><p>::</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">[</span><span class="token key atrule">KEY</span><span class="token punctuation">:</span> <span class="token punctuation">></span> input<span class="token punctuation">,</span> = target<span class="token punctuation">,</span> &lt; output<span class="token punctuation">]</span><span class="token punctuation">></span> il est en train de peindre un tableau .= he is painting a picture .&lt; he is painting a picture .<span class="token punctuation">></span> pourquoi ne pas essayer ce vin delicieux <span class="token punctuation">?</span>= why not try that delicious wine <span class="token punctuation">?</span>&lt; why not try that delicious wine <span class="token punctuation">?</span><span class="token punctuation">></span> elle n est pas poete mais romanciere .= she is not a poet but a novelist .&lt; she not not a poet but a novelist .<span class="token punctuation">></span> vous etes trop maigre .= you re too skinny .&lt; you re all alone .<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>… to varying degrees of success.</p><p>This is made possible by the simple but powerful idea of the <code>sequence to sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code>__, in which two<br>recurrent neural networks work together to transform one sequence to<br>another. An encoder network condenses an input sequence into a vector,<br>and a decoder network unfolds that vector into a new sequence.</p><p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p><p>To improve upon this model we’ll use an <code>attention mechanism &lt;https://arxiv.org/abs/1409.0473&gt;</code>__, which lets the decoder<br>learn to focus over a specific range of the input sequence.</p><p><strong>Recommended Reading:</strong></p><p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p><ul><li> <a href="https://pytorch.org/">https://pytorch.org/</a> For installation instructions</li><li> :doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li><li> :doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li><li> :doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li></ul><p>It would also be useful to know about Sequence to Sequence networks and<br>how they work:</p><ul><li><code>Learning Phrase Representations using RNN Encoder-Decoder for  Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li><li><code>Sequence to Sequence Learning with Neural  Networks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li><li><code>Neural Machine Translation by Jointly Learning to Align and  Translate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li><li> <code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li></ul><p>You will also find the previous tutorials on<br>:doc:<code>/intermediate/char_rnn_classification_tutorial</code><br>and :doc:<code>/intermediate/char_rnn_generation_tutorial</code><br>helpful as those concepts are very similar to the Encoder and Decoder<br>models, respectively.</p><p><strong>Requirements</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> unicode_literals<span class="token punctuation">,</span> print_function<span class="token punctuation">,</span> division<span class="token keyword">from</span> io <span class="token keyword">import</span> open<span class="token keyword">import</span> unicodedata<span class="token keyword">import</span> string<span class="token keyword">import</span> re<span class="token keyword">import</span> random<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> Fdevice <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Loading-data-files"><a href="#Loading-data-files" class="headerlink" title="Loading data files"></a>Loading data files</h1><p>The data for this project is a set of many thousands of English to<br>French translation pairs.</p><p><code>This question on Open Data Stack Exchange &lt;https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages&gt;</code>__<br>pointed me to the open translation site <a href="https://tatoeba.org/">https://tatoeba.org/</a> which has<br>downloads available at <a href="https://tatoeba.org/eng/downloads">https://tatoeba.org/eng/downloads</a> - and better<br>yet, someone did the extra work of splitting language pairs into<br>individual text files here: <a href="https://www.manythings.org/anki/">https://www.manythings.org/anki/</a></p><p>The English to French pairs are too big to include in the repo, so<br>download to <code>data/eng-fra.txt</code> before continuing. The file is a tab<br>separated list of translation pairs:</p><p>::</p><pre><code>I am cold.    J'ai froid.</code></pre><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p><p>Similar to the character encoding used in the character-level RNN<br>tutorials, we will be representing each word in a language as a one-hot<br>vector, or giant vector of zeros except for a single one (at the index<br>of the word). Compared to the dozens of characters that might exist in a<br>language, there are many many more words, so the encoding vector is much<br>larger. We will however cheat a bit and trim the data to only use a few<br>thousand words per language.</p><p>.. figure:: /_static/img/seq-seq-images/word-encoding.png<br>   :alt:</p><p>We’ll need a unique index per word to use as the inputs and targets of<br>the networks later. To keep track of all this we will use a helper class<br>called <code>Lang</code> which has word → index (<code>word2index</code>) and index → word<br>(<code>index2word</code>) dictionaries, as well as a count of each word<br><code>word2count</code> which will be used to replace rare words later.</p><pre class="line-numbers language-python"><code class="language-python">SOS_token <span class="token operator">=</span> <span class="token number">0</span>EOS_token <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">class</span> <span class="token class-name">Lang</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>name <span class="token operator">=</span> name        self<span class="token punctuation">.</span>word2index <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>word2count <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>index2word <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"SOS"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"EOS"</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>n_words <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment" spellcheck="true"># Count SOS and EOS</span>    <span class="token keyword">def</span> <span class="token function">addSentence</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>addWord<span class="token punctuation">(</span>word<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">addWord</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>word2index<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>word2index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>n_words            self<span class="token punctuation">.</span>word2count<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            self<span class="token punctuation">.</span>index2word<span class="token punctuation">[</span>self<span class="token punctuation">.</span>n_words<span class="token punctuation">]</span> <span class="token operator">=</span> word            self<span class="token punctuation">.</span>n_words <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>word2count<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The files are all in Unicode, to simplify we will turn Unicode<br>characters to ASCII, make everything lowercase, and trim most<br>punctuation.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Turn a Unicode string to plain ASCII, thanks to</span><span class="token comment" spellcheck="true"># https://stackoverflow.com/a/518232/2809427</span><span class="token keyword">def</span> <span class="token function">unicodeToAscii</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>        c <span class="token keyword">for</span> c <span class="token keyword">in</span> unicodedata<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span><span class="token string">'NFD'</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>        <span class="token keyword">if</span> unicodedata<span class="token punctuation">.</span>category<span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token string">'Mn'</span>    <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Lowercase, trim, and remove non-letter characters</span><span class="token keyword">def</span> <span class="token function">normalizeString</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>    s <span class="token operator">=</span> unicodeToAscii<span class="token punctuation">(</span>s<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    s <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">"([.!?])"</span><span class="token punctuation">,</span> r<span class="token string">" \1"</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>    s <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">"[^a-zA-Z.!?]+"</span><span class="token punctuation">,</span> r<span class="token string">" "</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>    <span class="token keyword">return</span> s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>To read the data file we will split the file into lines, and then split<br>lines into pairs. The files are all English → Other Language, so if we<br>want to translate from Other Language → English I added the <code>reverse</code><br>flag to reverse the pairs.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">readLangs</span><span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Reading lines..."</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Read the file and split into lines</span>    lines <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'data/%s-%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">)</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\   <span class="token operator">//</span> 相应数据集下载以后注意相对路径的设置        read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Split every line into pairs and normalize</span>    pairs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>normalizeString<span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> l<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Reverse pairs, make Lang instances</span>    <span class="token keyword">if</span> reverse<span class="token punctuation">:</span>        pairs <span class="token operator">=</span> <span class="token punctuation">[</span>list<span class="token punctuation">(</span>reversed<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> pairs<span class="token punctuation">]</span>        input_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang2<span class="token punctuation">)</span>        output_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang1<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        input_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang1<span class="token punctuation">)</span>        output_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang2<span class="token punctuation">)</span>    <span class="token keyword">return</span> input_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Since there are a <em>lot</em> of example sentences and we want to train<br>something quickly, we’ll trim the data set to only relatively short and<br>simple sentences. Here the maximum length is 10 words (that includes<br>ending punctuation) and we’re filtering to sentences that translate to<br>the form “I am” or “He is” etc. (accounting for apostrophes replaced<br>earlier).</p><pre class="line-numbers language-python"><code class="language-python">MAX_LENGTH <span class="token operator">=</span> <span class="token number">10</span>eng_prefixes <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"i am "</span><span class="token punctuation">,</span> <span class="token string">"i m "</span><span class="token punctuation">,</span>    <span class="token string">"he is"</span><span class="token punctuation">,</span> <span class="token string">"he s "</span><span class="token punctuation">,</span>    <span class="token string">"she is"</span><span class="token punctuation">,</span> <span class="token string">"she s "</span><span class="token punctuation">,</span>    <span class="token string">"you are"</span><span class="token punctuation">,</span> <span class="token string">"you re "</span><span class="token punctuation">,</span>    <span class="token string">"we are"</span><span class="token punctuation">,</span> <span class="token string">"we re "</span><span class="token punctuation">,</span>    <span class="token string">"they are"</span><span class="token punctuation">,</span> <span class="token string">"they re "</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">filterPair</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>p<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> MAX_LENGTH <span class="token operator">and</span> \        len<span class="token punctuation">(</span>p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> MAX_LENGTH <span class="token operator">and</span> \        p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>startswith<span class="token punctuation">(</span>eng_prefixes<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">filterPairs</span><span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>pair <span class="token keyword">for</span> pair <span class="token keyword">in</span> pairs <span class="token keyword">if</span> filterPair<span class="token punctuation">(</span>pair<span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The full process for preparing the data is:</p><ul><li> Read text file and split into lines, split lines into pairs</li><li> Normalize text, filter by length and content</li><li> Make word lists from sentences in pairs</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">prepareData</span><span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairs <span class="token operator">=</span> readLangs<span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">,</span> reverse<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Read %s sentence pairs"</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span>    pairs <span class="token operator">=</span> filterPairs<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Trimmed to %s sentence pairs"</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Counting words..."</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> pair <span class="token keyword">in</span> pairs<span class="token punctuation">:</span>        input_lang<span class="token punctuation">.</span>addSentence<span class="token punctuation">(</span>pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output_lang<span class="token punctuation">.</span>addSentence<span class="token punctuation">(</span>pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Counted words:"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>input_lang<span class="token punctuation">.</span>name<span class="token punctuation">,</span> input_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>output_lang<span class="token punctuation">.</span>name<span class="token punctuation">,</span> output_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">)</span>    <span class="token keyword">return</span> input_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairsinput_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairs <span class="token operator">=</span> prepareData<span class="token punctuation">(</span><span class="token string">'eng'</span><span class="token punctuation">,</span> <span class="token string">'fra'</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Reading lines...Read 135842 sentence pairsTrimmed to 10599 sentence pairsCounting words...Counted words:fra 4345eng 2803['je suis lessive et fatigue .', 'i m broke and tired .']</code></pre><h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>A Recurrent Neural Network, or RNN, is a network that operates on a<br>sequence and uses its own output as input for subsequent steps.</p><p>A <code>Sequence to Sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code><strong>, or<br>seq2seq network, or <code>Encoder Decoder network &lt;https://arxiv.org/pdf/1406.1078v3.pdf&gt;</code></strong>, is a model<br>consisting of two RNNs called the encoder and decoder. The encoder reads<br>an input sequence and outputs a single vector, and the decoder reads<br>that vector to produce an output sequence.</p><p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p><p>Unlike sequence prediction with a single RNN, where every input<br>corresponds to an output, the seq2seq model frees us from sequence<br>length and order, which makes it ideal for translation between two<br>languages.</p><p>Consider the sentence “Je ne suis pas le chat noir” → “I am not the<br>black cat”. Most of the words in the input sentence have a direct<br>translation in the output sentence, but are in slightly different<br>orders, e.g. “chat noir” and “black cat”. Because of the “ne/pas”<br>construction there is also one more word in the input sentence. It would<br>be difficult to produce a correct translation directly from the sequence<br>of input words.</p><p>With a seq2seq model the encoder creates a single vector which, in the<br>ideal case, encodes the “meaning” of the input sequence into a single<br>vector — a single point in some N dimensional space of sentences.</p><h2 id="The-Encoder"><a href="#The-Encoder" class="headerlink" title="The Encoder"></a>The Encoder</h2><p>The encoder of a seq2seq network is a RNN that outputs some value for<br>every word from the input sentence. For every input word the encoder<br>outputs a vector and a hidden state, and uses the hidden state for the<br>next input word.</p><p>.. figure:: /_static/img/seq-seq-images/encoder-network.png<br>   :alt:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>EncoderRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> embedded        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>output<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="The-Decoder"><a href="#The-Decoder" class="headerlink" title="The Decoder"></a>The Decoder</h2><p>The decoder is another RNN that takes the encoder output vector(s) and<br>outputs a sequence of words to create the translation.</p><p>Simple Decoder<br>^^^^^^^^^^^^^^</p><p>In the simplest seq2seq decoder we use only last output of the encoder.<br>This last output is sometimes called the <em>context vector</em> as it encodes<br>context from the entire sequence. This context vector is used as the<br>initial hidden state of the decoder.</p><p>At every step of decoding, the decoder is given an input token and<br>hidden state. The initial input token is the start-of-string <code>&lt;SOS&gt;</code><br>token, and the first hidden state is the context vector (the encoder’s<br>last hidden state).</p><p>.. figure:: /_static/img/seq-seq-images/decoder-network.png<br>   :alt:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>DecoderRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>output_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>output<span class="token punctuation">)</span>        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>output<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>I encourage you to train and observe the results of this model, but to<br>save space we’ll be going straight for the gold and introducing the<br>Attention Mechanism.</p><p>Attention Decoder<br>^^^^^^^^^^^^^^^^^</p><p>If only the context vector is passed between the encoder and decoder,<br>that single vector carries the burden of encoding the entire sentence.</p><p>Attention allows the decoder network to “focus” on a different part of<br>the encoder’s outputs for every step of the decoder’s own outputs. First<br>we calculate a set of <em>attention weights</em>. These will be multiplied by<br>the encoder output vectors to create a weighted combination. The result<br>(called <code>attn_applied</code> in the code) should contain information about<br>that specific part of the input sequence, and thus help the decoder<br>choose the right output words.</p><p>.. figure:: <a href="https://i.imgur.com/1152PYf.png">https://i.imgur.com/1152PYf.png</a><br>   :alt:</p><p>Calculating the attention weights is done with another feed-forward<br>layer <code>attn</code>, using the decoder’s input and hidden state as inputs.<br>Because there are sentences of all sizes in the training data, to<br>actually create and train this layer we have to choose a maximum<br>sentence length (input length, for encoder outputs) that it can apply<br>to. Sentences of the maximum length will use all the attention weights,<br>while shorter sentences will only use the first few.</p><p>.. figure:: /_static/img/seq-seq-images/attention-decoder-network.png<br>   :alt:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AttnDecoderRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> dropout_p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>AttnDecoderRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>output_size <span class="token operator">=</span> output_size        self<span class="token punctuation">.</span>dropout_p <span class="token operator">=</span> dropout_p        self<span class="token punctuation">.</span>max_length <span class="token operator">=</span> max_length        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>attn_combine <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout_p<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>embedded<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        attn_applied <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>attn_weights<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                 encoder_outputs<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>embedded<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> attn_applied<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_combine<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>output<span class="token punctuation">)</span>        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>output<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>        output <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> attn_weights    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="alert alert-info"><h4>Note</h4><p>There are other forms of attention that work around the length  limitation by using a relative position approach. Read about "local  attention" in `Effective Approaches to Attention-based Neural Machine  Translation <https: arxiv.org="" abs="" 1508.04025="">`__.</https:></p></div><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-Training-Data"><a href="#Preparing-Training-Data" class="headerlink" title="Preparing Training Data"></a>Preparing Training Data</h2><p>To train, for each pair we will need an input tensor (indexes of the<br>words in the input sentence) and target tensor (indexes of the words in<br>the target sentence). While creating these vectors we will append the<br>EOS token to both sequences.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">indexesFromSentence</span><span class="token punctuation">(</span>lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>lang<span class="token punctuation">.</span>word2index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">tensorFromSentence</span><span class="token punctuation">(</span>lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    indexes <span class="token operator">=</span> indexesFromSentence<span class="token punctuation">(</span>lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span>    indexes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>EOS_token<span class="token punctuation">)</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>indexes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tensorsFromPair</span><span class="token punctuation">(</span>pair<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_tensor <span class="token operator">=</span> tensorFromSentence<span class="token punctuation">(</span>input_lang<span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    target_tensor <span class="token operator">=</span> tensorFromSentence<span class="token punctuation">(</span>output_lang<span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> target_tensor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>To train we run the input sentence through the encoder, and keep track<br>of every output and the latest hidden state. Then the decoder is given<br>the <code>&lt;SOS&gt;</code> token as its first input, and the last hidden state of the<br>encoder as its first hidden state.</p><p>“Teacher forcing” is the concept of using the real target outputs as<br>each next input, instead of using the decoder’s guess as the next input.<br>Using teacher forcing causes it to converge faster but <code>when the trained network is exploited, it may exhibit instability &lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&amp;rep=rep1&amp;type=pdf&gt;</code>__.</p><p>You can observe outputs of teacher-forced networks that read with<br>coherent grammar but wander far from the correct translation -<br>intuitively it has learned to represent the output grammar and can “pick<br>up” the meaning once the teacher tells it the first few words, but it<br>has not properly learned how to create the sentence from the translation<br>in the first place.</p><p>Because of the freedom PyTorch’s autograd gives us, we can randomly<br>choose to use teacher forcing or not with a simple if statement. Turn<br><code>teacher_forcing_ratio</code> up to use more of it.</p><pre class="line-numbers language-python"><code class="language-python">teacher_forcing_ratio <span class="token operator">=</span> <span class="token number">0.5</span><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> target_tensor<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> encoder_optimizer<span class="token punctuation">,</span> decoder_optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> max_length<span class="token operator">=</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token punctuation">:</span>    encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">.</span>initHidden<span class="token punctuation">(</span><span class="token punctuation">)</span>    encoder_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    decoder_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    input_length <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    target_length <span class="token operator">=</span> target_tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    encoder_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> encoder<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    loss <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> ei <span class="token keyword">in</span> range<span class="token punctuation">(</span>input_length<span class="token punctuation">)</span><span class="token punctuation">:</span>        encoder_output<span class="token punctuation">,</span> encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">(</span>            input_tensor<span class="token punctuation">[</span>ei<span class="token punctuation">]</span><span class="token punctuation">,</span> encoder_hidden<span class="token punctuation">)</span>        encoder_outputs<span class="token punctuation">[</span>ei<span class="token punctuation">]</span> <span class="token operator">=</span> encoder_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>    decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>SOS_token<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    decoder_hidden <span class="token operator">=</span> encoder_hidden    use_teacher_forcing <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> teacher_forcing_ratio <span class="token keyword">else</span> <span class="token boolean">False</span>    <span class="token keyword">if</span> use_teacher_forcing<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Teacher forcing: Feed the target as the next input</span>        <span class="token keyword">for</span> di <span class="token keyword">in</span> range<span class="token punctuation">(</span>target_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            decoder_output<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> decoder_attention <span class="token operator">=</span> decoder<span class="token punctuation">(</span>                decoder_input<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span>            loss <span class="token operator">+=</span> criterion<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> target_tensor<span class="token punctuation">[</span>di<span class="token punctuation">]</span><span class="token punctuation">)</span>            decoder_input <span class="token operator">=</span> target_tensor<span class="token punctuation">[</span>di<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># Teacher forcing</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Without teacher forcing: use its own predictions as the next input</span>        <span class="token keyword">for</span> di <span class="token keyword">in</span> range<span class="token punctuation">(</span>target_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            decoder_output<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> decoder_attention <span class="token operator">=</span> decoder<span class="token punctuation">(</span>                decoder_input<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span>            topv<span class="token punctuation">,</span> topi <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            decoder_input <span class="token operator">=</span> topi<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># detach from history as input</span>            loss <span class="token operator">+=</span> criterion<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> target_tensor<span class="token punctuation">[</span>di<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> decoder_input<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> EOS_token<span class="token punctuation">:</span>                <span class="token keyword">break</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    encoder_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    decoder_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> target_length<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This is a helper function to print time elapsed and estimated time<br>remaining given the current time and progress %.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> time<span class="token keyword">import</span> math<span class="token keyword">def</span> <span class="token function">asMinutes</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>    m <span class="token operator">=</span> math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>s <span class="token operator">/</span> <span class="token number">60</span><span class="token punctuation">)</span>    s <span class="token operator">-=</span> m <span class="token operator">*</span> <span class="token number">60</span>    <span class="token keyword">return</span> <span class="token string">'%dm %ds'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>m<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">timeSince</span><span class="token punctuation">(</span>since<span class="token punctuation">,</span> percent<span class="token punctuation">)</span><span class="token punctuation">:</span>    now <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    s <span class="token operator">=</span> now <span class="token operator">-</span> since    es <span class="token operator">=</span> s <span class="token operator">/</span> <span class="token punctuation">(</span>percent<span class="token punctuation">)</span>    rs <span class="token operator">=</span> es <span class="token operator">-</span> s    <span class="token keyword">return</span> <span class="token string">'%s (- %s)'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>asMinutes<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">,</span> asMinutes<span class="token punctuation">(</span>rs<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The whole training process looks like this:</p><ul><li> Start a timer</li><li> Initialize optimizers and criterion</li><li> Create set of training pairs</li><li> Start empty losses array for plotting</li></ul><p>Then we call <code>train</code> many times and occasionally print the progress (%<br>of examples, time so far, estimated time) and average loss.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">trainIters</span><span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> n_iters<span class="token punctuation">,</span> print_every<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> plot_every<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    plot_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    print_loss_total <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true"># Reset every print_every</span>    plot_loss_total <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true"># Reset every plot_every</span>    encoder_optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>encoder<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>    decoder_optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>decoder<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>    training_pairs <span class="token operator">=</span> <span class="token punctuation">[</span>tensorsFromPair<span class="token punctuation">(</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span>                      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iters<span class="token punctuation">)</span><span class="token punctuation">]</span>    criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> iter <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_iters <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        training_pair <span class="token operator">=</span> training_pairs<span class="token punctuation">[</span>iter <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>        input_tensor <span class="token operator">=</span> training_pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        target_tensor <span class="token operator">=</span> training_pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> train<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> target_tensor<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span>                     decoder<span class="token punctuation">,</span> encoder_optimizer<span class="token punctuation">,</span> decoder_optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>        print_loss_total <span class="token operator">+=</span> loss        plot_loss_total <span class="token operator">+=</span> loss        <span class="token keyword">if</span> iter <span class="token operator">%</span> print_every <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            print_loss_avg <span class="token operator">=</span> print_loss_total <span class="token operator">/</span> print_every            print_loss_total <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%s (%d %d%%) %.4f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>timeSince<span class="token punctuation">(</span>start<span class="token punctuation">,</span> iter <span class="token operator">/</span> n_iters<span class="token punctuation">)</span><span class="token punctuation">,</span>                                         iter<span class="token punctuation">,</span> iter <span class="token operator">/</span> n_iters <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> print_loss_avg<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> iter <span class="token operator">%</span> plot_every <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            plot_loss_avg <span class="token operator">=</span> plot_loss_total <span class="token operator">/</span> plot_every            plot_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>plot_loss_avg<span class="token punctuation">)</span>            plot_loss_total <span class="token operator">=</span> <span class="token number">0</span>    showPlot<span class="token punctuation">(</span>plot_losses<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Plotting-results"><a href="#Plotting-results" class="headerlink" title="Plotting results"></a>Plotting results</h2><p>Plotting is done with matplotlib, using the array of loss values<br><code>plot_losses</code> saved while training.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>switch_backend<span class="token punctuation">(</span><span class="token string">'agg'</span><span class="token punctuation">)</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>ticker <span class="token keyword">as</span> ticker<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">showPlot</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">:</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>    fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># this locator puts ticks at regular intervals</span>    loc <span class="token operator">=</span> ticker<span class="token punctuation">.</span>MultipleLocator<span class="token punctuation">(</span>base<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>loc<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>points<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Evaluation is mostly the same as training, but there are no targets so<br>we simply feed the decoder’s predictions back to itself for each step.<br>Every time it predicts a word we add it to the output string, and if it<br>predicts the EOS token we stop there. We also store the decoder’s<br>attention outputs for display later.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> sentence<span class="token punctuation">,</span> max_length<span class="token operator">=</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        input_tensor <span class="token operator">=</span> tensorFromSentence<span class="token punctuation">(</span>input_lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span>        input_length <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">.</span>initHidden<span class="token punctuation">(</span><span class="token punctuation">)</span>        encoder_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> encoder<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        <span class="token keyword">for</span> ei <span class="token keyword">in</span> range<span class="token punctuation">(</span>input_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            encoder_output<span class="token punctuation">,</span> encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">(</span>input_tensor<span class="token punctuation">[</span>ei<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                     encoder_hidden<span class="token punctuation">)</span>            encoder_outputs<span class="token punctuation">[</span>ei<span class="token punctuation">]</span> <span class="token operator">+=</span> encoder_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>        decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>SOS_token<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># SOS</span>        decoder_hidden <span class="token operator">=</span> encoder_hidden        decoded_words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        decoder_attentions <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>        <span class="token keyword">for</span> di <span class="token keyword">in</span> range<span class="token punctuation">(</span>max_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            decoder_output<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> decoder_attention <span class="token operator">=</span> decoder<span class="token punctuation">(</span>                decoder_input<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span>            decoder_attentions<span class="token punctuation">[</span>di<span class="token punctuation">]</span> <span class="token operator">=</span> decoder_attention<span class="token punctuation">.</span>data            topv<span class="token punctuation">,</span> topi <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>data<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> topi<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> EOS_token<span class="token punctuation">:</span>                decoded_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">'&lt;EOS>'</span><span class="token punctuation">)</span>                <span class="token keyword">break</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                decoded_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output_lang<span class="token punctuation">.</span>index2word<span class="token punctuation">[</span>topi<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            decoder_input <span class="token operator">=</span> topi<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> decoded_words<span class="token punctuation">,</span> decoder_attentions<span class="token punctuation">[</span><span class="token punctuation">:</span>di <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We can evaluate random sentences from the training set and print out the<br>input, target, and output to make some subjective quality judgements:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluateRandomly</span><span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>        pair <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'>'</span><span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output_words<span class="token punctuation">,</span> attentions <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output_sentence <span class="token operator">=</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output_words<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'&lt;'</span><span class="token punctuation">,</span> output_sentence<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Training-and-Evaluating"><a href="#Training-and-Evaluating" class="headerlink" title="Training and Evaluating"></a>Training and Evaluating</h1><p>With all these helper functions in place (it looks like extra work, but<br>it makes it easier to run multiple experiments) we can actually<br>initialize a network and start training.</p><p>Remember that the input sentences were heavily filtered. For this small<br>dataset we can use relatively small networks of 256 hidden nodes and a<br>single GRU layer. After about 40 minutes on a MacBook CPU we’ll get some<br>reasonable results.</p><p>.. Note::<br>   If you run this notebook you can train, interrupt the kernel,<br>   evaluate, and continue training later. Comment out the lines where the<br>   encoder and decoder are initialized and run <code>trainIters</code> again.</p><pre class="line-numbers language-python"><code class="language-python">hidden_size <span class="token operator">=</span> <span class="token number">20</span>encoder1 <span class="token operator">=</span> EncoderRNN<span class="token punctuation">(</span>input_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>attn_decoder1 <span class="token operator">=</span> AttnDecoderRNN<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">,</span> dropout_p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>trainIters<span class="token punctuation">(</span>encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">,</span> <span class="token number">75000</span><span class="token punctuation">,</span> print_every<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>2m 24s (- 33m 37s) (5000 6%) 3.37745m 0s (- 32m 31s) (10000 13%) 2.86507m 36s (- 30m 26s) (15000 20%) 2.736810m 9s (- 27m 56s) (20000 26%) 2.655212m 38s (- 25m 17s) (25000 33%) 2.582015m 17s (- 22m 55s) (30000 40%) 2.538217m 50s (- 20m 23s) (35000 46%) 2.521520m 30s (- 17m 57s) (40000 53%) 2.459122m 51s (- 15m 14s) (45000 60%) 2.425925m 22s (- 12m 41s) (50000 66%) 2.362327m 50s (- 10m 7s) (55000 73%) 2.340230m 22s (- 7m 35s) (60000 80%) 2.308033m 3s (- 5m 5s) (65000 86%) 2.272235m 38s (- 2m 32s) (70000 93%) 2.276438m 20s (- 0m 0s) (75000 100%) 2.2802</code></pre><pre class="line-numbers language-python"><code class="language-python">evaluateRandomly<span class="token punctuation">(</span>encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&gt; il s en met plein les poches .= he s raking it in .&lt; he s always to the . . &lt;EOS&gt;&gt; je suis en train de griller du poisson .= i am grilling fish .&lt; i m a . . &lt;EOS&gt;&gt; c est un mannequin .= she s a model .&lt; he s a nice . &lt;EOS&gt;&gt; il n est pas un saint .= he s no saint .&lt; he s not a . . &lt;EOS&gt;&gt; je n abandonne pas .= i m not giving up .&lt; i m not alone . &lt;EOS&gt;&gt; vous etes jeunes .= you re young .&lt; you re a . &lt;EOS&gt;&gt; il fait un super boulot .= he is doing a super job .&lt; he s a to of . . &lt;EOS&gt;&gt; tu es trop maigre .= you re too skinny .&lt; you re very busy . &lt;EOS&gt;&gt; je ne suis pas intimide .= i m not intimidated .&lt; i m not alone . &lt;EOS&gt;&gt; il est plus fort que moi .= he s stronger than me .&lt; he s not as . &lt;EOS&gt;</code></pre><p>​    </p><h2 id="Visualizing-Attention"><a href="#Visualizing-Attention" class="headerlink" title="Visualizing Attention"></a>Visualizing Attention</h2><p>A useful property of the attention mechanism is its highly interpretable<br>outputs. Because it is used to weight specific encoder outputs of the<br>input sequence, we can imagine looking where the network is focused most<br>at each time step.</p><p>You could simply run <code>plt.matshow(attentions)</code> to see attention output<br>displayed as a matrix, with the columns being input steps and rows being<br>output steps:</p><pre class="line-numbers language-python"><code class="language-python">output_words<span class="token punctuation">,</span> attentions <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>    encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">,</span> <span class="token string">"je suis trop froid ."</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>matshow<span class="token punctuation">(</span>attentions<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>&lt;matplotlib.image.AxesImage at 0x7f68d8ef77b8&gt;</code></pre><p>For a better viewing experience we will do the extra work of adding axes<br>and labels:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">showAttention</span><span class="token punctuation">(</span>input_sentence<span class="token punctuation">,</span> output_words<span class="token punctuation">,</span> attentions<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Set up figure with colorbar</span>    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>    ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span>    cax <span class="token operator">=</span> ax<span class="token punctuation">.</span>matshow<span class="token punctuation">(</span>attentions<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'bone'</span><span class="token punctuation">)</span>    fig<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span>cax<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Set up axes</span>    ax<span class="token punctuation">.</span>set_xticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span> <span class="token operator">+</span> input_sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span> <span class="token operator">+</span>                       <span class="token punctuation">[</span><span class="token string">'&lt;EOS>'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> rotation<span class="token operator">=</span><span class="token number">90</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>set_yticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span> <span class="token operator">+</span> output_words<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Show label at every tick</span>    ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>ticker<span class="token punctuation">.</span>MultipleLocator<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>ticker<span class="token punctuation">.</span>MultipleLocator<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">evaluateAndShowAttention</span><span class="token punctuation">(</span>input_sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    output_words<span class="token punctuation">,</span> attentions <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>        encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">,</span> input_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input ='</span><span class="token punctuation">,</span> input_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'output ='</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output_words<span class="token punctuation">)</span><span class="token punctuation">)</span>    showAttention<span class="token punctuation">(</span>input_sentence<span class="token punctuation">,</span> output_words<span class="token punctuation">,</span> attentions<span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"elle a cinq ans de moins que moi ."</span><span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"elle est trop petit ."</span><span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"je ne crains pas de mourir ."</span><span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"c est un jeune directeur plein de talent ."</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>input = elle a cinq ans de moins que moi .output = she is always to of as me . &lt;EOS&gt;input = elle est trop petit .output = she is very nice . &lt;EOS&gt;input = je ne crains pas de mourir .output = i m not going to . . &lt;EOS&gt;input = c est un jeune directeur plein de talent .output = he s a a man . &lt;EOS&gt;</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul><li><p>Try with a different dataset</p><ul><li> Another language pair</li><li> Human → Machine (e.g. IOT commands)</li><li> Chat → Response</li><li> Question → Answer</li></ul></li><li><p>Replace the embeddings with pre-trained word embeddings such as word2vec or<br> GloVe</p></li><li><p>Try with more layers, more hidden units, and more sentences. Compare<br> the training time and results.</p></li><li><p>If you use a translation file where pairs have two of the same phrase<br> (<code>I am test \t I am test</code>), you can use this as an autoencoder. Try<br> this:</p><ul><li> Train as an autoencoder</li><li> Save only the Encoder network</li><li> Train a new Decoder for translation from there</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 论文复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ssh远程连接服务器</title>
      <link href="/year/09/21/004/"/>
      <url>/year/09/21/004/</url>
      
        <content type="html"><![CDATA[<p>​    本文简单介绍ssh远程连接实验室服务器的步骤，踩坑许多，很多原理依旧不懂，但最后实现:</p><ul><li>连接实验室网络后内网连接远程服务器功能</li><li>配置本地密钥和远程服务器用户密钥使其免密钥功能</li><li>2021/09/25更新：实现外网连接实验室服务器的功能</li></ul><h5 id="1-实现远程连接服务器"><a href="#1-实现远程连接服务器" class="headerlink" title="1.实现远程连接服务器"></a>1.实现远程连接服务器</h5><h6 id="1-本地服务准备"><a href="#1-本地服务准备" class="headerlink" title="1.本地服务准备"></a>1.本地服务准备</h6><p>本地主机上打开windows terminal窗口(现在的windows一般会自动安装openSSH客户端和服务端),执行命令：</p><pre class="line-numbers language-none"><code class="language-none">ssh-keygen -t rsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行命令后会在<code>.ssh</code>文件下生成两个密钥，<code>id_rsa</code>和<code>id_rsa.pub</code>一个私钥一个公钥;实现远程连接服务器关键是把<strong>公钥</strong>存放到远程服务器端</p><h6 id="2-配置本地config文件"><a href="#2-配置本地config文件" class="headerlink" title="2.配置本地config文件"></a>2.配置本地config文件</h6><h6 id="3-将本地公钥上传至服务器"><a href="#3-将本地公钥上传至服务器" class="headerlink" title="3.将本地公钥上传至服务器"></a>3.将本地公钥上传至服务器</h6><p>在windows terminal下执行命令</p><pre class="line-numbers language-none"><code class="language-none">scp C:\Users\VrShadow\.ssh\id_rsa.pub XXX@192.168.0.75:\home\xxx\<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>本地公钥拷贝至远程服务器[注意xxx更改为自己在远程服务器端分配的用户名！！！],此时传过来的公钥存在<code>./home/xxx</code>下</p><h6 id="4-公钥写入授权文件"><a href="#4-公钥写入授权文件" class="headerlink" title="4.公钥写入授权文件"></a>4.公钥写入授权文件</h6><p>在远程服务器上执行命令</p><pre class="line-numbers language-none"><code class="language-none">touch ./.ssh/authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>远程服务器端这边用的是linux系统，所以先要创建文件<code>authorized_keys</code></p><p>将本地传过来的密钥<strong>写入authorizd_keys</strong>中</p><pre class="line-numbers language-none"><code class="language-none">cat ./home/xxx/id_rsa.pub >> ./.ssh/authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-vscode免密登录"><a href="#2-vscode免密登录" class="headerlink" title="2.vscode免密登录"></a>2.vscode免密登录</h5><h6 id="1-准备插件SSH"><a href="#1-准备插件SSH" class="headerlink" title="1.准备插件SSH"></a>1.准备插件SSH</h6><p>在插件里搜索安装即可</p><h6 id="2-修改本地config配置文件"><a href="#2-修改本地config配置文件" class="headerlink" title="2.修改本地config配置文件"></a>2.修改本地config配置文件</h6><p>本地config文件里面加入<strong>”IdentifyFile” ”C:\Users\VrShadow.ssh\id_rsa”</strong></p><p>完成之后侧边导航栏会出现远程资源管理器图标，点击之后选择远程服务器时对应的端口下的分支用户，点击就会开启新的窗口(第一次会让你选择远程服务器的操作系统)，之后就会进入对应用户下的目录进行工作。</p><h5 id="3-外网远程连接"><a href="#3-外网远程连接" class="headerlink" title="3.外网远程连接"></a>3.外网远程连接</h5><p>自己的本地用户<code>.ssh</code>文件里面已经配置了<strong>config</strong>文件，已经配置了jump内网权限<br>命令行执行：</p><pre class="line-numbers language-none"><code class="language-none">ssh jumpnone<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行后需要远程服务器的密码：********</p><p>输入密码后进行远程连接操作</p><pre class="line-numbers language-none"><code class="language-none">ssh username@host<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果本地用户名和远程用户名一致,登录时可以省略用户名</p><pre class="line-numbers language-none"><code class="language-none">ssh host<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>SSH的默认端口是22,也就是说你的登录请求会送进远程主机的22端口.使用<code>-p</code>参数可以修改端口</p><pre><code>ssh -p 2222 user@host  # 此条命令表示ssh直接连接远程主机的2222端口</code></pre><p>我写的比较粗糙,(偷个懒)可以参考我朋友的blog：</p><blockquote><p><a href="https://lry89757.github.io/2021/09/24/linux-bi-ji/">朋友的博客</a></p></blockquote><p>、最后的实验就是如下的效果:</p><ul><li><p>连接外网的情况下</p><h5 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h5><pre><code># 两种方法：(在已经配置好confi和公钥文件下并且打开jump跳板和开启“Identiyfile"下)ssh 43004   # 必须要打开跳板权限,而且回车后每次都要输入服务器所在公网地址的密码            # 当然分配给user@hosts的密码要看你是否注释了公钥ssh gyf@192.168.0.75 # 这样访问在内网下使用，当然内网下也可以使用ssh 43004连接服务器</code></pre><ul><li><h5 id="外网访问"><a href="#外网访问" class="headerlink" title="外网访问"></a>外网访问</h5><ul><li><strong>ssh 43004</strong>:需要输入服务器所在公网地址密码和分配给用户的密码</li><li><strong>ssh user@host</strong>:不能连接服务器</li></ul></li><li><h5 id="内网访问"><a href="#内网访问" class="headerlink" title="内网访问"></a>内网访问</h5><ul><li><strong>ssh 43004</strong>:仍然需要输入服务器所在公网地址密码但是不用输入分配给用户的密码了</li><li><strong>ssh user@host</strong>:这样公网密码和分配给用户的密码都不用输入了直接连接</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 搭建环境 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows terminal </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读一:Attention Mechanism</title>
      <link href="/year/09/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB(%E4%B8%80)/"/>
      <url>/year/09/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract-amp-amp-Introduction"><a href="#Abstract-amp-amp-Introduction" class="headerlink" title="Abstract &amp;&amp; Introduction"></a>Abstract &amp;&amp; Introduction</h4><p>​    这几天阅读了一篇较早提出Attention machanism的论文<a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>,这篇论文将注意力机制应用在神经网络翻译中，论文的思路从传统NMT(Neural Machine Translation)系统的缺陷说起，针对其进行改进，最后进行了定量和定性分析.</p><p>​    首先我们要了解经典的Sea2Seq模型是如何进行翻译的：整体模型采用Encoder-Decoder进行分析，将输入的序列经过Encoder处理，压缩成一个Fixed-length Vector；在Decoder阶段，将这个向量的信息还原成一个序列完成翻译任务。基于RNN的Seq2Seq模型主要由两篇文章介绍，只是采用了不同的RNN模型。Ilya Sutskever等人2014年在论文《Sequence to Sequence Learning with Neural Networks》中使用LSTM来搭建Seq2Seq模型。随后，2015年，Kyunghyun Cho等人在论文《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》提出了基于GRU的Seq2Seq模型。想要解决的主要问题就是如何把机器翻译中，变长的输入X映射到一个变长输出Y。而这篇论文提出一种新的方法，这个方法也是基于<code>encoder-decoder</code>的，与之前的<code>encoder-decoder</code>模型不同的是，每次在翻译一个单词的时候，模型会自动搜寻该单词与源句子哪些单词有关联，并将这种关联的强度进行数字化表示(在模型中就是权重)，并且训练得出这种方法可以解决句子翻译不准的问题。</p><h4 id="传统RNN"><a href="#传统RNN" class="headerlink" title="传统RNN"></a>传统RNN</h4><p>​    大部分的神经机器翻译都是基于<code>encoder-decoder</code>框架的并且都会将源语言句子序列压缩成一个固定的向量，然后传递给decoder。传统的RNN Encoder-Decoder模型在训练阶段时候，会使模型去最大化源语言翻译成目标语言的条件概率。当模型训练好之后，当代翻译的源语言句子放入到模型中后，模型会自动计算最大目标句子的概率并且将这个句子当作是翻译后的句子。简单介绍以下传统的RNN:</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20210920180508294.png" alt="image-20210920180508294"></p><p>上图中<code>C</code>的左侧是<code>Encoder</code>,右侧是<code>Decoder</code>,”C”是待翻译语句的语义信息；输入一个句子的时候会经过Encoder，Encoder讲这句话进行编码，Encoder用到的模型是RNN，编码结束以后将最后一个时刻RNN的隐层的输出当作输入的这句话的”语义压缩”。然后解码器每产生一个翻译后的英文单词的时候，都会利用<strong>C</strong>并且还会接受输入t时刻的上一个隐藏向量<strong>s</strong>。这个时刻的输出端就会产生第一个单词(这里使用了softmax函数，输出层是一个词典大小维度的向量)，哪个维度的值最大就取哪个维度所对应的单词。大家可以明白的是训练阶段，Encoder和Decoder不可能立马产生目标单词，而是产生一个预测结果，训练的目的就是不断优化参数。</p><h4 id="Attention机制加入"><a href="#Attention机制加入" class="headerlink" title="Attention机制加入"></a>Attention机制加入</h4><p>本paper提出的模型叫做<strong>RNNsearch</strong>：</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20210920183014609.png" alt="image-20210920183014609"></p><p>​    图中的右半部分是encoder，这一部分和RNNenc模型一样，重点在decoder部分和传统的会有巨大的差别；在t=0时刻，decoder的BiLSTM接受三个输入，第一个是初始状态s0(这个是随机初始化的，无论是训练阶段还是预测阶段都是随机)；第二个输入来源于emdedding后的向量；第三个输入比较复杂，也是新模型的核心创新点</p><p>​    首先，随机初 算(计算方式有很多种可以自己定义)，各自得到一个e1 ~ e6的值，对这个6个值进行一次softmax得到α1 ~ α6，和是1；将α1，α2，α3，α4，α5，α6看作是s0和h1 ~ h6的相似度。然后α和h向量做一次元素乘积，得到的6个向量做一次元素的相加得到最终的向量。将这个向量当作0时刻BiLSTM的第三个输入。时刻0，BiLSTM就会有一个输出，时刻变为1，接下来的过程继续向后进行。</p><p><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a>实现Seq2Seq(Attention)后的模型，基本实现了此篇论文的创新点。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 注意力机制 </tag>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法而已</title>
      <link href="/year/09/17/003/"/>
      <url>/year/09/17/003/</url>
      
        <content type="html"><![CDATA[<p>算法入门：啊哈算法 算法图解 大话数据结构</p><p>算法进阶：cf 白书 紫书 蓝书</p><h2 id="第一章-基础算法"><a href="#第一章-基础算法" class="headerlink" title="第一章 基础算法"></a>第一章 基础算法</h2><h3 id="基础算法-一"><a href="#基础算法-一" class="headerlink" title="基础算法(一)"></a>基础算法(一)</h3><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><ul><li>含义:排序是指将一个无序序列按照某个规则进行有序排列(以下排序均实现的是从小到大排序)</li></ul><h5 id="简单排序"><a href="#简单排序" class="headerlink" title="简单排序"></a>简单排序</h5><ul><li><p>冒泡排序的本质在于==交换== ，即每次通过交换的方式把当前剩余元素的最大值移动到一端</p><pre class="line-numbers language-c++"><code class="language-c++"># 冒泡排序(以下实现从小到大排序)int a[n]={......};for(int i=1;i<n;i++){  //进行n-1躺//第i躺，从a[0]-a[n-i-1]每一个数都要与下一个数进行比较，遇到后面比自己较大的数就交换，实现每一趟剩余的数a[0]-a[n-    i]的冒泡排序，使当前a[0]~a[n-i]中的最大的元素移动到最后面的,a[n-i+1]-a[i]已经排好序    for(int j=0;j< n-i;j++){        if(a[j] > a[j+1]){  //如果左边的数更大，则a[j]与a[j+1]交换            int temp = a[j];            a[j] = a[j+1];            a[j+1] = temp;        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>(简单)选择排序：</p><pre class="line-numbers language-c++"><code class="language-c++"># 选择排序(以下实现从小到大排序)# 简单选择排序是指对一个序列a[n]中的元素a[1]~a[n]，令i从1~n进行枚举，进行n趟操作，每趟从待排序部分[i,n]其中选择最小的元素，令其与待排部分的第一个元素a[i]进行交换，这样元素a[i]就会与当前区间[1,i-1]形成新的有序区间[1,i],n趟操作以后，就形成有序区间int a[n]={......};void select_sort(){    for(int i=1;i<=n;i++){  //进行n趟操作        int k = i;        for(int j=i;j<=n;j++){  //选出[i,n]中最小元素的下标，并且将下标记为k            if(a[j]<a[k]){                k = j;            }        }        int temp = a[i];  //交换a[k]与当前待排序序列[i,n]的第一个元素a[i]        a[i] = a[k];        a[j] = temp;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>(直接)插入排序：</p><pre class="line-numbers language-c++"><code class="language-c++"># 直接插入排序# 直接插入排序是指对序列a[n]中的元素a[i]~a[n]，i从2~n进行枚举，进行n-1趟操作。假设某一趟，序列a[1]~a[i-1]已经有序，那么这一次就是从范围[1,i-1]中寻找某个位置j,使得a[i]插入到这个位置j后，此时a[j]~a[i-1]会自动向后移动一位到a[j+1]~a[i],范围a[1,i]有序int a[n]={......};  //n为元素个数，数组下标为1~nvoid insert_sort(){    for(int i=2;i<=n;i++){  //进行n-1趟排序        int temp = a[i],j = i; //temp临时存放a[i]        while(j>1 && temp<a[j-1]){            a[j] = a[j-1];            j--;        }        a[j] = temp;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="快排"><a href="#快排" class="headerlink" title="快排"></a>快排</h5><ul><li><p>快排的主要思想是分治</p><pre class="line-numbers language-c++"><code class="language-c++">//快排的时间复杂度是nlogn(这里所指的是平均复杂度)#include <iostream>acwing 785快速排序using namespace std;const int N = 1e6+10;int n;int q[N];void quick_sort(int q[], int l, int r){    if (l >= r) return;    int i = l - 1, j = r + 1, x = q[l + r >> 1];  //x的取值可以取区间里面任意一个    while (i < j)    {        do i ++ ; while (q[i] < x);        do j -- ; while (q[j] > x);        if (i < j) swap(q[i], q[j]);    }    quick_sort(q, l, j); //对左边的进行快排    quick_sort(q, j + 1, r); //对右边进行快排}int main(){    scanf("%d",&n);    for(int i=0;i<n;i++){        scanf("%d",&q[i]);    }    quick_sort(q,0,n-1);        for(int i=0;i<n;i++){        printf("%d ",q[i]);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h5><ul><li><p>归并的主要思想也是分治</p><pre class="line-numbers language-c++"><code class="language-c++">acwing787 归并排序#include <iostream>using namespace std;const int N = 1e6+10;int n;int q[N];int tmp[N];void merge_sort(int q[],int l,int r){    if(l>=r) return ;        int mid = l+r >> 1;  //1：确定分界点        merge_sort(q,l,mid);   //对左右两边分别进行归并排序    merge_sort(q,mid+1,r);        // 将左右两边进行归并排序，把两个有序的序列拼接在一起，拼接的方法就是归并    int k=0,i=l,j=mid+1;     while(i<=mid && j<= r){        if(q[i]<=q[j]) tmp[k++] = q[i++];        else tmp[k++] = q[j++];    }    while(i<=mid) tmp[k++]=q[i++];  //对于q[l]~[mid]和q[mid+1~r]两个序列，如果存在序列没有循环结束的话就直接                                到tmp序列后面即可    while(j<=r) tmp[k++]=q[j++];        for(i=l,j=0;i <= r;i++,j++) q[i] = tmp[j];}int main(){    scanf("%d",&n);    for(int i=0;i<n;i++) scanf("%d",&q[i]);        merge_sort(q,0,n-1);        for(int i=0;i<n;i++) printf("%d ",q[i]);        return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="二分"><a href="#二分" class="headerlink" title="二分"></a>二分</h4><h5 id="整数"><a href="#整数" class="headerlink" title="整数"></a>整数</h5><ul><li><p>整数二分的本质:有单调性的话一定可以二分；但是能二分的不一定具有单调性<br>二分的本质是对于一个整数区间，我们先定义一个性质，要找到一个中间点，是的在这个点的右半边满足这个性质，左半边不满足这个性质，这样就可以把一个区间一分为二，找到这个边界</p><pre class="line-numbers language-c++"><code class="language-c++">#1.找到一个中间值mid# if(check(mid)) true:mid满足这个性质  false:mid不满足这个性质# 二分的时候一定要保证要寻找的值一定在不断缩小的那个区间里面，当区间的长度为1的时候就代表找到答案#acwing789:数的范围#include <iostream>#include <algorithm>#include <cstring>using namespace std;const int N=100010;int a,b;int q[N];int main(){    scanf("%d %d",&a,&b);    for(int i=0;i<a;i++) scanf("%d",&q[i]);        while(b--){        int x;        scanf("%d",&x);                int l=0,r=a-1;        while(l<r){            int mid= l+r >> 1;            if(q[mid]>=x) r=mid;            else l=mid+1;        }                if(q[l]!=x) cout<<"-1 -1"<<endl;  // 这个表示要寻找的那个值不在区间里面，此时q[l]的值是第一个满足大于x的数        else{            cout<<l<<' ';                        int l=0,r=a-1;            while(l<r){                int mid= l+r+1 >> 1;                if(q[mid]<=x) l=mid;                else r=mid-1;            }            cout<<l        }    }    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h5><ul><li><p>浮点数二分:本质上也是寻找边界，满足左半边满足性质，右半边不满足性质，知道</p></li><li><pre class="line-numbers language-c++"><code class="language-c++"># 例子:算平方根#include <iostream>using namespace std;int mian(){    double x;    cin>>x;        double l=0,r=x;    double mid = (l+r)/2;    while(r-l > 1e-8){        if(mid*mid>=x)  r=mid;        else l=mid;    }        printf("%lf",&l);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序算法 </tag>
            
            <tag> acwing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇，表达点看法吧</title>
      <link href="/year/09/15/001/"/>
      <url>/year/09/15/001/</url>
      
        <content type="html"><![CDATA[<p><span class="github-emoji"><span>😄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>😆</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>😆</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>值得记录一下</p><p>这是我的第一篇博文，花了很长时间来进行操作，在网上搜索的教程参差不齐，也同样会出现各种各样的问题，比如nodejs版本过高与hexo不兼容问题，我觉得还是有问题还是要多和其他人沟通，另外对主题的设置可以按照自己的风格来，但是这就需要对web知识有一定的了解，对排版有自己的理解才可以。<br>在我看来，博客更加注重的应该是内容，以及养成记录日记的习惯，对自己每个阶段的学习有一个适当的总结，可以让自己计划更加明确。<br>所以我就简单介绍hexo+github.io搭建博客过程中重要的点吧(我是用的主题是matery)</p><h4 id="本地配置文件"><a href="#本地配置文件" class="headerlink" title="本地配置文件"></a>本地配置文件</h4><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20210916214915453.png" alt="image-20210916214915453"></p><ul><li><p><strong>_config.yml</strong>：</p><p>网站<strong>站点配置文件</strong>，又叫根目录站点配置文件，在这个文件里面可以配置大部分的参数</p></li><li><p><strong>scaffolds</strong>:</p><p>此文件夹会放一些默认的文件，用来当作创建博文的模板md文件，hexo会根据scaffold来建立文件。模板是指新建的md文件会默认放入模板文件的初始内容</p></li><li><p><strong>public</strong>：</p><p>这个文件的内容最终都会push到github仓库中</p></li><li><p><strong>source</strong>:</p><p>这个文件夹是存放用户资源的地方，除了<code>_posts</code>文件夹之外，开头命名为_(下划线的文件/文件夹以及隐藏的文件都会被忽略)。markdown和html文件都会被解析并放到<strong>public</strong>文件夹里面，而其他文件会被拷贝到public文件夹。</p></li><li><p>**为github仓库添加readme</p><p>既然<code>source</code>文件夹中的内容会被全部推送到public文件夹，public文件夹中的内容最终又会被push到github仓库，所以如果想要为github仓库添加readme.md，只要在source文件夹中创建就好了。最后<strong>部署</strong>到github就有readme了。但是会发现，README.md文件部署的时候会被解析成html文件，显示的是html代码，不是我们想要的文档内容。</p><p><strong>解决办法</strong>：将在source文件夹新建的README.md重命名为REMADE.MDWN，在重新部署到github。(source文件夹中，.md会被解析为html。并放到public文件夹被push到github，但.MDWN不会被解析)</p></li></ul><h4 id="一些常用的Hexo命令"><a href="#一些常用的Hexo命令" class="headerlink" title="一些常用的Hexo命令"></a>一些常用的Hexo命令</h4><ul><li><p>常用命令</p><pre><code>hexo new "postName" #新建博文hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’crtl+c'关闭server）hexo deploy #部署到githubhexo help #查看帮助hexo version #查看版本</code></pre></li><li><p>缩写</p><pre><code>hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy</code></pre></li><li><p>组合命令</p><pre><code>hexo s -g #生成并本地预览hexo d -g #生成并部署到云端</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 测试，测试的子分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博文 </tag>
            
            <tag> 测试 </tag>
            
            <tag> whatever </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>README.md</title>
      <link href="/year/09/14/README/"/>
      <url>/year/09/14/README/</url>
      
        <content type="html"><![CDATA[<h3 id="20201-9-17-第一次更新"><a href="#20201-9-17-第一次更新" class="headerlink" title="20201.9.17 第一次更新"></a>20201.9.17 第一次更新</h3><ul><li>对文章Front-matter介绍的一些应用尝试增加<ul><li>比如title,date,top，summary等，剩下的待更新尝试<span class="github-emoji"><span>👊</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44a.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>💤</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a4.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></li></ul></li></ul><h3 id="2021-9-14-水第一篇，啥功能没有"><a href="#2021-9-14-水第一篇，啥功能没有" class="headerlink" title="2021.9.14 水第一篇，啥功能没有"></a>2021.9.14 水第一篇，啥功能没有</h3><ul><li>能正常部署文章和渲染正常</li><li>能够访问blog</li></ul><h3 id="2021-11-05"><a href="#2021-11-05" class="headerlink" title="2021.11.05"></a>2021.11.05</h3><ul><li>建站功能</li><li>不蒜子初始化计数</li></ul><h3 id="2021-11-07"><a href="#2021-11-07" class="headerlink" title="2021.11.07"></a>2021.11.07</h3><ul><li>全局搜索</li><li>代码高亮</li></ul>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
