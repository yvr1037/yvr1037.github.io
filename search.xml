<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>notes of huggingface transformer</title>
      <link href="/year/11/25/notes-of-huggingface-transformer/"/>
      <url>/year/11/25/notes-of-huggingface-transformer/</url>
      
        <content type="html"><![CDATA[<h4 id="Perface"><a href="#Perface" class="headerlink" title="Perface"></a>Perface</h4><p>HuggingFace-Transformersæ‰‹å†Œæ˜¯å¼€æºå…¬å¸HuggingFaceå¼€å‘çš„æ¶µç›–å¾ˆå¤šæ¨¡å‹çš„æ¡†æ¶ã€‚</p><p>Transformers(å‰èº«æ˜¯ç§°ä¸ºpytorch Transformerså’Œpytorch pretrained bert)ä¸ºè‡ªç„¶è¯­è¨€ç†è§£(NLU)å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG)æä¾›äº†æœ€å…ˆè¿›çš„é€šç”¨æ¶æ„(bert,GPT-2,RoBERTTa,XLM,DistileBert,XLNet,CTRLâ€¦..),å…¶ä¸­è¶…è¿‡32ä¸ª100å¤šç§è¯­è¨€çš„é¢„è®­ç»ƒæ¨¡å‹å¹¶åŒæ—¶æ”¯æŒTensorflow 2.0å’ŒPytorchä¸¤å¤§æ·±åº¦å­¦ä¹ æ¡†æ¶.</p><p>The library was designed with two strong goals in mind:</p><ul><li><p>Be as easy and fast to use as possible:</p><blockquote><ul><li>We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions, just three standard classes required to use each model: <a href="https://huggingface.co/transformers/main_classes/configuration.html">configuration</a>, <a href="https://huggingface.co/transformers/main_classes/model.html">models</a> and <a href="https://huggingface.co/transformers/main_classes/tokenizer.html">tokenizer</a>.</li><li>All of these classes can be initialized in a simple and unified way from pretrained instances by using a common <code>from_pretrained()</code> instantiation method which will take care of downloading (if needed), caching and loading the related class instance and associated data (configurationsâ€™ hyper-parameters, tokenizersâ€™ vocabulary, and modelsâ€™ weights) from a pretrained checkpoint provided on <a href="https://huggingface.co/models">Hugging Face Hub</a> or your own saved checkpoint.</li><li>On top of those three base classes, the library provides two APIs: <a href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline"><code>pipeline()</code></a> for quickly using a model (plus its associated tokenizer and configuration) on a given task and <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer()</code></a>/<a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TFTrainer"><code>TFTrainer()</code></a> to quickly train or fine-tune a given model.</li><li>As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to extend/build-upon the library, just use regular Python/PyTorch/TensorFlow/Keras modules and inherit from the base classes of the library to reuse functionalities like model loading/saving.</li></ul></blockquote></li><li><p>Provide state-of-the-art models with performances as close as possible to the original models:</p><blockquote><ul><li>We provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture.</li><li>The code is usually as close to the original code base as possible which means some PyTorch code may be not as <em>pytorchic</em> as it could be as a result of being converted TensorFlow code and vice versa.</li></ul></blockquote></li></ul><p>è¿™æ˜¯<a href="https://huggingface.co/transformers/philosophy.html%E5%AE%98%E7%BD%91%E7%BB%99%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A%EF%BC%9A">https://huggingface.co/transformers/philosophy.htmlå®˜ç½‘ç»™å‡ºçš„è§£é‡Šï¼š</a></p><ul><li>æ¶æ„<ul><li>ä½¿ç”¨æ¯ä¸ªæ¨¡å‹éƒ½éœ€è¦ä¸‰ä¸ªæ ‡å‡†ç±»:<strong>configuration</strong>,<strong>models</strong>,<strong>tokenizer</strong>.modelç”¨äºæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹,ä¾‹å¦‚modelä¸ºbertï¼Œé‚£ä¹ˆç›¸åº”çš„ç½‘ç»œç»“æ„æ˜¯bertçš„ç½‘ç»œç»“æ„ï¼›configurationæ˜¯æ¨¡å‹å…·ä½“çš„æ¶æ„é…ç½®ï¼Œä¾‹å¦‚å¯ä»¥é…ç½®å¤šå¤´çš„æ•°é‡ç­‰ç­‰,è¿™é‡Œé…ç½®éœ€è¦æ³¨æ„çš„åœ°æ–¹å°±æ˜¯ï¼Œå¦‚æœè‡ªå®šä¹‰é…ç½®ä¸æ”¹å˜æ ¸å¿ƒç½‘ç»œç»“æ„çš„åˆ™ä»æ—§å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œå¦‚æœé…ç½®æ¶‰åŠåˆ°æ ¸å¿ƒç»“æ„çš„ä¿®æ”¹ï¼Œä¾‹å¦‚å‰é¦ˆç½‘ç»œçš„éšå±‚ç¥ç»å…ƒçš„ä¸ªæ•°ï¼Œåˆ™æ— æ³•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œè¿™ä¸ªæ—¶å€™transformersä¼šé»˜è®¤ä½ è¦é‡æ–°è‡ªå·±é¢„è®­ç»ƒä¸€ä¸ªæ¨¡å‹ä»è€Œéšæœºåˆå§‹åŒ–æ•´ä¸ªæ¨¡å‹çš„æƒé‡ï¼Œè¿™æ˜¯æ˜¯ä¸€ç§åŠçµæ´»æ€§çš„è®¾è®¡.</li><li>æ‰€æœ‰è¿™äº›ç±»éƒ½å¯ä»¥ä½¿ç”¨é€šç”¨çš„from_pretrained()å®ä¾‹åŒ–æ–¹æ³•ï¼Œä»¥ç®€å•ç»Ÿä¸€çš„æ–¹å¼ä»å—è¿‡è®­ç»ƒçš„å®ä¾‹ä¸­åˆå§‹åŒ–ï¼Œè¯¥æ–¹æ³•å°†è´Ÿè´£ä¸‹è½½ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œç¼“å­˜å’ŒåŠ è½½ç›¸å…³çš„ç±»å®ä¾‹ä»¥åŠç›¸å…³çš„æ•°æ®(configçš„çš„è¶…å‚æ•°ï¼Œtokenizerç”Ÿæˆå™¨çš„è¯æ±‡è¡¨å’Œæ¨¡å‹çš„æƒé‡)åœ¨ <a href="https://link.zhihu.com/?target=https://huggingface.co/models">Hugging Face Hub</a> ä¸Šæä¾›çš„é¢„å…ˆè®­ç»ƒçš„æ£€æŸ¥ç‚¹æˆ–æ‚¨è‡ªå·±ä¿å­˜çš„æ£€æŸ¥ç‚¹</li><li>åœ¨è¿™ä¸‰ä¸ªåŸºæœ¬ç±»çš„åŸºç¡€ä¸Šï¼Œè¯¥åº“æä¾›äº†ä¸¤ä¸ªAPIï¼š<ul><li><a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/main_classes/pipelines.html%23transformers.pipeline">pipeline()</a>ç”¨äºåœ¨ç»™å®šä»»åŠ¡ä¸Šå¿«é€Ÿä½¿ç”¨æ¨¡å‹ï¼ˆåŠå…¶å…³è”çš„tokenizerå’Œconfigurationï¼‰å’Œ </li><li>Traineræˆ–è€…<a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/main_classes/trainer.html%23transformers.TFTrainer">TF</a>trainer å¿«é€Ÿè®­ç»ƒæˆ–å¾®è°ƒç»™å®šæ¨¡å‹</li></ul></li></ul></li></ul><p>å› æ­¤<strong>Transformers</strong>ä¸æ˜¯ç¥ç»ç½‘ç»œæ„å»ºæ¨¡å—åŒ–çš„æ¨¡å—å·¥å…·ç®±ã€‚å¦‚æœè¦æ‰©å±•/æ„å»ºåº“ï¼Œåªéœ€ä½¿ç”¨å¸¸è§„çš„Python / PyTorch / TensorFlow / Kerasæ¨¡å—å¹¶ä»åº“çš„åŸºç±»ç»§æ‰¿å³å¯é‡ç”¨æ¨¡å‹åŠ è½½/ä¿å­˜ä¹‹ç±»çš„åŠŸèƒ½ã€‚</p><p>ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æ•´ä½“ä¸Šéƒ½å±äºä¸‹é¢çš„äº”ä¸ªç±»åˆ«ï¼š</p><h5 id="Decoders-or-autoregressive-models"><a href="#Decoders-or-autoregressive-models" class="headerlink" title="Decoders or autoregressive models"></a>Decoders or autoregressive models</h5><p>è‡ªå›å½’æ¨¡å‹åœ¨ç»å…¸è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼šçŒœæµ‹ä¸‹ä¸€ä¸ªå·²è¯»å®Œæ‰€æœ‰å…ˆå‰tokençš„tokenã€‚å®ƒä»¬å¯¹åº”äºtransformeræ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªå¥å­çš„é¡¶éƒ¨ä½¿ç”¨äº†ä¸€ä¸ªæ©ç ï¼Œä»¥ä¾¿æ³¨æ„å¤´åªèƒ½çœ‹åˆ°æ–‡æœ¬ä¸­çš„ä¹‹å‰å†…å®¹ï¼Œè€Œä¸èƒ½çœ‹åˆ°å…¶åçš„å†…å®¹ã€‚å°½ç®¡å¯ä»¥å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶åœ¨è®¸å¤šä»»åŠ¡ä¸Šå–å¾—å‡ºè‰²çš„ç»“æœï¼Œä½†å…¶æœ€è‡ªç„¶çš„åº”ç”¨æ˜¯æ–‡æœ¬ç”Ÿæˆã€‚æ­¤ç±»æ¨¡å‹çš„å…¸å‹ä¾‹å­æ˜¯GPT</p><h5 id="Encoders-or-autoencoding-models"><a href="#Encoders-or-autoencoding-models" class="headerlink" title="Encoders or autoencoding models"></a>Encoders or autoencoding models</h5><p>é€šè¿‡ä»¥æŸç§æ–¹å¼ç ´åè¾“å…¥tokenå¹¶å°è¯•é‡å»ºåŸå§‹å¥å­æ¥å¯¹è‡ªç¼–ç æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒä»¬ä¸transformerä¸­çš„çš„ç¼–ç å™¨ç›¸å¯¹åº”ï¼Œå› ä¸ºå®ƒä»¬æ— éœ€ä»»ä½•æ©ç å³å¯è®¿é—®å®Œæ•´çš„è¾“å…¥ã€‚è¿™äº›æ¨¡å‹é€šå¸¸å»ºç«‹æ•´ä¸ªå¥å­çš„åŒå‘è¡¨ç¤ºã€‚å¯ä»¥å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒå¹¶åœ¨è®¸å¤šä»»åŠ¡ï¼ˆä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆï¼‰ä¸Šå–å¾—å‡ºè‰²çš„ç»“æœï¼Œä½†æ˜¯å®ƒä»¬æœ€è‡ªç„¶çš„åº”ç”¨æ˜¯æ–‡æœ¬åˆ†ç±»æˆ–tokenåˆ†ç±»ï¼ˆæ¯”å¦‚è¯æ€§æ ‡æ³¨ï¼‰ã€‚æ­¤ç±»æ¨¡å‹çš„å…¸å‹ä¾‹å­æ˜¯BERT</p><p>è‡ªåŠ¨å›å½’æ¨¡å‹å’Œè‡ªåŠ¨ç¼–ç æ¨¡å‹ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«åœ¨äºæ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼ã€‚å› æ­¤ï¼Œç›¸åŒçš„ä½“ç³»ç»“æ„æ—¢å¯ä»¥ç”¨äºè‡ªåŠ¨å›å½’æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç”¨äºè‡ªåŠ¨ç¼–ç æ¨¡å‹.</p><h5 id="Sequence-to-Sequence-models"><a href="#Sequence-to-Sequence-models" class="headerlink" title="Sequence-to-Sequence models"></a>Sequence-to-Sequence models</h5><p>åºåˆ—åˆ°åºåˆ—æ¨¡å‹å°†transformersçš„ç¼–ç å™¨å’Œè§£ç å™¨åŒæ—¶ç”¨äºç¿»è¯‘ä»»åŠ¡æˆ–é€šè¿‡å°†å…¶ä»–ä»»åŠ¡è½¬æ¢ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜æ¥è®­ç»ƒå¾—åˆ°çš„ã€‚å¯ä»¥å°†å®ƒä»¬å¾®è°ƒæ¥é€‚åº”è®¸å¤šä»»åŠ¡ï¼ˆè¿™é‡Œåº”è¯¥æ˜¯è¯´æŠŠsequence to sequenceçš„é¢„è®­ç»ƒæ¨¡å‹çš„encoderæˆ–è€…decoderå•ç‹¬æŠ½å–å‡ºæ¥ï¼Œç„¶åç”¨æ³•å°±å’Œä¸Šé¢ä¸¤ç§æ¨¡å‹çš„ç”¨æ³•ä¸€è‡´ï¼‰ï¼Œä½†æœ€è‡ªç„¶çš„åº”ç”¨æ˜¯ç¿»è¯‘ï¼Œæ‘˜è¦å’Œé—®é¢˜è§£ç­”ã€‚T5æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­.</p><h5 id="Multimodal-models"><a href="#Multimodal-models" class="headerlink" title="Multimodal models"></a>Multimodal models</h5><p>å¤šæ¨¡æ€æ¨¡å‹å°†æ–‡æœ¬è¾“å…¥ä¸å…¶ä»–ç±»å‹çš„è¾“å…¥ï¼ˆä¾‹å¦‚å›¾åƒï¼‰æ··åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æ›´ç‰¹å®šäºç»™å®šä»»åŠ¡.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211125193439861.png" alt="image-20211125193439861"></p><p>è¿™ç§æ¨¡å‹æ²¡æœ‰æä¾›ä»»ä½•é¢„è®­ç»ƒæƒé‡åªæ˜¯å®šä¹‰äº†æ¨¡å‹çš„ç»“æ„.</p><h5 id="Retrieval-based-models"><a href="#Retrieval-based-models" class="headerlink" title="Retrieval-based models"></a>Retrieval-based models</h5><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211125193530708.png" alt="image-20211125193530708"></p><h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>  The most basic object in the ğŸ¤— Transformers library is the <code>pipeline()</code> function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:</p><p>  There are three main steps involved when you pass some text to a pipeline:</p><ol><li>The text is preprocessed into a format the model can understand.</li><li>The preprocessed inputs are passde to the model .</li><li>The predictions of the model are post-processed,so you can make sense of them.</li></ol><p>Some of the currently <strong>available pipelines</strong> are:</p><ul><li>feature-extraction(get the vector representation of a text)</li><li>file-mask</li><li>ner(named entity recogniton)</li><li>question-answering</li><li>sentiment-analysis</li><li>summarization</li><li>text-generation</li><li>translation</li><li>zero-shot-classification</li></ul><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><h5 id="Transformer-history"><a href="#Transformer-history" class="headerlink" title="Transformer history"></a>Transformer history</h5><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126210322407.png" alt="image-20211126210322407"></p><p>The <a href="https://arxiv.org/abs/1706.03762">Transformer architecture</a> was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:</p><ul><li><strong>June 2018</strong>: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results</li><li><strong>October 2018</strong>: <a href="https://arxiv.org/abs/1810.04805">BERT</a>, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)</li><li><strong>February 2019</strong>: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns</li><li><strong>October 2019</strong>: <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERTâ€™s performance</li><li><strong>October 2019</strong>: <a href="https://arxiv.org/abs/1910.13461">BART</a> and <a href="https://arxiv.org/abs/1910.10683">T5</a>, two large pretrained models using the same architecture as the original Transformer model (the first to do so)</li><li><strong>May 2020</strong>, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called <em>zero-shot learning</em>)</li></ul><p>This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:</p><ul><li>GPT-like (also called <em>auto-regressive</em> Transformer models)</li><li>BERT-like (also called <em>auto-encoding</em> Transformer models)</li><li>BART/T5-like (also called <em>sequence-to-sequence</em> Transformer models)</li></ul><p>We will dive into these families in more depth later on.</p><h5 id="Transformers-are-language-models"><a href="#Transformers-are-language-models" class="headerlink" title="Transformers are language models"></a>Transformers are language models</h5><p>All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as <em>language models</em>. This means they have been trained on large amounts of raw text in a self-supervised fashion. <strong>Self-supervised</strong> learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!</p><p>æ‰€æœ‰ä¸Šè¿°æåˆ°çš„æ¨¡å‹éƒ½å·²ç»è¢«è®­ç»ƒæˆäº†å¯¹åº”çš„è¯­è¨€æ¨¡å‹ã€‚è¿™ä¹Ÿå°±æ˜¯è¯´è¿™äº›æ¨¡å‹ä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼æ¥å—äº†å¤§é‡åŸå§‹æ–‡æœ¬çš„è®­ç»ƒã€‚è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§è®­ç»ƒç±»å‹ï¼Œç›®æ ‡æ˜¯æ ¹æ®æ¨¡å‹çš„è¾“å…¥è‡ªåŠ¨è®¡ç®—çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ä¸éœ€è¦äººç±»æ‰‹åŠ¨æ ‡è®°æ•°æ®ã€‚</p><hr><p>This type of model develops a statistical understanding of the language it has been trained on, but itâ€™s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called <em><strong>transfer learning</strong></em>. During this process, the model is fine-tuned in a supervised way â€” that is, using human-annotated labels â€” on a given task</p><p>è¿™ç§ç±»å‹çš„æ¨¡å‹å¯¹å…¶æ‰€è®­ç»ƒçš„è¯­è¨€æœ‰ç»Ÿè®¡ç†è§£ä½†å¯¹äºç‰¹å®šçš„å®é™…ä»»åŠ¡ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚å› æ­¤é€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹éƒ½ä¼šç»å†ä¸€ä¸ªç§°ä¸º<strong>è¿ç§»å­¦ä¹ </strong>çš„è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨ç»™å®šçš„ä»»åŠ¡ä¸Šä»¥æœ‰ç›‘ç£çš„æ–¹å¼è¿›è¡Œå¾®è°ƒâ€”â€”å³ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ•°æ®æ ‡ç­¾ã€‚</p><hr><p>An example of a task is predicting the next word in a sentence having read the <em>n</em> previous words. This is called *<strong>causal language modeling</strong> because the output depends on the past and present inputs, but not the future ones.  </p><p>ä»»åŠ¡çš„ä¸€ä¸ªå®ä¾‹å°±æ˜¯é¢„æµ‹å·²ç»é˜…è¯»çš„å‰nä¸ªå•è¯çš„å¥å­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è¿™ä¹Ÿè¢«ç§°ä¸º<strong>å› æœè¯­è¨€å»ºæ¨¡</strong>ï¼Œå› ä¸ºè¾“å‡ºå–å†³äºè¿‡å»å’Œç°åœ¨çš„è¾“å…¥è€Œä¸æ˜¯æœªæ¥çš„è¾“å…¥ã€‚</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126212440273.png" alt="image-20211126212440273"></p><p>Another example is <em>masked language modeling</em>, in which the model predicts a masked word in the sentence.</p><p>å¦ä¸€ä¸ªä¾‹å­æ˜¯æ©ç è¯­è¨€å»ºæ¨¡ï¼Œå…¶ä¸­æ¨¡å‹é¢„æµ‹å¥å­çš„æ©ç è¯ã€‚</p><p><img src="C:\Users\VrShadow\Desktop\work\IS_Lab\Web\image-20211126212540879.png" alt="image-20211126212540879"></p><h5 id="Transformer-are-big-models"><a href="#Transformer-are-big-models" class="headerlink" title="Transformer are big models"></a>Transformer are big models</h5><p>Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the modelsâ€™ sizes as well as the amount of data they are pretrained on.</p><p>é™¤äº†ä¸€äº›ç‰¹æ®Š(å¦‚ DistilBERT)å¤–ï¼Œå®ç°æ›´å¥½æ€§èƒ½çš„ä¸€èˆ¬ç­–ç•¥æ˜¯å¢åŠ æ¨¡å‹çš„å¤§å°ä»¥åŠé¢„è®­ç»ƒçš„æ•°æ®é‡ã€‚</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126213252541.png" alt="image-20211126213252541"></p><p>Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126213702715.png" alt="image-20211126213702715"></p><h5 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h5><p><em>Pretraining</em> is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.</p><p>é¢„è®­ç»ƒæ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„è¡Œä¸ºï¼šæƒé‡éšæœºåˆå§‹åŒ–,è®­ç»ƒåœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å¼€å§‹.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126222209255.png" alt="image-20211126222209255"></p><p>This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.</p><p><em>Fine-tuning</em>, on the other hand, is the training done <strong>after</strong> a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.</p><p>å¾®è°ƒæ˜¯åœ¨æ¨¡å‹é¢„è®­ç»ƒåè¿›è¡Œçš„è®­ç»ƒã€‚è¦è¿›è¡Œå¾®è°ƒï¼Œé¦–å…ˆéœ€è¦è·å¾—ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç„¶åä½¿ç”¨ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œé¢å¤–çš„è®­ç»ƒã€‚</p><ul><li>The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).</li><li>Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.</li><li>For the same reason, the amount of time and resources needed to get good results are much lower</li></ul><p>For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is â€œtransferred,â€ hence the term <em>transfer learning</em>.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126222820143.png" alt="image-20211126222820143"></p><p>Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.</p><p>å¾®è°ƒæ¨¡å‹å…·æœ‰æ›´ä½çš„æ—¶é—´,æ•°æ®,ç»æµå’Œç¯å¢ƒæˆæœ¬ã€‚è¿­ä»£ä¸åŒçš„å¾®è°ƒæ–¹æ¡ˆä¹Ÿæ›´å¿«æ›´å®¹æ˜“ï¼Œå› ä¸ºè®­ç»ƒæ¯”å®Œå…¨é¢„è®­ç»ƒçš„çº¦æŸæ›´å°‘ã€‚</p><p>This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model â€” one as close as possible to the task you have at hand â€” and fine-tune it.</p><h5 id="General-Transformer-Architecture"><a href="#General-Transformer-Architecture" class="headerlink" title="General Transformer Architecture"></a>General Transformer Architecture</h5><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126223610672.png" alt="image-20211126223610672"></p><p>The transformer is based on the attention mechanism.</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211126223727115.png" alt="image-20211126223727115"></p><p>The combination of the two parts is known as an encoder-decoder or a sequence-to-sequence transformer.</p><p>The model is primarily composed of two blocks:</p><ul><li><strong>Encoder (left)</strong>: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.</li></ul><p>ç¼–ç å™¨æ¥å—è¾“å…¥å¹¶æ„å»ºå®ƒçš„è¡¨ç¤º(å…¶ç‰¹å¾)ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ç»è¿‡ä¼˜åŒ–ä»¥ä»è¾“å…¥ä¸­è·å–ç†è§£ã€‚</p><ul><li><strong>Decoder (right)</strong>: The decoder uses the encoderâ€™s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.</li></ul><p>è§£ç å™¨ä½¿ç”¨ç¼–ç å™¨çš„è¡¨ç¤º(ç‰¹å¾)å’Œå…¶ä»–è¾“å…¥ç”Ÿæˆç›®æ ‡åºåˆ—ï¼Œè¿™æ„å‘³ç€æ¨¡å‹é’ˆå¯¹ç”Ÿæˆè¾“å‡ºè¿›è¡Œä¼˜åŒ–ã€‚</p><p>Each of these parts can be used independently, depending on the task:</p><ul><li><p><strong>Encoder-only models</strong>: Good for tasks that require understanding of the input, such as <strong>sentence classification and named entity recognition.</strong></p><p>é€‚ç”¨äº<strong>éœ€è¦ç†è§£è¾“å…¥çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¥å­åˆ†ç±»å’Œå‘½åå®ä½“è¯†åˆ«ã€‚</strong></p></li><li><p><strong>Decoder-only models</strong>: Good for generative tasks such as <strong>text generation</strong>.</p><p>é€‚ç”¨äº<strong>ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆ</strong></p></li><li><p><strong>Encoder-decoder models</strong> or <strong>sequence-to-sequence models</strong>: Good for generative tasks that require an input, such as <strong>translation or summarization</strong>.</p><p>é€‚ç”¨äº<strong>éœ€è¦è¾“å…¥çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘æˆ–è€…æ‘˜è¦ã€‚</strong> </p></li></ul><h5 id="Atention-layers"><a href="#Atention-layers" class="headerlink" title="Atention layers"></a>Atention layers</h5><p>A key feature of Transformer models is that they are built with special layers called <em>attention layers</em>. In fact, the title of the paper introducing the Transformer architecture was <a href="https://arxiv.org/abs/1706.03762">â€œAttention Is All You Needâ€</a>! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.</p><p>Transformeræ¨¡å‹çš„å…³é”®ç‚¹å°±æ˜¯ä»–ä»¬ç”±ç§°ä¸ºæ³¨æ„åŠ›å±‚çš„ç‰¹æ®Šå±‚æ„å»ºè€Œæˆã€‚äº‹å®ä¸Šï¼Œæå‡ºTransformeræ¶æ„çš„è®ºæ–‡æ˜¯â€Attention is all your needâ€ã€‚åé¢ä¼šè¯¦ç»†æ¢ç©¶Attenton layerçš„ç»†èŠ‚ã€‚</p><p>To put this into context, consider the task of translating text from English to French. Given the input â€œYou like this courseâ€, a translation model will need to also attend to the adjacent word â€œYouâ€ to get the proper translation for the word â€œlikeâ€, because in French the verb â€œlikeâ€ is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating â€œthisâ€ the model will also need to pay attention to the word â€œcourseâ€, because â€œthisâ€ translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of â€œthisâ€. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.</p><h5 id="The-original-architecture"><a href="#The-original-architecture" class="headerlink" title="The original architecture"></a>The original architecture</h5><p>The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language</p><p>Transformeræ¶æ„æœ€åˆæ˜¯ä¸ºäº†ç¿»è¯‘è€Œè®¾è®¡çš„,åœ¨è®­ç»ƒæœŸé—´ï¼Œç¼–ç å™¨æ¥å—æŸç§è¯­è¨€çš„è¾“å…¥å¥å­ï¼Œè€Œè§£ç å™¨æ¥å—æ‰€éœ€ç›®æ ‡è¯­è¨€çš„ç›¸åŒå¥å­ã€‚</p><ul><li><p>In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence).</p><p>åœ¨ç¼–ç å™¨ä¸­ï¼Œæ³¨æ„åŠ›å±‚å¯ä»¥ä½¿ç”¨å¥å­ä¸­çš„æ‰€æœ‰å•è¯(ç»™å®šå•è¯çš„ç¿»è¯‘å¯ä»¥ä¾èµ–äºå¥å­ä¸­å®ƒä¹‹åå’Œä¹‹å‰çš„å†…å®¹)</p></li><li><p>The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.</p><p>è§£ç å™¨æ˜¯æŒ‰ç…§é¡ºåºå·¥ä½œï¼Œåªèƒ½å…³æ³¨å·²ç»ç¿»è¯‘çš„å¥å­ä¸­å•è¯ã€‚æ¯”å¦‚ï¼Œå½“æˆ‘ä»¬é¢„æµ‹ç¿»è¯‘ç›®æ ‡ä¸­çš„å‰ä¸‰ä¸ªå•è¯æ—¶ï¼Œæˆ‘ä»¬å°†ä»–ä»¬æä¾›ç»™è§£ç å™¨ç„¶åè§£ç å™¨ä½¿ç”¨ç¼–ç å™¨çš„æ‰€æœ‰è¾“å…¥å°è¯•é¢„æµ‹ç¬¬å››ä¸ªå•è¯ã€‚</p></li></ul><p>Note that the the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.</p><p>æ³¨æ„ï¼Œè§£ç å™¨çš„ç¬¬ä¸€ä¸ªæ³¨æ„å±‚å…³æ³¨è§£ç å™¨æ‰€æœ‰è¿‡å»çš„è¾“å…¥ï¼Œä½†ç¬¬äºŒä¸ªæ³¨æ„å±‚ä½¿ç”¨ç¼–ç å™¨çš„è¾“å‡ºã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥è®¿é—®æ•´ä¸ªè¾“å…¥å¥å­ä»¥æœ€å¥½çš„é¢„æµ‹å½“å‰çš„å•è¯ï¼Œè¿™æ˜¯å¾ˆæœ‰ç”¨çš„å› ä¸ºä¸åŒçš„è¯­è¨€æœ‰ä¸åŒçš„è¯­æ³•è§„åˆ™ï¼ŒæŠŠå•è¯æ”¾åœ¨ä¸åŒçš„é¡ºåºæˆ–è€…å¥å­åé¢æä¾›çš„ä¸Šä¸‹æ–‡å¯èƒ½æœ‰åŠ©äºç¡®å®šä¸€ä¸ªç»™å®šå•è¯çš„æœ€ä½³ç¿»è¯‘ã€‚</p><p>The <em>attention mask</em> can also be used in the encoder/decoder to prevent the model from paying attention to some special words â€” for instance, the special padding word used to make all the inputs the same length when batching together sentences.</p><p><strong>attention mask</strong>ä¹Ÿå¯ä»¥è¿ç”¨åœ¨ç¼–ç /è§£ç ä¸­ï¼Œé˜²æ­¢æ¨¡å‹æ³¨æ„åˆ°æŸäº›ç‰¹æ®Šçš„å•è¯</p><h5 id="Architecture-amp-amp-Checkpoints"><a href="#Architecture-amp-amp-Checkpoints" class="headerlink" title="Architecture &amp;&amp; Checkpoints"></a>Architecture &amp;&amp; Checkpoints</h5><p>As we dive into Transformer models in this course, youâ€™ll see mentions of <em>architectures</em> and <em>checkpoints</em> as well as <em>models</em>. These terms all have slightly different meanings:</p><ul><li><strong>Architecture</strong>: This is the skeleton of the model â€” the definition of each layer and each operation that happens within the model.</li><li><strong>Checkpoints</strong>: These are the weights that will be loaded in a given architecture.</li><li><strong>Model</strong>: This is an umbrella term that isnâ€™t as precise as â€œarchitectureâ€ or â€œcheckpointâ€: it can mean both. This course will specify <em>architecture</em> or <em>checkpoint</em> when it matters to reduce ambiguity.</li></ul><p>For example, BERT is an architecture while <code>bert-base-cased</code>, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say â€œthe BERT modelâ€ and â€œthe <code>bert-base-cased</code> model.â€</p><h4 id="Encoder-models"><a href="#Encoder-models" class="headerlink" title="Encoder models"></a>Encoder models</h4><p>Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having â€œbi-directionalâ€ attention, and are often called <em>auto-encoding models</em>.</p><p>Encoder modelåªä½¿ç”¨the transformer modelçš„encoderéƒ¨åˆ†ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œattention layerséƒ½å¯ä»¥è®¿é—®åˆå§‹å¥å­çš„æ‰€æœ‰è¯ã€‚è¿™äº›æ¨¡å‹åŒvè¡Œè¢«æè¿°ä¸ºå…·æœ‰â€bi-directionalâ€ï¼Œé€šå¸¸è¢«ç§°ä¸ºè‡ªç¼–ç æ¨¡å‹ã€‚</p><p>The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.</p><p>è¿™äº›æ¨¡å‹çš„é¢„è®­ç»ƒé€šå¸¸å›´ç»•æŸç§æ–¹å¼ç ´åç»™å®šçš„å¥å­(ä¾‹å¦‚ï¼Œé€šè¿‡å±è”½å…¶ä¸­çš„éšæœºè¯)ï¼Œå¹¶è®©æ¨¡å‹æŸ¥æ‰¾æˆ–é‡æ„åˆå§‹å¥å­ã€‚</p><p>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.</p><p>ç¼–ç å™¨æ¨¡å‹æœ€é€‚åˆéœ€è¦ç†è§£å®Œæ•´å¥å­çš„ä»»åŠ¡ï¼Œä¾‹å¦‚sentence classification,ner(å‘½åå®ä½“è¯†åˆ«)(ä»¥åŠæ›´åŠ ä¸€èˆ¬çš„å•è¯åˆ†ç±»)å’Œeqaæå–å¼å›ç­”.</p><h5 id="Representatives-of-this-family-of-models-include"><a href="#Representatives-of-this-family-of-models-include" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul><li><a href="https://huggingface.co/transformers/model_doc/albert.html">ALBERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/bert.html">BERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a></li><li><a href="https://huggingface.co/transformers/model_doc/electra.html">ELECTRA</a></li><li><a href="https://huggingface.co/transformers/model_doc/roberta.html">RoBERTa</a></li></ul><h4 id="Decoder-models"><a href="#Decoder-models" class="headerlink" title="Decoder models"></a>Decoder models</h4><p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em>auto-regressive models</em>.</p><p>è§£ç å™¨æ¨¡å‹ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„è§£ç å™¨ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¯¹äºç»™å®šçš„å•è¯ï¼Œæ³¨æ„åŠ›å±‚åªèƒ½è®¿é—®ä½äºå¥å­ä¹‹å‰çš„å•è¯ã€‚è¿™äº›æ¨¡å‹é€šå¸¸è¢«ç§°ä¸ºè‡ªå›å½’æ¨¡å‹ã€‚</p><p>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p><p>è§£ç å™¨æ¨¡å‹çš„é¢„è®­ç»ƒé€šå¸¸å›´ç»•é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚</p><p>These models are best suited for tasks involving text generation.</p><p>è¿™äº›æ¨¡å‹æœ€é€‚åˆè®¾è®¡æ–‡æœ¬ç”Ÿæˆçš„ä»»åŠ¡.</p><h5 id="Representatives-of-this-family-of-models-include-1"><a href="#Representatives-of-this-family-of-models-include-1" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul><li><a href="https://huggingface.co/transformers/model_doc/ctrl.html">CTRL</a></li><li><a href="https://huggingface.co/transformers/model_doc/gpt.html">GPT</a></li><li><a href="https://huggingface.co/transformers/model_doc/gpt2.html">GPT-2</a></li><li><a href="https://huggingface.co/transformers/model_doc/transformerxl.html">Transformer XL</a></li></ul><h4 id="Seq-to-Seq-models"><a href="#Seq-to-Seq-models" class="headerlink" title="Seq-to-Seq models"></a>Seq-to-Seq models</h4><p>Encoder-decoder models (also called <em>sequence-to-sequence models</em>) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.</p><p>encoder-decoder models(ä¹Ÿç§°ä¸ºSeqtoSeq models)ä½¿ç”¨Transformerä½“ç³»ç»“æ„çš„æ‰€æœ‰éƒ¨åˆ†ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®¿é—®åˆå§‹å¥å­çš„æ¯ä¸ªå•è¯,è€Œè§£ç å™¨çš„æ³¨æ„åŠ›å±‚åªèƒ½è®¿é—®è¾“å…¥ä¸­æŸä¸ªå•è¯å‰é¢çš„å•è¯ã€‚</p><p>The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, <a href="https://huggingface.co/t5-base">T5</a> is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.</p><p>è¿™äº›æ¨¡å‹çš„é¢„è®­ç»ƒå¯ä»¥ä½¿ç”¨ç¼–ç å™¨/è§£ç å™¨æ¨¡å‹çš„ç›®æ ‡æ¥å®Œæˆ,ä½†é€šå¸¸æ¶‰åŠä¸€äº›æ›´å¤æ‚çš„ä¸œè¥¿ã€‚ä¾‹å¦‚ï¼ŒT5æ˜¯é€šè¿‡ä¸€ä¸ªæ©ç ç‰¹æ®Šè¯å–ä»£éšæœºæ–‡æœ¬è·¨åº¦(å¯ä»¥åŒ…å«å¤šä¸ªå•è¯)è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œç„¶åç›®æ ‡æ˜¯é¢„æµ‹è¿™ä¸ªæ©ç è¯å–ä»£çš„æ–‡æœ¬ã€‚</p><p>Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.</p><p>Seq-to-Seqæ¨¡å‹æœ€é€‚åˆæ ¹æ®ç»™å®šçš„è¾“å…¥ç”Ÿæˆæ–°å¥å­çš„ä»»åŠ¡ï¼Œæ¯”å¦‚æ‘˜è¦ï¼Œç¿»è¯‘æˆ–è€…ç”Ÿæˆå¼é—®é¢˜å›ç­”ã€‚</p><h5 id="Representatives-of-this-family-of-models-include-2"><a href="#Representatives-of-this-family-of-models-include-2" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul><li><a href="https://huggingface.co/transformers/model_doc/bart.html">BART</a></li><li><a href="https://huggingface.co/transformers/model_doc/mbart.html">mBART</a></li><li><a href="https://huggingface.co/transformers/model_doc/marian.html">Marian</a></li><li><a href="https://huggingface.co/transformers/model_doc/t5.html">T5</a></li></ul><h4 id="Bias-and-limitations"><a href="#Bias-and-limitations" class="headerlink" title="Bias and limitations"></a>Bias and limitations</h4><p>If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.</p><p>å¦‚æœæ‰“ç®—åœ¨productionä¸­ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹æˆ–è€…ç»è¿‡å¾®è°ƒçš„ç‰ˆæœ¬ï¼Œè¯·æ³¨æ„ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹æ˜¯æœ€å¼ºå¤§çš„å·¥å…·ä½†æ˜¯ä»–ä»¬ä¹Ÿæœ‰å±€é™æ€§ã€‚å…¶ä¸­æœ€å¤§çš„é—®é¢˜æ˜¯æ˜¯ä¸ºäº†èƒ½å¤Ÿå¯¹å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç ”ç©¶äººå‘˜ç»å¸¸æœé›†ä»–ä»¬èƒ½å¤Ÿæ‰¾åˆ°çš„æ‰€æœ‰å†…å®¹ï¼Œå¹¶ä¸”ä»äº’è”ç½‘ä¸Šå¯è·å¾—çš„ä¿¡æ¯ä¸­æŒ‘é€‰å‡ºæœ€å¥½çš„å’Œæœ€å·®çš„ã€‚</p><p>Exampleï¼š<strong>pipeline:fill-mask model:bert-base-uncased</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipelineunmasker <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"fill-mask"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">)</span>result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">"This man works as a [MASK]."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">"token_str"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">"This woman works as a [MASK]."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">"token_str"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">Some weights of the model checkpoint at bert<span class="token operator">-</span>base<span class="token operator">-</span>uncased were <span class="token operator">not</span> used when initializing BertForMaskedLM<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'cls.seq_relationship.bias'</span><span class="token punctuation">,</span> <span class="token string">'cls.seq_relationship.weight'</span><span class="token punctuation">]</span><span class="token operator">-</span> This IS expected <span class="token keyword">if</span> you are initializing BertForMaskedLM <span class="token keyword">from</span> the checkpoint of a model trained on another task <span class="token operator">or</span> <span class="token keyword">with</span> another architecture <span class="token punctuation">(</span>e<span class="token punctuation">.</span>g<span class="token punctuation">.</span> initializing a BertForSequenceClassification model <span class="token keyword">from</span> a BertForPreTraining model<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token operator">-</span> This IS NOT expected <span class="token keyword">if</span> you are initializing BertForMaskedLM <span class="token keyword">from</span> the checkpoint of a model that you expect to be exactly identical <span class="token punctuation">(</span>initializing a BertForSequenceClassification model <span class="token keyword">from</span> a BertForSequenceClassification model<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">[</span><span class="token string">'carpenter'</span><span class="token punctuation">,</span> <span class="token string">'lawyer'</span><span class="token punctuation">,</span> <span class="token string">'farmer'</span><span class="token punctuation">,</span> <span class="token string">'businessman'</span><span class="token punctuation">,</span> <span class="token string">'doctor'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'nurse'</span><span class="token punctuation">,</span> <span class="token string">'maid'</span><span class="token punctuation">,</span> <span class="token string">'teacher'</span><span class="token punctuation">,</span> <span class="token string">'waitress'</span><span class="token punctuation">,</span> <span class="token string">'prostitute'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender â€” and yes, prostitute ended up in the top 5 possibilities the model associates with â€œwomanâ€ and â€œwork.â€ This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (itâ€™s trained on the <a href="https://huggingface.co/datasets/wikipedia">English Wikipedia</a> and <a href="https://huggingface.co/datasets/bookcorpus">BookCorpus</a> datasets).</p><p>å¯ä»¥é€šè¿‡ä¾‹å­å‘ç°,å½“éœ€è¦å¡«å†™è¿™ä¸¤å¥è¯ä¸­è¢«å±è”½çš„å•è¯æ—¶,æ¨¡å‹åªç»™å‡ºäº†ä¸€ä¸ªä¸åˆ†æ€§åˆ«çš„ç­”æ¡ˆå¹¶æŒ‰ç…§â€˜manâ€™å’Œâ€˜workâ€™ç›¸å…³è”ï¼Œâ€˜womanâ€™å’Œâ€˜workâ€™ç›¸å…³è”å¯èƒ½æ€§æœ€å¤§çš„å‰5ç§å¯èƒ½æ€§ä¸­ã€‚</p><p>When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data wonâ€™t make this intrinsic bias disappear.</p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>In this chapter, you saw how to approach different NLP tasks using the high-level <code>pipeline()</code> function from ğŸ¤— Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser.</p><p>We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:</p><p>å…³é”®ç‚¹åœ¨äºï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´çš„transformeræ¶æ„ä¹Ÿå¯ä»¥ä½¿ç”¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå…·ä½“å´å†³äºä½ è¦è§£å†³çš„taskç‰¹ç‚¹ï¼Œä¸‹è¡¨è¿›è¡Œç®€å•æ€»ç»“ï¼š</p><table><thead><tr><th>Model</th><th>Examples</th><th>Tasks</th></tr></thead><tbody><tr><td>Encoder</td><td>ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa</td><td>Sentence classification, named entity recognition, extractive question answering</td></tr><tr><td>Decoder</td><td>CTRL, GPT, GPT-2, Transformer XL</td><td>Text generation</td></tr><tr><td>Encoder-decoder</td><td>BART, T5, Marian, mBART</td><td>Summarization, translation, generative question answering</td></tr></tbody></table><h4 id="Using-Transformers"><a href="#Using-Transformers" class="headerlink" title="Using Transformers"></a>Using Transformers</h4><ul><li><strong>Ease of use</strong>: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.</li><li><strong>Flexibility</strong>: At their core, all models are simple PyTorch <code>nn.Module</code> or TensorFlow <code>tf.keras.Model</code> classes and can be handled like any other models in their respective machine learning (ML) frameworks.</li><li><strong>Simplicity</strong>: Hardly any abstractions are made across the library. The â€œAll in one fileâ€ is a core concept: a modelâ€™s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.</li></ul>]]></content>
      
      
      <categories>
          
          <category> dialogue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dialogue_system</title>
      <link href="/year/11/21/Dialogue-system/"/>
      <url>/year/11/21/Dialogue-system/</url>
      
        <content type="html"><![CDATA[<p>NLPé¢†åŸŸæ¯”è¾ƒä¼ ç»Ÿå’Œæ ¸å¿ƒçš„taskæœ‰å¾ˆå¤š</p><p>ä¸‹é¢å…ˆä»‹ç»Chinese NLPçš„åŸºæœ¬ä»»åŠ¡:</p><h4 id="Co-reference-Resolution"><a href="#Co-reference-Resolution" class="headerlink" title="Co-reference Resolution"></a>Co-reference Resolution</h4><p>Background</p><hr><p>â€‹    Co-reference identifies pieces of text and links them with other pieces of text that refer to the same thing. Sometimes pieces of text have zero-length, where an overt pronoun or noun is omitted.</p><p>Example</p><hr><p>input:</p><pre><code>æˆ‘çš„å§å§ç»™æˆ‘å¥¹çš„ç‹—ã€‚å¾ˆå–œæ¬¢.</code></pre><p>output</p><pre><code>[æˆ‘]0çš„[å§å§]1ç»™[æˆ‘]0[å¥¹]1çš„[ç‹—]2ã€‚[]0å¾ˆå–œæ¬¢[]2.</code></pre><h6 id="Standard-Metrics"><a href="#Standard-Metrics" class="headerlink" title="Standard Metrics"></a>Standard Metrics</h6><p>Average of F1-scores returned by these three precison/recall metrics:</p><ul><li>MUC</li><li>B-cubed</li><li>Entity-based CEAF</li><li>BLANC</li><li>Link-Based Entity-Aware metric(LEA)</li></ul><h4 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a>Sentiment Analysis</h4><p>Background</p><hr><p>Sentiment Analysis detects identifies and extracts subjective information from text.<br>æƒ…æ„Ÿåˆ†ææ£€æµ‹è¯†åˆ«å¹¶ä»æ–‡æœ¬ä¸­æå–ä¸»è§‚ä¿¡æ¯.</p><hr><p>Example</p><hr><p>inputs:</p><pre><code>æ€»çš„æ„Ÿè§‰è¿™å°æœºå™¨è¿˜ä¸é”™ï¼Œå®ç”¨çš„æœ‰ï¼šé˜´é˜³å†æ˜¾ç¤ºï¼Œæ—¶é—´ä¸æ—¥æœŸå¿«é€Ÿè½¬æ¢, è®°äº‹æœ¬ç­‰ã€‚</code></pre><p>Output:</p><pre><code>Positive</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN</title>
      <link href="/year/11/06/Faster%20R-CNN/"/>
      <url>/year/11/06/Faster%20R-CNN/</url>
      
        <content type="html"><![CDATA[<h5 id="Perface"><a href="#Perface" class="headerlink" title="Perface:"></a>Perface:</h5><p>åœ¨ğŸ¦ŒåŒå­¦çš„æ„ŸæŸ“ä¸‹ï¼Œç¬”è€…æœ€è¿‘ä¹Ÿå­¦ä¹ äº†ç›®æ ‡æ£€æµ‹æ–¹å‘çš„ç›¸å…³å†…å®¹ï¼Œçœ‹çš„ç¬¬ä¸€ç¯‡è®ºæ–‡æ˜¯<a href="https://arxiv.org/abs/1504.08083#">Faster R-CNNï¼šTowards Rel-Time Objection Dection with Region Proposal Networks</a>ï¼Œé‡Œé¢æ¶‰åŠåˆ°å¾ˆå¤šå‰ç½®æ¨¡å‹éœ€è¦äº†è§£ç»“æ„ï¼Œåœ¨è¿™é‡Œåˆ†äº«ä¸€ç‚¹ç¬”è®°</p><h5 id="ç›®æ ‡æ£€æµ‹èƒŒæ™¯"><a href="#ç›®æ ‡æ£€æµ‹èƒŒæ™¯" class="headerlink" title="ç›®æ ‡æ£€æµ‹èƒŒæ™¯"></a>ç›®æ ‡æ£€æµ‹èƒŒæ™¯</h5><p>ç›®æ ‡æ£€æµ‹æ˜¯å¾ˆå¤šè®¡ç®—æœºè§†è§‰äººç‰©çš„åŸºç¡€ï¼Œç›®å‰ä¸»æµçš„ç›®æ ‡æ£€æµ‹çš„ç®—æ³•ä¸»è¦åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»</p><ol><li>one-stageæ£€æµ‹ç®—æ³•,è¿™ç§ç®—æ³•ç›´æ¥äº§ç”Ÿç‰©ä½“çš„ç±»åˆ«æ¦‚ç‡å’Œåæ ‡ä½ç½®,ä¸éœ€è¦ç›´æ¥äº§ç”Ÿå€™é€‰åŒºåŸŸ.æ¯”å¦‚è¯´YOLOå’ŒSSD</li><li>two-stageæ£€æµ‹ç®—æ³•,è¿™æ˜¯å°†æ£€æµ‹é—®é¢˜åˆ’åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µ,é¦–å…ˆæ˜¯äº§ç”Ÿå€™é€‰åŒºåŸŸ,ç„¶åå¯¹å€™é€‰åŒºåŸŸåˆ†ç±»;å…¸å‹ç®—æ³•æ˜¯R-CNNç³»åˆ—,faster rcnnå°±æ˜¯åŸºäº<strong>region proposal</strong>(å€™é€‰åŒºåŸŸ)</li></ol><h5 id="backbone-network"><a href="#backbone-network" class="headerlink" title="backbone network"></a>backbone network</h5><p><strong>Faster R-CNN</strong>ä½¿ç”¨çš„ä¸»å¹²ç½‘ç»œæ˜¯VGG-16,åœ¨è®ºæ–‡ä¸­ç§°ä¸»å¹²ç½‘ç»œæ—¶<strong>backbone network</strong>,ä¸»å¹²ç½‘ç»œå°±æ˜¯ç”¨æ¥<strong>feature extraction</strong>,å½“ç„¶è¿™ä¸ªä¸æ˜¯ä¸€æˆä¸å˜çš„,å¯ä»¥æ›¿æ¢,æ¯”å¦‚ç°åœ¨ä¹ŸåŒæ ·æµè¡Œä½¿ç”¨<strong>Resnet</strong>,å†å¦‚<strong>CornerNet</strong>ç®—æ³•ä¸­ä½¿ç”¨çš„backbone networkæ˜¯Hourglass Network.<br>å…³äºVGG-16å¯ä»¥å‚è€ƒ<a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/vgg.html">VGGä»‹ç»</a>,16çš„å«ä¹‰æ˜¯å«æœ‰å‚æ•°æœ‰16å±‚,åˆ†åˆ«æ˜¯13ä¸ªå·ç§¯å±‚+3ä¸ªå…¨è¿æ¥å±‚</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027195132710.png" alt="image-20211027195132710" style="zoom:65%;"><p>å›¾æ¥è‡ªç½‘ç»œ</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027195339689.png" alt="image-20211027195339689" style="zoom:60%;"><h5 id="Faster-R-CNNç®—æ³•æ­¥éª¤"><a href="#Faster-R-CNNç®—æ³•æ­¥éª¤" class="headerlink" title="Faster R-CNNç®—æ³•æ­¥éª¤"></a>Faster R-CNNç®—æ³•æ­¥éª¤</h5><p>è¿™éƒ¨åˆ†æ˜¯ä¸ºäº†ç†è§£Faster R-CNN,æ€»ä½“æè¿°ä¸‹ç®—æ³•çš„æ•´ä¸ªè¿‡ç¨‹ä»¥ä¾¿åæœŸåšç»†èŠ‚åˆ†æ</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027195754031.png" alt="image-20211027195754031" style="zoom:50%;"><p>å¤§è‡´æµç¨‹æ˜¯:å°†æ•´å¼ å›¾ç‰‡è¾“å…¥CNNå±‚,å¾—åˆ°feature map,å·ç§¯ç‰¹å¾è¾“å…¥åˆ°**RPN(Region Proposal Network)**å¾—åˆ°å€™é€‰æ¡†çš„ç‰¹å¾ä¿¡æ¯,å¯¹å€™é€‰æ¡†ä¸­æå–çš„ç‰¹å¾ä½¿ç”¨åˆ†ç±»å™¨åˆ¤åˆ«æ˜¯å¦å±äºä¸€ä¸ªç‰¹å®šç±»åˆ«,å¯¹äºå±äºæŸä¸€ç‰¹å¾çš„å€™é€‰æ¡†ç”¨å›å½’å™¨è¿›ä¸€æ­¥è°ƒæ•´å…¶ä½ç½®.</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027200749657.png" alt="image-20211027200749657" style="zoom:50%;"><p>Faster R-CNNå¯ä»¥çœ‹ä½œRPNå’ŒFast R-CNNæ¨¡å‹çš„ç»“åˆ,å³Faster R-CNN = RPN + Fast R-CNN.ä¸‹é¢ä»‹ç»æ¯ä¸€æ­¥éª¤çš„è¾“å…¥è¾“å‡ºçš„ç»†èŠ‚.</p><ul><li>é¦–å…ˆé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒå¾—åˆ°Conv layers(è¿™ä¸ªconv layerå®é™…ä¸Šå°±æ˜¯VGG-16)èƒ½å¤Ÿæ¥æ”¶æ•´å¼ å›¾ç‰‡å¹¶æå–ç‰¹å¾å›¾feature maps,è¿™ä¸ªfeature mapæ˜¯åœ¨convå±‚ä¹‹åè·å¾—çš„ç‰¹å¾.</li><li>feature mapè¢«å…±äº«ä¹‹åç”¨äºåç»­çš„RPNå’ŒRolæ± åŒ–å±‚<ul><li>BPNå±‚:BPNç½‘ç»œç”¨äºç”Ÿæˆregion proposals.è¯¥å±‚é€šè¿‡softmaxåˆ¤æ–­anchorså±äºå‰æ™¯(foreground)è¿˜æ˜¯èƒŒæ™¯(background),å†åˆ©ç”¨è¾¹æ¡†å›å½’ä¿®æ­£anchors,è·å¾—ç²¾ç¡®çš„proposals </li><li>RoI Poolingå±‚:è¯¥å±‚æ”¶é›†è¾“å…¥çš„feature mapå’Œproposalsç»¼åˆè¿™äº›ä¿¡æ¯æå–proposal feature map,è¿›å…¥åˆ°åé¢å¯åˆ©ç”¨å…¨è¿æ¥æ“ä½œå±‚è¿›è¡Œç›®æ ‡è¯†åˆ«å’Œå®šä½</li></ul></li><li>æœ€åçš„classifierä¼šå°†Roi Poolingå±‚å½¢æˆå›ºå®šå¤§å°çš„feature mapè¿›è¡Œå…¨è¿æ¥æ“ä½œ,åˆ©ç”¨softmaxè¿›è¡Œå…·ä½“ç±»åˆ«çš„åˆ†ç±»,åŒæ—¶åˆ©ç”¨L1 losså®Œæˆbounding box regressionå›å½’æ“ä½œè·å¾—ç‰©ä½“çš„å‡†ç¡®ä½ç½®</li></ul><h5 id="ç»†èŠ‚"><a href="#ç»†èŠ‚" class="headerlink" title="ç»†èŠ‚"></a>ç»†èŠ‚</h5><h6 id="1-RPN"><a href="#1-RPN" class="headerlink" title="1.RPN"></a>1.RPN</h6><p>ä¹‹å‰çš„R-CNNå’ŒFast R-CNNéƒ½æ˜¯é‡‡ç”¨å¯é€‰æ‹©æ€§æœç´¢(SS)æ¥äº§ç”Ÿå€™é€‰æ¡†çš„,ä½†æ˜¯è¿™ç§æ–¹æ³•ç‰¹åˆ«è€—æ—¶;Faster R-CNNæœ€å¤§çš„äº®ç‚¹æ˜¯æŠ›å¼ƒSS,é‡‡ç”¨RPNç”Ÿæˆå€™é€‰æ¡†.<br><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027204057233.png" alt="image-20211027204057233" style="zoom:67%;"></p><p>è¯´æ˜:</p><ol><li>Conv feature map:VGG-16ç½‘ç»œæœ€åä¸€ä¸ªå·ç§¯å±‚è¾“å‡ºçš„feature map</li><li>Sliding window:æ»‘åŠ¨çª—å£å®é™…ä¸Šå°±æ˜¯3*3çš„å·ç§¯æ ¸,æ»‘çª—åªè¦é€‰å–æ‰€æœ‰å¯èƒ½çš„åŒºåŸŸå¹¶æ²¡æœ‰é¢å¤–çš„ä½œç”¨</li><li>K anchor boxes:åœ¨æ¯ä¸ªsliding windowçš„ç‚¹ä¸Šåˆå§‹åŒ–çš„å‚è€ƒåŒºåŸŸ(è®ºæ–‡ä¸­k=9)å°±æ˜¯9ä¸ªçŸ©å½¢æ¡†</li><li>Intermediate layer:ä¸­é—´å±‚ï¼Œ256-dæ˜¯ä¸­é—´å±‚çš„ç»´åº¦(è®ºæ–‡ä¸­è°ç”¨ZFç½‘ç»œå°±æ˜¯256ç»´,VGGå°±æ˜¯512ç»´)</li><li>Cls layer:åˆ†ç±»å±‚,é¢„æµ‹proposalçš„anchorå¯¹åº”çš„proposalçš„(x,y,w,h)</li><li>2k scores:2kä¸ªåˆ†æ•°(18ä¸ª)</li><li>Reg layer:å›å½’å±‚,åˆ¤æ–­è¯¥proposalæ˜¯å‰æ™¯è¿˜æ˜¯èƒŒæ™¯</li><li>4k coordinates:4kåæ ‡(36ä¸ª)</li></ol><ul><li>RPNçš„è¾“å…¥æ˜¯å·ç§¯ç‰¹å¾å›¾,è¾“å‡ºæ˜¯å›¾ç‰‡ç”Ÿæˆçš„proposals,RPNé€šè¿‡ä¸€ä¸ªæ»‘åŠ¨çª—å£è¿æ¥åœ¨æœ€åä¸€ä¸ªå·ç§¯å±‚çš„feature mapä¸Š,ç”Ÿæˆä¸€ä¸ªé•¿åº¦256çš„å…¨è¿æ¥ç‰¹å¾</li><li>è¿™ä¸ªå…¨è¿æ¥å±‚ç‰¹å¾åˆ†åˆ«é€å…¥ä¸¤ä¸ªå…¨è¿æ¥å±‚ä¸€ä¸ªæ˜¯åˆ†ç±»å±‚,ç”¨äºåˆ†ç±»æ£€æµ‹;ä¸€ä¸ªæ˜¯å›å½’å±‚,ç”¨äºå›å½’;å¯¹äºæ¯ä¸ªæ»‘åŠ¨çª—å£ä½ç½®ä¸€èˆ¬è®¾ç½®k(è®ºæ–‡ä¸­k=9)ä¸ªä¸åŒå¤§å°æˆ–è€…æ¯”ä¾‹çš„anchorsè¿™æ„å‘³ç€æ¯ä¸ªæ»‘çª—è¦†ç›–çš„ä½ç½®å°±ä¼šé¢„æµ‹9å“¥å€™é€‰åŒºåŸŸ<br><strong>åˆ†ç±»å±‚</strong>:æ¯ä¸ªanchorè¾“å‡ºä¸¤ä¸ªé¢„æµ‹å€¼:anchoræ˜¯èƒŒæ™¯(background,éobject)çš„scoreå’Œanchoræ˜¯å‰æ™¯(foreground,object)çš„score<br><strong>å›å½’å±‚</strong>:è¾“å‡º4k(4*9=36)ä¸ªåæ ‡å€¼è¡¨ç¤ºæ¯ä¸ªå€™é€‰åŒºåŸŸçš„ä½ç½®(x,y,w,h)</li></ul><p>ä¹Ÿå°±æ˜¯è¯´æˆ‘ä¹ˆæ˜¯é€šè¿‡è¿™äº›ç‰¹å¾å›¾åº”ç”¨æ»‘åŠ¨çª—å£åŠ anchoræœºåˆ¶è¿›è¡Œç›®æ ‡åŒºåŸŸåˆ¤å®šå’Œåˆ†ç±»çš„,è¿™é‡Œçš„æ»‘çª—åŠ anchoræœºåˆ¶åŠŸèƒ½ç±»ä¼¼äºfast rcnnçš„selective searchç”Ÿæˆproposalsçš„ä½œç”¨,è€Œæˆ‘ä»¬æ˜¯é€šè¿‡RPNç”Ÿæˆproposals.RPNå°±æ˜¯ä¸€ä¸ªå·ç§¯å±‚ + relu +å·¦å³ä¸¤ä¸ªå±‚(cls layerå’Œreg layer)çš„å°å‹ç½‘ç»œ</p><h6 id="2-anchor"><a href="#2-anchor" class="headerlink" title="2.anchor"></a>2.anchor</h6><p>è®ºæ–‡å†…å®¹:The k proposals are parameterized relative to k reference boxes, which we call anchors;å¯ä»¥ç†è§£ä¸ºé”šç‚¹ä½äºä¹‹å‰è¯´çš„3 * 3çš„æ»‘çª—ä¸­å¿ƒå¤„,å°±æ˜¯å› ä¸ºæœ‰å¤šä¸ªanchor.è¿™9ä¸ªanchoræ˜¯ä½œè€…è®¾ç½®çš„,è®ºæ–‡ä¸­scale=[128,256,512],é•¿å®½æ¯”[1:1,1:2,2:1]æœ‰9ç§ï¼›è‡ªå·±å¯ä»¥æ ¹æ®ç›®æ ‡çš„ç‰¹ç‚¹åšå‡ºä¸åŒçš„è®¾è®¡;å¯¹äºä¸€å¹… w * hçš„feature mapä¸€å…±æœ‰w * h * kä¸ªé”šç‚¹.</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027213420072.png" alt="image-20211027213420072" style="zoom:50%;"><h6 id="3-VGGæå–ç‰¹å¾"><a href="#3-VGGæå–ç‰¹å¾" class="headerlink" title="3.VGGæå–ç‰¹å¾"></a>3.VGGæå–ç‰¹å¾</h6><p>VGGçš„ç½‘ç»œæµç¨‹å›¾:</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027213725540.png" alt="image-20211027213725540" style="zoom:67%;"><p>æ¯ä¸ªå·ç§¯å±‚åˆ©ç”¨å‰é¢ç½‘ç»œä¿¡æ¯ç”ŸæˆæŠ½è±¡æè¿°:<br>ç¬¬ä¸€å±‚å­¦ä¹ è¾¹ç¼˜edgesä¿¡æ¯ï¼›<br>ç¬¬äºŒå±‚:å­¦ä¹ è¾¹ç¼˜edgesä¸­å›¾æ¡ˆpatternsä»¥å­¦ä¹ æ›´åŠ å¤æ‚çš„å½¢çŠ¶ä¿¡æ¯ï¼›æœ€ç»ˆå¾—åˆ°å·ç§¯ç‰¹å¾å›¾å…¶ç©ºé—´ç»´åº¦(åˆ†è¾¨ç‡)æ¯”åŸå›¾å°äº†å¾ˆå¤šä½†æ›´æ·±ï¼›<br>ç‰¹å¾å›¾çš„widthå’Œheightç”±äºå·ç§¯å±‚é—´çš„æ± åŒ–å±‚è€Œé™ä½,è€Œdepthç”±äºå·ç§¯å±‚å­¦ä¹ çš„filtersæ•°é‡è€Œå¢åŠ .</p><h6 id="4-ROI-pooling"><a href="#4-ROI-pooling" class="headerlink" title="4.ROI pooling"></a>4.ROI pooling</h6><p>ROIå°±æ˜¯region of interestæŒ‡çš„æ˜¯æ„Ÿå…´è¶£åŒºåŸŸ;å¦‚æœæ˜¯åŸå›¾ï¼Œroiå°±æ˜¯ç›®æ ‡ï¼Œå¦‚æœæ˜¯featuremapï¼Œroiå°±æ˜¯ç‰¹å¾å›¾åƒç›®æ ‡çš„ç‰¹å¾äº†ï¼Œroiåœ¨è¿™é‡Œå°±æ˜¯ç»è¿‡RPNç½‘ç»œå¾—åˆ°çš„ï¼Œæ€»ä¹‹å°±æ˜¯ä¸€ä¸ªæ¡†ã€‚poolingå°±æ˜¯æ± åŒ–ã€‚æ‰€ä»¥ROI Poolingå°±æ˜¯Poolingçš„ä¸€ç§ï¼Œåªæ˜¯æ˜¯é’ˆå¯¹äºRoisçš„poolingæ“ä½œè€Œå·²ã€‚RPN å¤„ç†åï¼Œå¯ä»¥å¾—åˆ°ä¸€å †æ²¡æœ‰ class score çš„ object proposals.å¾…å¤„ç†é—®é¢˜ä¸ºï¼šå¦‚ä½•åˆ©ç”¨è¿™äº›proposalsåˆ†ç±».Roi poolingå±‚çš„è¿‡ç¨‹å°±æ˜¯ä¸ºäº†å°†ä¸åŒè¾“å…¥å°ºå¯¸çš„feature mapï¼ˆROIï¼‰æŠ å‡ºæ¥ï¼Œç„¶åresizeåˆ°ç»Ÿä¸€çš„å¤§å°.</p><p>ROI poolingå±‚çš„è¾“å…¥:</p><ol><li>ç‰¹å¾å›¾features map(è¿™ä¸ªç‰¹å¾å›¾å°±æ˜¯cnnå·ç§¯å‡ºæ¥ä»¥åç”¨äºå…±äº«çš„é‚£ä¸ªç‰¹å¾å›¾)</li><li>roiä¿¡æ¯:(å°±æ˜¯RPNç½‘ç»œçš„è¾“å‡º,ä¸€ä¸ªè¡¨ç¤ºæ‰€æœ‰ROIçš„N*5çŸ©é˜µ,Nè¡¨ç¤ºROIçš„æ•°ç›®;ç¬¬ä¸€åˆ—è¡¨ç¤ºå›¾åƒindex,å…¶ä½™å››åˆ—è¡¨ç¤ºå…¶ä½™çš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡,åæ ‡ä¿¡æ¯æ˜¯å¯¹åº”åŸå›¾ä¸­çš„ç»å¯¹åæ ‡)</li></ol><p>ROI poolingå±‚çš„è¿‡ç¨‹:</p><p>é¦–å…ˆå°†RPNä¸­å¾—åˆ°çš„åŸå›¾ä¸­roiä¿¡æ¯æ˜ å°„åˆ°feature mapä¸ŠæŒ‰åŸå›¾ä¸featuremapçš„æ¯”ä¾‹ç¼©å°roiåæ ‡å°±è¡Œäº†ï¼‰ï¼Œç„¶åç»è¿‡æœ€å¤§æ± åŒ–ï¼Œæ± åŒ–åˆ°å›ºå®šå¤§å°wÃ—hã€‚ä½†è¿™ä¸ªpoolingä¸æ˜¯ä¸€èˆ¬çš„Poolingï¼Œè€Œæ˜¯å°†åŒºåŸŸç­‰åˆ†ï¼Œç„¶åå–æ¯ä¸€å°å—çš„æœ€å¤§å€¼ï¼Œæœ€åæ‰èƒ½å¾—åˆ°å›ºå®šå°ºå¯¸çš„roiã€‚</p><p>ä¹Ÿå°±æ˜¯ï¼š</p><p>æ ¹æ®è¾“å…¥çš„imageï¼Œå°†Roiæ˜ å°„åˆ°feature mapå¯¹åº”çš„ä½ç½®ï¼›<br>å°†æ˜ å°„åçš„åŒºåŸŸåˆ’åˆ†ä¸ºç›¸åŒå¤§å°çš„sectionsï¼ˆsectionsæ•°é‡å’Œè¾“å‡ºçš„ç»´åº¦ç›¸åŒï¼‰ï¼›<br>å¯¹æ¯ä¸ªsectionè¿›è¡Œmax poolingæ“ä½œï¼›<br>ROI poolingå±‚çš„è¾“å‡ºï¼š</p><p>ç»“æœæ˜¯ï¼Œç”±ä¸€ç»„å¤§å°å„å¼‚çš„çŸ©å½¢ï¼Œæˆ‘ä»¬å¿«é€Ÿè·å–åˆ°å…·æœ‰å›ºå®šå¤§å°çš„ç›¸åº”ç‰¹å¾å›¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRoI pooling è¾“å‡ºçš„ç»´åº¦å®é™…ä¸Šå¹¶ä¸å–å†³äºè¾“å…¥ç‰¹å¾å›¾çš„å¤§å°ï¼Œä¹Ÿä¸å–å†³äºåŒºåŸŸææ¡ˆçš„å¤§å°ã€‚è¿™å®Œå…¨å–å†³äºæˆ‘ä»¬å°†åŒºåŸŸåˆ†æˆå‡ éƒ¨åˆ†ã€‚ä¹Ÿå°±æ˜¯ï¼Œbatchä¸ªroiçŸ©é˜µï¼Œæ¯ä¸€ä¸ªroiçŸ©é˜µä¸ºï¼šé€šé“æ•°xWxH,ä¹Ÿå°±æ˜¯ä»selective searchå¾—åˆ°batchä¸ªroiï¼Œç„¶åæ˜ å°„ä¸ºå›ºå®šå¤§å°ã€‚</p><h6 id="5-NMS"><a href="#5-NMS" class="headerlink" title="5.NMS"></a>5.NMS</h6><p>NMSï¼ˆNon Maximum Suppressionï¼Œéæå¤§å€¼æŠ‘åˆ¶ï¼‰ç”¨äºåæœŸçš„ç‰©ä½“å†—ä½™è¾¹ç•Œæ¡†å»é™¤ï¼Œå› ä¸ºç›®æ ‡æ£€æµ‹æœ€ç»ˆä¸€ä¸ªç›®æ ‡åªéœ€è¦ä¸€ä¸ªæ¡†ï¼Œæ‰€ä»¥è¦æŠŠå¤šä½™çš„æ¡†å¹²æ‰ï¼Œç•™ä¸‹æœ€å‡†ç¡®çš„é‚£ä¸ªã€‚</p><p>NMSçš„è¾“å…¥ï¼š</p><p>æ£€æµ‹åˆ°çš„Boxes(åŒä¸€ä¸ªç‰©ä½“å¯èƒ½è¢«æ£€æµ‹åˆ°å¾ˆå¤šBoxesï¼Œæ¯ä¸ªboxå‡æœ‰åˆ†ç±»score)</p><p>NMSçš„è¾“å‡ºï¼š</p><p>æœ€ä¼˜çš„Box.</p><h6 id="6-FC-layer"><a href="#6-FC-layer" class="headerlink" title="6.FC layer"></a>6.FC layer</h6><p>ç»è¿‡roi poolingå±‚ä¹‹åï¼Œbatch_size=300, proposal feature mapçš„å¤§å°æ˜¯7Ã—7,512-d,å¯¹ç‰¹å¾å›¾è¿›è¡Œå…¨è¿æ¥ï¼Œå‚ç…§ä¸‹å›¾ï¼Œæœ€ååŒæ ·åˆ©ç”¨Softmax Losså’ŒL1 Losså®Œæˆåˆ†ç±»å’Œå®šä½ã€‚</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211027220232527.png" alt="image-20211027220232527"></p><p>é€šè¿‡å…¨è¿æ¥å±‚ä¸softmaxè®¡ç®—æ¯ä¸ªregion proposalå…·ä½“å±äºå“ªä¸ªç±»åˆ«ï¼ˆå¦‚äººï¼Œé©¬ï¼Œè½¦ç­‰ï¼‰ï¼Œè¾“å‡ºcls_probæ¦‚ç‡å‘é‡ï¼›åŒæ—¶å†æ¬¡åˆ©ç”¨bounding box regressionè·å¾—æ¯ä¸ªregion proposalçš„ä½ç½®åç§»é‡bbox_predï¼Œç”¨äºå›å½’è·å¾—æ›´åŠ ç²¾ç¡®çš„ç›®æ ‡æ£€æµ‹æ¡†</p><p>å³ä»PoI Poolingè·å–åˆ°7x7å¤§å°çš„proposal feature mapsåï¼Œé€šè¿‡å…¨è¿æ¥ä¸»è¦åšäº†ï¼š</p><p>é€šè¿‡å…¨è¿æ¥å’Œsoftmaxå¯¹region proposalsè¿›è¡Œå…·ä½“ç±»åˆ«çš„åˆ†ç±»ï¼›</p><p>å†æ¬¡å¯¹region proposalsè¿›è¡Œbounding box regressionï¼Œè·å–æ›´é«˜ç²¾åº¦çš„rectangle boxã€‚</p><h5 id="ä¸»è¦éƒ¨åˆ†"><a href="#ä¸»è¦éƒ¨åˆ†" class="headerlink" title="ä¸»è¦éƒ¨åˆ†"></a>ä¸»è¦éƒ¨åˆ†</h5><p><strong>Faster</strong> <strong>RCNN</strong>å…¶å®å¯ä»¥åˆ†ä¸ºå››éƒ¨åˆ†ä¸»è¦å†…å®¹</p><h6 id="1-Conv-Layer"><a href="#1-Conv-Layer" class="headerlink" title="1.Conv Layer"></a>1.Conv Layer</h6><p>ä½œä¸ºä¸€ç§CNNç›®æ ‡æ£€æµ‹æ–¹æ³•,Faster RCNNé¦–å…ˆä½¿ç”¨ä¸€ç»„åŸºç¡€çš„cnn+relu+poolingå±‚æå–imageçš„feature map,è¿™ä¸ªfeature mapè¢«å…±äº«ç”¨ç”¨äºåç»­RPNå±‚å’Œå…¨è¿æ¥å±‚</p><h6 id="2-Region-Proposal-NetWorks"><a href="#2-Region-Proposal-NetWorks" class="headerlink" title="2.Region Proposal NetWorks"></a>2.Region Proposal NetWorks</h6><p>RPNç½‘ç»œç”¨äºç”Ÿæˆregion proposals,è¯¥å±‚é€šè¿‡softmaxåˆ¤æ–­anchorså±äºpositiveè¿˜æ˜¯negative,å†åˆ©ç”¨bounding</p><p>box regressionä¿®æ­£anchorsè·å¾—ç²¾ç¡®çš„proposals</p><h6 id="3-Roi-Pooling"><a href="#3-Roi-Pooling" class="headerlink" title="3.Roi Pooling"></a>3.Roi Pooling</h6><p>è¯¥å±‚æ‰‹æœºè¾“å…¥çš„feature mapå’Œproposals,ç»¼åˆè¿™äº›ä¿¡æ¯ä¹‹åæå–proposals,ç»¼åˆè¿™äº›ä¿¡æ¯æå–proposals feature mapsé€å…¥åç»­å…¨è¿æ¥å±‚åˆ¤å®šç›®æ ‡ç±»åˆ«</p><h6 id="4-Classfication"><a href="#4-Classfication" class="headerlink" title="4.Classfication"></a>4.Classfication</h6><p>åˆ©ç”¨proposals feature mapè®¡ç®—proposalsçš„ç±»åˆ«åŒæ—¶å†æ¬¡bounding box regressionè·å¾—æ£€æµ‹æ¡†æœ€ç»ˆçš„ç²¾ç¡®ä½ç½®</p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20211028212022996.png" alt="image-20211028212022996" style="zoom:67%;"><p>ä¸Šå›¾å±•ç¤ºäº†pythonç‰ˆæœ¬ä¸­çš„VGG16æ¨¡å‹ä¸­çš„faster rcnnçš„ç½‘ç»œç»“æ„å¯ä»¥æ¸…æ™°çš„çœ‹åˆ°è¯¥ç½‘ç»œå¯¹äºä¸€å¹…ä»»æ„å¤§å°çš„P*Qçš„å›¾åƒ:</p><ul><li>é¦–å…ˆå›ºå®šè‡³å¤§å°MÃ—Nç„¶åå°†MÃ—Nå›¾åƒé€å…¥ç½‘ç»œ;</li><li>è€ŒConv layer</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> RCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dian2021å¤ä»¤è¥</title>
      <link href="/year/11/06/002/"/>
      <url>/year/11/06/002/</url>
      
        <content type="html"><![CDATA[<p>æŠ¥åå‚åŠ å¤ä»¤è¥èµ·åˆæ˜¯æƒ³èŠ±æ—¶é—´ç ”ç©¶AIæœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸ç®—æ³•ï¼Œç„¶ååšäº†diançš„ä¸€ä¸ª<strong>lab</strong></p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dian </tag>
            
            <tag> æœºå™¨å­¦ä¹  </tag>
            
            <tag> CNNç®€æ˜“æ¡†æ¶æ­å»º </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SeqToSeq_Translation(Attention)</title>
      <link href="/year/09/27/seq2seq_translation_tutorial/"/>
      <url>/year/09/27/seq2seq_translation_tutorial/</url>
      
        <content type="html"><![CDATA[<pre class="line-numbers language-python"><code class="language-python"><span class="token operator">%</span>matplotlib inline<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</p><hr><p><strong>Author</strong>: <code>Sean Robertson &lt;https://github.com/spro/practical-pytorch&gt;</code>_</p><p>This is the third and final tutorial on doing â€œNLP From Scratchâ€, where we<br>write our own classes and functions to preprocess the data to do our NLP<br>modeling tasks. We hope after you complete this tutorial that youâ€™ll proceed to<br>learn how <code>torchtext</code> can handle much of this preprocessing for you in the<br>three tutorials immediately following this one.</p><p>In this project we will be teaching a neural network to translate from<br>French to English.</p><p>::</p><pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token punctuation">[</span><span class="token key atrule">KEY</span><span class="token punctuation">:</span> <span class="token punctuation">></span> input<span class="token punctuation">,</span> = target<span class="token punctuation">,</span> &lt; output<span class="token punctuation">]</span><span class="token punctuation">></span> il est en train de peindre un tableau .= he is painting a picture .&lt; he is painting a picture .<span class="token punctuation">></span> pourquoi ne pas essayer ce vin delicieux <span class="token punctuation">?</span>= why not try that delicious wine <span class="token punctuation">?</span>&lt; why not try that delicious wine <span class="token punctuation">?</span><span class="token punctuation">></span> elle n est pas poete mais romanciere .= she is not a poet but a novelist .&lt; she not not a poet but a novelist .<span class="token punctuation">></span> vous etes trop maigre .= you re too skinny .&lt; you re all alone .<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>â€¦ to varying degrees of success.</p><p>This is made possible by the simple but powerful idea of the <code>sequence to sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code>__, in which two<br>recurrent neural networks work together to transform one sequence to<br>another. An encoder network condenses an input sequence into a vector,<br>and a decoder network unfolds that vector into a new sequence.</p><p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p><p>To improve upon this model weâ€™ll use an <code>attention mechanism &lt;https://arxiv.org/abs/1409.0473&gt;</code>__, which lets the decoder<br>learn to focus over a specific range of the input sequence.</p><p><strong>Recommended Reading:</strong></p><p>I assume you have at least installed PyTorch, know Python, and<br>understand Tensors:</p><ul><li> <a href="https://pytorch.org/">https://pytorch.org/</a> For installation instructions</li><li> :doc:<code>/beginner/deep_learning_60min_blitz</code> to get started with PyTorch in general</li><li> :doc:<code>/beginner/pytorch_with_examples</code> for a wide and deep overview</li><li> :doc:<code>/beginner/former_torchies_tutorial</code> if you are former Lua Torch user</li></ul><p>It would also be useful to know about Sequence to Sequence networks and<br>how they work:</p><ul><li><code>Learning Phrase Representations using RNN Encoder-Decoder for  Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;</code>__</li><li><code>Sequence to Sequence Learning with Neural  Networks &lt;https://arxiv.org/abs/1409.3215&gt;</code>__</li><li><code>Neural Machine Translation by Jointly Learning to Align and  Translate &lt;https://arxiv.org/abs/1409.0473&gt;</code>__</li><li> <code>A Neural Conversational Model &lt;https://arxiv.org/abs/1506.05869&gt;</code>__</li></ul><p>You will also find the previous tutorials on<br>:doc:<code>/intermediate/char_rnn_classification_tutorial</code><br>and :doc:<code>/intermediate/char_rnn_generation_tutorial</code><br>helpful as those concepts are very similar to the Encoder and Decoder<br>models, respectively.</p><p><strong>Requirements</strong></p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> unicode_literals<span class="token punctuation">,</span> print_function<span class="token punctuation">,</span> division<span class="token keyword">from</span> io <span class="token keyword">import</span> open<span class="token keyword">import</span> unicodedata<span class="token keyword">import</span> string<span class="token keyword">import</span> re<span class="token keyword">import</span> random<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> Fdevice <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Loading-data-files"><a href="#Loading-data-files" class="headerlink" title="Loading data files"></a>Loading data files</h1><p>The data for this project is a set of many thousands of English to<br>French translation pairs.</p><p><code>This question on Open Data Stack Exchange &lt;https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages&gt;</code>__<br>pointed me to the open translation site <a href="https://tatoeba.org/">https://tatoeba.org/</a> which has<br>downloads available at <a href="https://tatoeba.org/eng/downloads">https://tatoeba.org/eng/downloads</a> - and better<br>yet, someone did the extra work of splitting language pairs into<br>individual text files here: <a href="https://www.manythings.org/anki/">https://www.manythings.org/anki/</a></p><p>The English to French pairs are too big to include in the repo, so<br>download to <code>data/eng-fra.txt</code> before continuing. The file is a tab<br>separated list of translation pairs:</p><p>::</p><pre><code>I am cold.    J'ai froid.</code></pre><p>.. Note::<br>   Download the data from<br>   <code>here &lt;https://download.pytorch.org/tutorial/data.zip&gt;</code>_<br>   and extract it to the current directory.</p><p>Similar to the character encoding used in the character-level RNN<br>tutorials, we will be representing each word in a language as a one-hot<br>vector, or giant vector of zeros except for a single one (at the index<br>of the word). Compared to the dozens of characters that might exist in a<br>language, there are many many more words, so the encoding vector is much<br>larger. We will however cheat a bit and trim the data to only use a few<br>thousand words per language.</p><p>.. figure:: /_static/img/seq-seq-images/word-encoding.png<br>   :alt:</p><p>Weâ€™ll need a unique index per word to use as the inputs and targets of<br>the networks later. To keep track of all this we will use a helper class<br>called <code>Lang</code> which has word â†’ index (<code>word2index</code>) and index â†’ word<br>(<code>index2word</code>) dictionaries, as well as a count of each word<br><code>word2count</code> which will be used to replace rare words later.</p><pre class="line-numbers language-python"><code class="language-python">SOS_token <span class="token operator">=</span> <span class="token number">0</span>EOS_token <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">class</span> <span class="token class-name">Lang</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>name <span class="token operator">=</span> name        self<span class="token punctuation">.</span>word2index <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>word2count <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>index2word <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"SOS"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"EOS"</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>n_words <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment" spellcheck="true"># Count SOS and EOS</span>    <span class="token keyword">def</span> <span class="token function">addSentence</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>addWord<span class="token punctuation">(</span>word<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">addWord</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>word2index<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>word2index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>n_words            self<span class="token punctuation">.</span>word2count<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            self<span class="token punctuation">.</span>index2word<span class="token punctuation">[</span>self<span class="token punctuation">.</span>n_words<span class="token punctuation">]</span> <span class="token operator">=</span> word            self<span class="token punctuation">.</span>n_words <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>word2count<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The files are all in Unicode, to simplify we will turn Unicode<br>characters to ASCII, make everything lowercase, and trim most<br>punctuation.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Turn a Unicode string to plain ASCII, thanks to</span><span class="token comment" spellcheck="true"># https://stackoverflow.com/a/518232/2809427</span><span class="token keyword">def</span> <span class="token function">unicodeToAscii</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>        c <span class="token keyword">for</span> c <span class="token keyword">in</span> unicodedata<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span><span class="token string">'NFD'</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>        <span class="token keyword">if</span> unicodedata<span class="token punctuation">.</span>category<span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token string">'Mn'</span>    <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Lowercase, trim, and remove non-letter characters</span><span class="token keyword">def</span> <span class="token function">normalizeString</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>    s <span class="token operator">=</span> unicodeToAscii<span class="token punctuation">(</span>s<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    s <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">"([.!?])"</span><span class="token punctuation">,</span> r<span class="token string">" \1"</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>    s <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">"[^a-zA-Z.!?]+"</span><span class="token punctuation">,</span> r<span class="token string">" "</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>    <span class="token keyword">return</span> s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>To read the data file we will split the file into lines, and then split<br>lines into pairs. The files are all English â†’ Other Language, so if we<br>want to translate from Other Language â†’ English I added the <code>reverse</code><br>flag to reverse the pairs.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">readLangs</span><span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Reading lines..."</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Read the file and split into lines</span>    lines <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'data/%s-%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">)</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\   <span class="token operator">//</span> ç›¸åº”æ•°æ®é›†ä¸‹è½½ä»¥åæ³¨æ„ç›¸å¯¹è·¯å¾„çš„è®¾ç½®        read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Split every line into pairs and normalize</span>    pairs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>normalizeString<span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> l<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> lines<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Reverse pairs, make Lang instances</span>    <span class="token keyword">if</span> reverse<span class="token punctuation">:</span>        pairs <span class="token operator">=</span> <span class="token punctuation">[</span>list<span class="token punctuation">(</span>reversed<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> pairs<span class="token punctuation">]</span>        input_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang2<span class="token punctuation">)</span>        output_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang1<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        input_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang1<span class="token punctuation">)</span>        output_lang <span class="token operator">=</span> Lang<span class="token punctuation">(</span>lang2<span class="token punctuation">)</span>    <span class="token keyword">return</span> input_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Since there are a <em>lot</em> of example sentences and we want to train<br>something quickly, weâ€™ll trim the data set to only relatively short and<br>simple sentences. Here the maximum length is 10 words (that includes<br>ending punctuation) and weâ€™re filtering to sentences that translate to<br>the form â€œI amâ€ or â€œHe isâ€ etc. (accounting for apostrophes replaced<br>earlier).</p><pre class="line-numbers language-python"><code class="language-python">MAX_LENGTH <span class="token operator">=</span> <span class="token number">10</span>eng_prefixes <span class="token operator">=</span> <span class="token punctuation">(</span>    <span class="token string">"i am "</span><span class="token punctuation">,</span> <span class="token string">"i m "</span><span class="token punctuation">,</span>    <span class="token string">"he is"</span><span class="token punctuation">,</span> <span class="token string">"he s "</span><span class="token punctuation">,</span>    <span class="token string">"she is"</span><span class="token punctuation">,</span> <span class="token string">"she s "</span><span class="token punctuation">,</span>    <span class="token string">"you are"</span><span class="token punctuation">,</span> <span class="token string">"you re "</span><span class="token punctuation">,</span>    <span class="token string">"we are"</span><span class="token punctuation">,</span> <span class="token string">"we re "</span><span class="token punctuation">,</span>    <span class="token string">"they are"</span><span class="token punctuation">,</span> <span class="token string">"they re "</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">filterPair</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> len<span class="token punctuation">(</span>p<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> MAX_LENGTH <span class="token operator">and</span> \        len<span class="token punctuation">(</span>p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> MAX_LENGTH <span class="token operator">and</span> \        p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>startswith<span class="token punctuation">(</span>eng_prefixes<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">filterPairs</span><span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>pair <span class="token keyword">for</span> pair <span class="token keyword">in</span> pairs <span class="token keyword">if</span> filterPair<span class="token punctuation">(</span>pair<span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The full process for preparing the data is:</p><ul><li> Read text file and split into lines, split lines into pairs</li><li> Normalize text, filter by length and content</li><li> Make word lists from sentences in pairs</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">prepareData</span><span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairs <span class="token operator">=</span> readLangs<span class="token punctuation">(</span>lang1<span class="token punctuation">,</span> lang2<span class="token punctuation">,</span> reverse<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Read %s sentence pairs"</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span>    pairs <span class="token operator">=</span> filterPairs<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Trimmed to %s sentence pairs"</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Counting words..."</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> pair <span class="token keyword">in</span> pairs<span class="token punctuation">:</span>        input_lang<span class="token punctuation">.</span>addSentence<span class="token punctuation">(</span>pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output_lang<span class="token punctuation">.</span>addSentence<span class="token punctuation">(</span>pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Counted words:"</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>input_lang<span class="token punctuation">.</span>name<span class="token punctuation">,</span> input_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>output_lang<span class="token punctuation">.</span>name<span class="token punctuation">,</span> output_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">)</span>    <span class="token keyword">return</span> input_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairsinput_lang<span class="token punctuation">,</span> output_lang<span class="token punctuation">,</span> pairs <span class="token operator">=</span> prepareData<span class="token punctuation">(</span><span class="token string">'eng'</span><span class="token punctuation">,</span> <span class="token string">'fra'</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>Reading lines...Read 135842 sentence pairsTrimmed to 10599 sentence pairsCounting words...Counted words:fra 4345eng 2803['je suis lessive et fatigue .', 'i m broke and tired .']</code></pre><h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>A Recurrent Neural Network, or RNN, is a network that operates on a<br>sequence and uses its own output as input for subsequent steps.</p><p>A <code>Sequence to Sequence network &lt;https://arxiv.org/abs/1409.3215&gt;</code><strong>, or<br>seq2seq network, or <code>Encoder Decoder network &lt;https://arxiv.org/pdf/1406.1078v3.pdf&gt;</code></strong>, is a model<br>consisting of two RNNs called the encoder and decoder. The encoder reads<br>an input sequence and outputs a single vector, and the decoder reads<br>that vector to produce an output sequence.</p><p>.. figure:: /_static/img/seq-seq-images/seq2seq.png<br>   :alt:</p><p>Unlike sequence prediction with a single RNN, where every input<br>corresponds to an output, the seq2seq model frees us from sequence<br>length and order, which makes it ideal for translation between two<br>languages.</p><p>Consider the sentence â€œJe ne suis pas le chat noirâ€ â†’ â€œI am not the<br>black catâ€. Most of the words in the input sentence have a direct<br>translation in the output sentence, but are in slightly different<br>orders, e.g. â€œchat noirâ€ and â€œblack catâ€. Because of the â€œne/pasâ€<br>construction there is also one more word in the input sentence. It would<br>be difficult to produce a correct translation directly from the sequence<br>of input words.</p><p>With a seq2seq model the encoder creates a single vector which, in the<br>ideal case, encodes the â€œmeaningâ€ of the input sequence into a single<br>vector â€” a single point in some N dimensional space of sentences.</p><h2 id="The-Encoder"><a href="#The-Encoder" class="headerlink" title="The Encoder"></a>The Encoder</h2><p>The encoder of a seq2seq network is a RNN that outputs some value for<br>every word from the input sentence. For every input word the encoder<br>outputs a vector and a hidden state, and uses the hidden state for the<br>next input word.</p><p>.. figure:: /_static/img/seq-seq-images/encoder-network.png<br>   :alt:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>EncoderRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> embedded        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>output<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="The-Decoder"><a href="#The-Decoder" class="headerlink" title="The Decoder"></a>The Decoder</h2><p>The decoder is another RNN that takes the encoder output vector(s) and<br>outputs a sequence of words to create the translation.</p><p>Simple Decoder<br>^^^^^^^^^^^^^^</p><p>In the simplest seq2seq decoder we use only last output of the encoder.<br>This last output is sometimes called the <em>context vector</em> as it encodes<br>context from the entire sequence. This context vector is used as the<br>initial hidden state of the decoder.</p><p>At every step of decoding, the decoder is given an input token and<br>hidden state. The initial input token is the start-of-string <code>&lt;SOS&gt;</code><br>token, and the first hidden state is the context vector (the encoderâ€™s<br>last hidden state).</p><p>.. figure:: /_static/img/seq-seq-images/decoder-network.png<br>   :alt:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>DecoderRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>output_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>output<span class="token punctuation">)</span>        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>output<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>I encourage you to train and observe the results of this model, but to<br>save space weâ€™ll be going straight for the gold and introducing the<br>Attention Mechanism.</p><p>Attention Decoder<br>^^^^^^^^^^^^^^^^^</p><p>If only the context vector is passed between the encoder and decoder,<br>that single vector carries the burden of encoding the entire sentence.</p><p>Attention allows the decoder network to â€œfocusâ€ on a different part of<br>the encoderâ€™s outputs for every step of the decoderâ€™s own outputs. First<br>we calculate a set of <em>attention weights</em>. These will be multiplied by<br>the encoder output vectors to create a weighted combination. The result<br>(called <code>attn_applied</code> in the code) should contain information about<br>that specific part of the input sequence, and thus help the decoder<br>choose the right output words.</p><p>.. figure:: <a href="https://i.imgur.com/1152PYf.png">https://i.imgur.com/1152PYf.png</a><br>   :alt:</p><p>Calculating the attention weights is done with another feed-forward<br>layer <code>attn</code>, using the decoderâ€™s input and hidden state as inputs.<br>Because there are sentences of all sizes in the training data, to<br>actually create and train this layer we have to choose a maximum<br>sentence length (input length, for encoder outputs) that it can apply<br>to. Sentences of the maximum length will use all the attention weights,<br>while shorter sentences will only use the first few.</p><p>.. figure:: /_static/img/seq-seq-images/attention-decoder-network.png<br>   :alt:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AttnDecoderRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> dropout_p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>AttnDecoderRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size        self<span class="token punctuation">.</span>output_size <span class="token operator">=</span> output_size        self<span class="token punctuation">.</span>dropout_p <span class="token operator">=</span> dropout_p        self<span class="token punctuation">.</span>max_length <span class="token operator">=</span> max_length        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>attn_combine <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout_p<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>        attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>embedded<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        attn_applied <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>attn_weights<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                 encoder_outputs<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>embedded<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> attn_applied<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_combine<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>output<span class="token punctuation">)</span>        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>gru<span class="token punctuation">(</span>output<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>        output <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden<span class="token punctuation">,</span> attn_weights    <span class="token keyword">def</span> <span class="token function">initHidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="alert alert-info"><h4>Note</h4><p>There are other forms of attention that work around the length  limitation by using a relative position approach. Read about "local  attention" in `Effective Approaches to Attention-based Neural Machine  Translation <https: arxiv.org="" abs="" 1508.04025="">`__.</https:></p></div><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><h2 id="Preparing-Training-Data"><a href="#Preparing-Training-Data" class="headerlink" title="Preparing Training Data"></a>Preparing Training Data</h2><p>To train, for each pair we will need an input tensor (indexes of the<br>words in the input sentence) and target tensor (indexes of the words in<br>the target sentence). While creating these vectors we will append the<br>EOS token to both sequences.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">indexesFromSentence</span><span class="token punctuation">(</span>lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>lang<span class="token punctuation">.</span>word2index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">tensorFromSentence</span><span class="token punctuation">(</span>lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    indexes <span class="token operator">=</span> indexesFromSentence<span class="token punctuation">(</span>lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span>    indexes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>EOS_token<span class="token punctuation">)</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>indexes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tensorsFromPair</span><span class="token punctuation">(</span>pair<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_tensor <span class="token operator">=</span> tensorFromSentence<span class="token punctuation">(</span>input_lang<span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    target_tensor <span class="token operator">=</span> tensorFromSentence<span class="token punctuation">(</span>output_lang<span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> target_tensor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>To train we run the input sentence through the encoder, and keep track<br>of every output and the latest hidden state. Then the decoder is given<br>the <code>&lt;SOS&gt;</code> token as its first input, and the last hidden state of the<br>encoder as its first hidden state.</p><p>â€œTeacher forcingâ€ is the concept of using the real target outputs as<br>each next input, instead of using the decoderâ€™s guess as the next input.<br>Using teacher forcing causes it to converge faster but <code>when the trained network is exploited, it may exhibit instability &lt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&amp;rep=rep1&amp;type=pdf&gt;</code>__.</p><p>You can observe outputs of teacher-forced networks that read with<br>coherent grammar but wander far from the correct translation -<br>intuitively it has learned to represent the output grammar and can â€œpick<br>upâ€ the meaning once the teacher tells it the first few words, but it<br>has not properly learned how to create the sentence from the translation<br>in the first place.</p><p>Because of the freedom PyTorchâ€™s autograd gives us, we can randomly<br>choose to use teacher forcing or not with a simple if statement. Turn<br><code>teacher_forcing_ratio</code> up to use more of it.</p><pre class="line-numbers language-python"><code class="language-python">teacher_forcing_ratio <span class="token operator">=</span> <span class="token number">0.5</span><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> target_tensor<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> encoder_optimizer<span class="token punctuation">,</span> decoder_optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> max_length<span class="token operator">=</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token punctuation">:</span>    encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">.</span>initHidden<span class="token punctuation">(</span><span class="token punctuation">)</span>    encoder_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    decoder_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    input_length <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    target_length <span class="token operator">=</span> target_tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    encoder_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> encoder<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    loss <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> ei <span class="token keyword">in</span> range<span class="token punctuation">(</span>input_length<span class="token punctuation">)</span><span class="token punctuation">:</span>        encoder_output<span class="token punctuation">,</span> encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">(</span>            input_tensor<span class="token punctuation">[</span>ei<span class="token punctuation">]</span><span class="token punctuation">,</span> encoder_hidden<span class="token punctuation">)</span>        encoder_outputs<span class="token punctuation">[</span>ei<span class="token punctuation">]</span> <span class="token operator">=</span> encoder_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>    decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>SOS_token<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    decoder_hidden <span class="token operator">=</span> encoder_hidden    use_teacher_forcing <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token keyword">if</span> random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> teacher_forcing_ratio <span class="token keyword">else</span> <span class="token boolean">False</span>    <span class="token keyword">if</span> use_teacher_forcing<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Teacher forcing: Feed the target as the next input</span>        <span class="token keyword">for</span> di <span class="token keyword">in</span> range<span class="token punctuation">(</span>target_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            decoder_output<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> decoder_attention <span class="token operator">=</span> decoder<span class="token punctuation">(</span>                decoder_input<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span>            loss <span class="token operator">+=</span> criterion<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> target_tensor<span class="token punctuation">[</span>di<span class="token punctuation">]</span><span class="token punctuation">)</span>            decoder_input <span class="token operator">=</span> target_tensor<span class="token punctuation">[</span>di<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># Teacher forcing</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Without teacher forcing: use its own predictions as the next input</span>        <span class="token keyword">for</span> di <span class="token keyword">in</span> range<span class="token punctuation">(</span>target_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            decoder_output<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> decoder_attention <span class="token operator">=</span> decoder<span class="token punctuation">(</span>                decoder_input<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span>            topv<span class="token punctuation">,</span> topi <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            decoder_input <span class="token operator">=</span> topi<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># detach from history as input</span>            loss <span class="token operator">+=</span> criterion<span class="token punctuation">(</span>decoder_output<span class="token punctuation">,</span> target_tensor<span class="token punctuation">[</span>di<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> decoder_input<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> EOS_token<span class="token punctuation">:</span>                <span class="token keyword">break</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    encoder_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    decoder_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> target_length<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This is a helper function to print time elapsed and estimated time<br>remaining given the current time and progress %.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> time<span class="token keyword">import</span> math<span class="token keyword">def</span> <span class="token function">asMinutes</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>    m <span class="token operator">=</span> math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>s <span class="token operator">/</span> <span class="token number">60</span><span class="token punctuation">)</span>    s <span class="token operator">-=</span> m <span class="token operator">*</span> <span class="token number">60</span>    <span class="token keyword">return</span> <span class="token string">'%dm %ds'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>m<span class="token punctuation">,</span> s<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">timeSince</span><span class="token punctuation">(</span>since<span class="token punctuation">,</span> percent<span class="token punctuation">)</span><span class="token punctuation">:</span>    now <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    s <span class="token operator">=</span> now <span class="token operator">-</span> since    es <span class="token operator">=</span> s <span class="token operator">/</span> <span class="token punctuation">(</span>percent<span class="token punctuation">)</span>    rs <span class="token operator">=</span> es <span class="token operator">-</span> s    <span class="token keyword">return</span> <span class="token string">'%s (- %s)'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>asMinutes<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">,</span> asMinutes<span class="token punctuation">(</span>rs<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The whole training process looks like this:</p><ul><li> Start a timer</li><li> Initialize optimizers and criterion</li><li> Create set of training pairs</li><li> Start empty losses array for plotting</li></ul><p>Then we call <code>train</code> many times and occasionally print the progress (%<br>of examples, time so far, estimated time) and average loss.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">trainIters</span><span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> n_iters<span class="token punctuation">,</span> print_every<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> plot_every<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    plot_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    print_loss_total <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true"># Reset every print_every</span>    plot_loss_total <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment" spellcheck="true"># Reset every plot_every</span>    encoder_optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>encoder<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>    decoder_optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>decoder<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>    training_pairs <span class="token operator">=</span> <span class="token punctuation">[</span>tensorsFromPair<span class="token punctuation">(</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span><span class="token punctuation">)</span>                      <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iters<span class="token punctuation">)</span><span class="token punctuation">]</span>    criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> iter <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> n_iters <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        training_pair <span class="token operator">=</span> training_pairs<span class="token punctuation">[</span>iter <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>        input_tensor <span class="token operator">=</span> training_pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        target_tensor <span class="token operator">=</span> training_pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> train<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> target_tensor<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span>                     decoder<span class="token punctuation">,</span> encoder_optimizer<span class="token punctuation">,</span> decoder_optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>        print_loss_total <span class="token operator">+=</span> loss        plot_loss_total <span class="token operator">+=</span> loss        <span class="token keyword">if</span> iter <span class="token operator">%</span> print_every <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            print_loss_avg <span class="token operator">=</span> print_loss_total <span class="token operator">/</span> print_every            print_loss_total <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'%s (%d %d%%) %.4f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>timeSince<span class="token punctuation">(</span>start<span class="token punctuation">,</span> iter <span class="token operator">/</span> n_iters<span class="token punctuation">)</span><span class="token punctuation">,</span>                                         iter<span class="token punctuation">,</span> iter <span class="token operator">/</span> n_iters <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> print_loss_avg<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> iter <span class="token operator">%</span> plot_every <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            plot_loss_avg <span class="token operator">=</span> plot_loss_total <span class="token operator">/</span> plot_every            plot_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>plot_loss_avg<span class="token punctuation">)</span>            plot_loss_total <span class="token operator">=</span> <span class="token number">0</span>    showPlot<span class="token punctuation">(</span>plot_losses<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Plotting-results"><a href="#Plotting-results" class="headerlink" title="Plotting results"></a>Plotting results</h2><p>Plotting is done with matplotlib, using the array of loss values<br><code>plot_losses</code> saved while training.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> pltplt<span class="token punctuation">.</span>switch_backend<span class="token punctuation">(</span><span class="token string">'agg'</span><span class="token punctuation">)</span><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>ticker <span class="token keyword">as</span> ticker<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">showPlot</span><span class="token punctuation">(</span>points<span class="token punctuation">)</span><span class="token punctuation">:</span>    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>    fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># this locator puts ticks at regular intervals</span>    loc <span class="token operator">=</span> ticker<span class="token punctuation">.</span>MultipleLocator<span class="token punctuation">(</span>base<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>loc<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>points<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Evaluation is mostly the same as training, but there are no targets so<br>we simply feed the decoderâ€™s predictions back to itself for each step.<br>Every time it predicts a word we add it to the output string, and if it<br>predicts the EOS token we stop there. We also store the decoderâ€™s<br>attention outputs for display later.</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> sentence<span class="token punctuation">,</span> max_length<span class="token operator">=</span>MAX_LENGTH<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        input_tensor <span class="token operator">=</span> tensorFromSentence<span class="token punctuation">(</span>input_lang<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span>        input_length <span class="token operator">=</span> input_tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">.</span>initHidden<span class="token punctuation">(</span><span class="token punctuation">)</span>        encoder_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> encoder<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>        <span class="token keyword">for</span> ei <span class="token keyword">in</span> range<span class="token punctuation">(</span>input_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            encoder_output<span class="token punctuation">,</span> encoder_hidden <span class="token operator">=</span> encoder<span class="token punctuation">(</span>input_tensor<span class="token punctuation">[</span>ei<span class="token punctuation">]</span><span class="token punctuation">,</span>                                                     encoder_hidden<span class="token punctuation">)</span>            encoder_outputs<span class="token punctuation">[</span>ei<span class="token punctuation">]</span> <span class="token operator">+=</span> encoder_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>        decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>SOS_token<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># SOS</span>        decoder_hidden <span class="token operator">=</span> encoder_hidden        decoded_words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        decoder_attentions <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>        <span class="token keyword">for</span> di <span class="token keyword">in</span> range<span class="token punctuation">(</span>max_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            decoder_output<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> decoder_attention <span class="token operator">=</span> decoder<span class="token punctuation">(</span>                decoder_input<span class="token punctuation">,</span> decoder_hidden<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span>            decoder_attentions<span class="token punctuation">[</span>di<span class="token punctuation">]</span> <span class="token operator">=</span> decoder_attention<span class="token punctuation">.</span>data            topv<span class="token punctuation">,</span> topi <span class="token operator">=</span> decoder_output<span class="token punctuation">.</span>data<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> topi<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> EOS_token<span class="token punctuation">:</span>                decoded_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">'&lt;EOS>'</span><span class="token punctuation">)</span>                <span class="token keyword">break</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                decoded_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output_lang<span class="token punctuation">.</span>index2word<span class="token punctuation">[</span>topi<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            decoder_input <span class="token operator">=</span> topi<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> decoded_words<span class="token punctuation">,</span> decoder_attentions<span class="token punctuation">[</span><span class="token punctuation">:</span>di <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>We can evaluate random sentences from the training set and print out the<br>input, target, and output to make some subjective quality judgements:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluateRandomly</span><span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>        pair <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>pairs<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'>'</span><span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output_words<span class="token punctuation">,</span> attentions <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        output_sentence <span class="token operator">=</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output_words<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'&lt;'</span><span class="token punctuation">,</span> output_sentence<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Training-and-Evaluating"><a href="#Training-and-Evaluating" class="headerlink" title="Training and Evaluating"></a>Training and Evaluating</h1><p>With all these helper functions in place (it looks like extra work, but<br>it makes it easier to run multiple experiments) we can actually<br>initialize a network and start training.</p><p>Remember that the input sentences were heavily filtered. For this small<br>dataset we can use relatively small networks of 256 hidden nodes and a<br>single GRU layer. After about 40 minutes on a MacBook CPU weâ€™ll get some<br>reasonable results.</p><p>.. Note::<br>   If you run this notebook you can train, interrupt the kernel,<br>   evaluate, and continue training later. Comment out the lines where the<br>   encoder and decoder are initialized and run <code>trainIters</code> again.</p><pre class="line-numbers language-python"><code class="language-python">hidden_size <span class="token operator">=</span> <span class="token number">20</span>encoder1 <span class="token operator">=</span> EncoderRNN<span class="token punctuation">(</span>input_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>attn_decoder1 <span class="token operator">=</span> AttnDecoderRNN<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_lang<span class="token punctuation">.</span>n_words<span class="token punctuation">,</span> dropout_p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>trainIters<span class="token punctuation">(</span>encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">,</span> <span class="token number">75000</span><span class="token punctuation">,</span> print_every<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>2m 24s (- 33m 37s) (5000 6%) 3.37745m 0s (- 32m 31s) (10000 13%) 2.86507m 36s (- 30m 26s) (15000 20%) 2.736810m 9s (- 27m 56s) (20000 26%) 2.655212m 38s (- 25m 17s) (25000 33%) 2.582015m 17s (- 22m 55s) (30000 40%) 2.538217m 50s (- 20m 23s) (35000 46%) 2.521520m 30s (- 17m 57s) (40000 53%) 2.459122m 51s (- 15m 14s) (45000 60%) 2.425925m 22s (- 12m 41s) (50000 66%) 2.362327m 50s (- 10m 7s) (55000 73%) 2.340230m 22s (- 7m 35s) (60000 80%) 2.308033m 3s (- 5m 5s) (65000 86%) 2.272235m 38s (- 2m 32s) (70000 93%) 2.276438m 20s (- 0m 0s) (75000 100%) 2.2802</code></pre><pre class="line-numbers language-python"><code class="language-python">evaluateRandomly<span class="token punctuation">(</span>encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>&gt; il s en met plein les poches .= he s raking it in .&lt; he s always to the . . &lt;EOS&gt;&gt; je suis en train de griller du poisson .= i am grilling fish .&lt; i m a . . &lt;EOS&gt;&gt; c est un mannequin .= she s a model .&lt; he s a nice . &lt;EOS&gt;&gt; il n est pas un saint .= he s no saint .&lt; he s not a . . &lt;EOS&gt;&gt; je n abandonne pas .= i m not giving up .&lt; i m not alone . &lt;EOS&gt;&gt; vous etes jeunes .= you re young .&lt; you re a . &lt;EOS&gt;&gt; il fait un super boulot .= he is doing a super job .&lt; he s a to of . . &lt;EOS&gt;&gt; tu es trop maigre .= you re too skinny .&lt; you re very busy . &lt;EOS&gt;&gt; je ne suis pas intimide .= i m not intimidated .&lt; i m not alone . &lt;EOS&gt;&gt; il est plus fort que moi .= he s stronger than me .&lt; he s not as . &lt;EOS&gt;</code></pre><p>â€‹    </p><h2 id="Visualizing-Attention"><a href="#Visualizing-Attention" class="headerlink" title="Visualizing Attention"></a>Visualizing Attention</h2><p>A useful property of the attention mechanism is its highly interpretable<br>outputs. Because it is used to weight specific encoder outputs of the<br>input sequence, we can imagine looking where the network is focused most<br>at each time step.</p><p>You could simply run <code>plt.matshow(attentions)</code> to see attention output<br>displayed as a matrix, with the columns being input steps and rows being<br>output steps:</p><pre class="line-numbers language-python"><code class="language-python">output_words<span class="token punctuation">,</span> attentions <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>    encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">,</span> <span class="token string">"je suis trop froid ."</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>matshow<span class="token punctuation">(</span>attentions<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre><code>&lt;matplotlib.image.AxesImage at 0x7f68d8ef77b8&gt;</code></pre><p>For a better viewing experience we will do the extra work of adding axes<br>and labels:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">showAttention</span><span class="token punctuation">(</span>input_sentence<span class="token punctuation">,</span> output_words<span class="token punctuation">,</span> attentions<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Set up figure with colorbar</span>    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>    ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span>    cax <span class="token operator">=</span> ax<span class="token punctuation">.</span>matshow<span class="token punctuation">(</span>attentions<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'bone'</span><span class="token punctuation">)</span>    fig<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span>cax<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Set up axes</span>    ax<span class="token punctuation">.</span>set_xticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span> <span class="token operator">+</span> input_sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span> <span class="token operator">+</span>                       <span class="token punctuation">[</span><span class="token string">'&lt;EOS>'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> rotation<span class="token operator">=</span><span class="token number">90</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>set_yticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span> <span class="token operator">+</span> output_words<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Show label at every tick</span>    ax<span class="token punctuation">.</span>xaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>ticker<span class="token punctuation">.</span>MultipleLocator<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    ax<span class="token punctuation">.</span>yaxis<span class="token punctuation">.</span>set_major_locator<span class="token punctuation">(</span>ticker<span class="token punctuation">.</span>MultipleLocator<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">evaluateAndShowAttention</span><span class="token punctuation">(</span>input_sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    output_words<span class="token punctuation">,</span> attentions <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>        encoder1<span class="token punctuation">,</span> attn_decoder1<span class="token punctuation">,</span> input_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input ='</span><span class="token punctuation">,</span> input_sentence<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'output ='</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>output_words<span class="token punctuation">)</span><span class="token punctuation">)</span>    showAttention<span class="token punctuation">(</span>input_sentence<span class="token punctuation">,</span> output_words<span class="token punctuation">,</span> attentions<span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"elle a cinq ans de moins que moi ."</span><span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"elle est trop petit ."</span><span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"je ne crains pas de mourir ."</span><span class="token punctuation">)</span>evaluateAndShowAttention<span class="token punctuation">(</span><span class="token string">"c est un jeune directeur plein de talent ."</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre><code>input = elle a cinq ans de moins que moi .output = she is always to of as me . &lt;EOS&gt;input = elle est trop petit .output = she is very nice . &lt;EOS&gt;input = je ne crains pas de mourir .output = i m not going to . . &lt;EOS&gt;input = c est un jeune directeur plein de talent .output = he s a a man . &lt;EOS&gt;</code></pre><h1 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h1><ul><li><p>Try with a different dataset</p><ul><li> Another language pair</li><li> Human â†’ Machine (e.g. IOT commands)</li><li> Chat â†’ Response</li><li> Question â†’ Answer</li></ul></li><li><p>Replace the embeddings with pre-trained word embeddings such as word2vec or<br> GloVe</p></li><li><p>Try with more layers, more hidden units, and more sentences. Compare<br> the training time and results.</p></li><li><p>If you use a translation file where pairs have two of the same phrase<br> (<code>I am test \t I am test</code>), you can use this as an autoencoder. Try<br> this:</p><ul><li> Train as an autoencoder</li><li> Save only the Encoder network</li><li> Train a new Decoder for translation from there</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> è®ºæ–‡å¤ç° </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sshè¿œç¨‹è¿æ¥æœåŠ¡å™¨</title>
      <link href="/year/09/21/004/"/>
      <url>/year/09/21/004/</url>
      
        <content type="html"><![CDATA[<p>â€‹    æœ¬æ–‡ç®€å•ä»‹ç»sshè¿œç¨‹è¿æ¥å®éªŒå®¤æœåŠ¡å™¨çš„æ­¥éª¤ï¼Œè¸©å‘è®¸å¤šï¼Œå¾ˆå¤šåŸç†ä¾æ—§ä¸æ‡‚ï¼Œä½†æœ€åå®ç°:</p><ul><li>è¿æ¥å®éªŒå®¤ç½‘ç»œåå†…ç½‘è¿æ¥è¿œç¨‹æœåŠ¡å™¨åŠŸèƒ½</li><li>é…ç½®æœ¬åœ°å¯†é’¥å’Œè¿œç¨‹æœåŠ¡å™¨ç”¨æˆ·å¯†é’¥ä½¿å…¶å…å¯†é’¥åŠŸèƒ½</li><li>2021/09/25æ›´æ–°ï¼šå®ç°å¤–ç½‘è¿æ¥å®éªŒå®¤æœåŠ¡å™¨çš„åŠŸèƒ½</li></ul><h5 id="1-å®ç°è¿œç¨‹è¿æ¥æœåŠ¡å™¨"><a href="#1-å®ç°è¿œç¨‹è¿æ¥æœåŠ¡å™¨" class="headerlink" title="1.å®ç°è¿œç¨‹è¿æ¥æœåŠ¡å™¨"></a>1.å®ç°è¿œç¨‹è¿æ¥æœåŠ¡å™¨</h5><h6 id="1-æœ¬åœ°æœåŠ¡å‡†å¤‡"><a href="#1-æœ¬åœ°æœåŠ¡å‡†å¤‡" class="headerlink" title="1.æœ¬åœ°æœåŠ¡å‡†å¤‡"></a>1.æœ¬åœ°æœåŠ¡å‡†å¤‡</h6><p>æœ¬åœ°ä¸»æœºä¸Šæ‰“å¼€windows terminalçª—å£(ç°åœ¨çš„windowsä¸€èˆ¬ä¼šè‡ªåŠ¨å®‰è£…openSSHå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯),æ‰§è¡Œå‘½ä»¤ï¼š</p><pre class="line-numbers language-none"><code class="language-none">ssh-keygen -t rsa<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>æ‰§è¡Œå‘½ä»¤åä¼šåœ¨<code>.ssh</code>æ–‡ä»¶ä¸‹ç”Ÿæˆä¸¤ä¸ªå¯†é’¥ï¼Œ<code>id_rsa</code>å’Œ<code>id_rsa.pub</code>ä¸€ä¸ªç§é’¥ä¸€ä¸ªå…¬é’¥;å®ç°è¿œç¨‹è¿æ¥æœåŠ¡å™¨å…³é”®æ˜¯æŠŠ<strong>å…¬é’¥</strong>å­˜æ”¾åˆ°è¿œç¨‹æœåŠ¡å™¨ç«¯</p><h6 id="2-é…ç½®æœ¬åœ°configæ–‡ä»¶"><a href="#2-é…ç½®æœ¬åœ°configæ–‡ä»¶" class="headerlink" title="2.é…ç½®æœ¬åœ°configæ–‡ä»¶"></a>2.é…ç½®æœ¬åœ°configæ–‡ä»¶</h6><h6 id="3-å°†æœ¬åœ°å…¬é’¥ä¸Šä¼ è‡³æœåŠ¡å™¨"><a href="#3-å°†æœ¬åœ°å…¬é’¥ä¸Šä¼ è‡³æœåŠ¡å™¨" class="headerlink" title="3.å°†æœ¬åœ°å…¬é’¥ä¸Šä¼ è‡³æœåŠ¡å™¨"></a>3.å°†æœ¬åœ°å…¬é’¥ä¸Šä¼ è‡³æœåŠ¡å™¨</h6><p>åœ¨windows terminalä¸‹æ‰§è¡Œå‘½ä»¤</p><pre class="line-numbers language-none"><code class="language-none">scp C:\Users\VrShadow\.ssh\id_rsa.pub XXX@192.168.0.75:\home\xxx\<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>æœ¬åœ°å…¬é’¥æ‹·è´è‡³è¿œç¨‹æœåŠ¡å™¨[æ³¨æ„xxxæ›´æ”¹ä¸ºè‡ªå·±åœ¨è¿œç¨‹æœåŠ¡å™¨ç«¯åˆ†é…çš„ç”¨æˆ·åï¼ï¼ï¼],æ­¤æ—¶ä¼ è¿‡æ¥çš„å…¬é’¥å­˜åœ¨<code>./home/xxx</code>ä¸‹</p><h6 id="4-å…¬é’¥å†™å…¥æˆæƒæ–‡ä»¶"><a href="#4-å…¬é’¥å†™å…¥æˆæƒæ–‡ä»¶" class="headerlink" title="4.å…¬é’¥å†™å…¥æˆæƒæ–‡ä»¶"></a>4.å…¬é’¥å†™å…¥æˆæƒæ–‡ä»¶</h6><p>åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šæ‰§è¡Œå‘½ä»¤</p><pre class="line-numbers language-none"><code class="language-none">touch ./.ssh/authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>è¿œç¨‹æœåŠ¡å™¨ç«¯è¿™è¾¹ç”¨çš„æ˜¯linuxç³»ç»Ÿï¼Œæ‰€ä»¥å…ˆè¦åˆ›å»ºæ–‡ä»¶<code>authorized_keys</code></p><p>å°†æœ¬åœ°ä¼ è¿‡æ¥çš„å¯†é’¥<strong>å†™å…¥authorizd_keys</strong>ä¸­</p><pre class="line-numbers language-none"><code class="language-none">cat ./home/xxx/id_rsa.pub >> ./.ssh/authorized_keys<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h5 id="2-vscodeå…å¯†ç™»å½•"><a href="#2-vscodeå…å¯†ç™»å½•" class="headerlink" title="2.vscodeå…å¯†ç™»å½•"></a>2.vscodeå…å¯†ç™»å½•</h5><h6 id="1-å‡†å¤‡æ’ä»¶SSH"><a href="#1-å‡†å¤‡æ’ä»¶SSH" class="headerlink" title="1.å‡†å¤‡æ’ä»¶SSH"></a>1.å‡†å¤‡æ’ä»¶SSH</h6><p>åœ¨æ’ä»¶é‡Œæœç´¢å®‰è£…å³å¯</p><h6 id="2-ä¿®æ”¹æœ¬åœ°configé…ç½®æ–‡ä»¶"><a href="#2-ä¿®æ”¹æœ¬åœ°configé…ç½®æ–‡ä»¶" class="headerlink" title="2.ä¿®æ”¹æœ¬åœ°configé…ç½®æ–‡ä»¶"></a>2.ä¿®æ”¹æœ¬åœ°configé…ç½®æ–‡ä»¶</h6><p>æœ¬åœ°configæ–‡ä»¶é‡Œé¢åŠ å…¥<strong>â€IdentifyFileâ€ â€C:\Users\VrShadow.ssh\id_rsaâ€</strong></p><p>å®Œæˆä¹‹åä¾§è¾¹å¯¼èˆªæ ä¼šå‡ºç°è¿œç¨‹èµ„æºç®¡ç†å™¨å›¾æ ‡ï¼Œç‚¹å‡»ä¹‹åé€‰æ‹©è¿œç¨‹æœåŠ¡å™¨æ—¶å¯¹åº”çš„ç«¯å£ä¸‹çš„åˆ†æ”¯ç”¨æˆ·ï¼Œç‚¹å‡»å°±ä¼šå¼€å¯æ–°çš„çª—å£(ç¬¬ä¸€æ¬¡ä¼šè®©ä½ é€‰æ‹©è¿œç¨‹æœåŠ¡å™¨çš„æ“ä½œç³»ç»Ÿ)ï¼Œä¹‹åå°±ä¼šè¿›å…¥å¯¹åº”ç”¨æˆ·ä¸‹çš„ç›®å½•è¿›è¡Œå·¥ä½œã€‚</p><h5 id="3-å¤–ç½‘è¿œç¨‹è¿æ¥"><a href="#3-å¤–ç½‘è¿œç¨‹è¿æ¥" class="headerlink" title="3.å¤–ç½‘è¿œç¨‹è¿æ¥"></a>3.å¤–ç½‘è¿œç¨‹è¿æ¥</h5><p>è‡ªå·±çš„æœ¬åœ°ç”¨æˆ·<code>.ssh</code>æ–‡ä»¶é‡Œé¢å·²ç»é…ç½®äº†<strong>config</strong>æ–‡ä»¶ï¼Œå·²ç»é…ç½®äº†jumpå†…ç½‘æƒé™<br>å‘½ä»¤è¡Œæ‰§è¡Œï¼š</p><pre class="line-numbers language-none"><code class="language-none">ssh jumpnone<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>æ‰§è¡Œåéœ€è¦è¿œç¨‹æœåŠ¡å™¨çš„å¯†ç ï¼š********</p><p>è¾“å…¥å¯†ç åè¿›è¡Œè¿œç¨‹è¿æ¥æ“ä½œ</p><pre class="line-numbers language-none"><code class="language-none">ssh username@host<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>å¦‚æœæœ¬åœ°ç”¨æˆ·åå’Œè¿œç¨‹ç”¨æˆ·åä¸€è‡´,ç™»å½•æ—¶å¯ä»¥çœç•¥ç”¨æˆ·å</p><pre class="line-numbers language-none"><code class="language-none">ssh host<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>SSHçš„é»˜è®¤ç«¯å£æ˜¯22,ä¹Ÿå°±æ˜¯è¯´ä½ çš„ç™»å½•è¯·æ±‚ä¼šé€è¿›è¿œç¨‹ä¸»æœºçš„22ç«¯å£.ä½¿ç”¨<code>-p</code>å‚æ•°å¯ä»¥ä¿®æ”¹ç«¯å£</p><pre><code>ssh -p 2222 user@host  # æ­¤æ¡å‘½ä»¤è¡¨ç¤ºsshç›´æ¥è¿æ¥è¿œç¨‹ä¸»æœºçš„2222ç«¯å£</code></pre><p>æˆ‘å†™çš„æ¯”è¾ƒç²—ç³™,(å·ä¸ªæ‡’)å¯ä»¥å‚è€ƒæˆ‘æœ‹å‹çš„blogï¼š</p><blockquote><p><a href="https://lry89757.github.io/2021/09/24/linux-bi-ji/">æœ‹å‹çš„åšå®¢</a></p></blockquote><p>ã€æœ€åçš„å®éªŒå°±æ˜¯å¦‚ä¸‹çš„æ•ˆæœ:</p><ul><li><p>è¿æ¥å¤–ç½‘çš„æƒ…å†µä¸‹</p><h5 id="è¿æ¥æœåŠ¡å™¨"><a href="#è¿æ¥æœåŠ¡å™¨" class="headerlink" title="è¿æ¥æœåŠ¡å™¨"></a>è¿æ¥æœåŠ¡å™¨</h5><pre><code># ä¸¤ç§æ–¹æ³•ï¼š(åœ¨å·²ç»é…ç½®å¥½confiå’Œå…¬é’¥æ–‡ä»¶ä¸‹å¹¶ä¸”æ‰“å¼€jumpè·³æ¿å’Œå¼€å¯â€œIdentiyfile"ä¸‹)ssh 43004   # å¿…é¡»è¦æ‰“å¼€è·³æ¿æƒé™,è€Œä¸”å›è½¦åæ¯æ¬¡éƒ½è¦è¾“å…¥æœåŠ¡å™¨æ‰€åœ¨å…¬ç½‘åœ°å€çš„å¯†ç             # å½“ç„¶åˆ†é…ç»™user@hostsçš„å¯†ç è¦çœ‹ä½ æ˜¯å¦æ³¨é‡Šäº†å…¬é’¥ssh gyf@192.168.0.75 # è¿™æ ·è®¿é—®åœ¨å†…ç½‘ä¸‹ä½¿ç”¨ï¼Œå½“ç„¶å†…ç½‘ä¸‹ä¹Ÿå¯ä»¥ä½¿ç”¨ssh 43004è¿æ¥æœåŠ¡å™¨</code></pre><ul><li><h5 id="å¤–ç½‘è®¿é—®"><a href="#å¤–ç½‘è®¿é—®" class="headerlink" title="å¤–ç½‘è®¿é—®"></a>å¤–ç½‘è®¿é—®</h5><ul><li><strong>ssh 43004</strong>:éœ€è¦è¾“å…¥æœåŠ¡å™¨æ‰€åœ¨å…¬ç½‘åœ°å€å¯†ç å’Œåˆ†é…ç»™ç”¨æˆ·çš„å¯†ç </li><li><strong>ssh user@host</strong>:ä¸èƒ½è¿æ¥æœåŠ¡å™¨</li></ul></li><li><h5 id="å†…ç½‘è®¿é—®"><a href="#å†…ç½‘è®¿é—®" class="headerlink" title="å†…ç½‘è®¿é—®"></a>å†…ç½‘è®¿é—®</h5><ul><li><strong>ssh 43004</strong>:ä»ç„¶éœ€è¦è¾“å…¥æœåŠ¡å™¨æ‰€åœ¨å…¬ç½‘åœ°å€å¯†ç ä½†æ˜¯ä¸ç”¨è¾“å…¥åˆ†é…ç»™ç”¨æˆ·çš„å¯†ç äº†</li><li><strong>ssh user@host</strong>:è¿™æ ·å…¬ç½‘å¯†ç å’Œåˆ†é…ç»™ç”¨æˆ·çš„å¯†ç éƒ½ä¸ç”¨è¾“å…¥äº†ç›´æ¥è¿æ¥</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> æ­å»ºç¯å¢ƒ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows terminal </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>è®ºæ–‡é˜…è¯»ä¸€:Attention Mechanism</title>
      <link href="/year/09/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB(%E4%B8%80)/"/>
      <url>/year/09/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB(%E4%B8%80)/</url>
      
        <content type="html"><![CDATA[<h4 id="Abstract-amp-amp-Introduction"><a href="#Abstract-amp-amp-Introduction" class="headerlink" title="Abstract &amp;&amp; Introduction"></a>Abstract &amp;&amp; Introduction</h4><p>â€‹    è¿™å‡ å¤©é˜…è¯»äº†ä¸€ç¯‡è¾ƒæ—©æå‡ºAttention machanismçš„è®ºæ–‡<a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>,è¿™ç¯‡è®ºæ–‡å°†æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨åœ¨ç¥ç»ç½‘ç»œç¿»è¯‘ä¸­ï¼Œè®ºæ–‡çš„æ€è·¯ä»ä¼ ç»ŸNMT(Neural Machine Translation)ç³»ç»Ÿçš„ç¼ºé™·è¯´èµ·ï¼Œé’ˆå¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œæœ€åè¿›è¡Œäº†å®šé‡å’Œå®šæ€§åˆ†æ.</p><p>â€‹    é¦–å…ˆæˆ‘ä»¬è¦äº†è§£ç»å…¸çš„Sea2Seqæ¨¡å‹æ˜¯å¦‚ä½•è¿›è¡Œç¿»è¯‘çš„ï¼šæ•´ä½“æ¨¡å‹é‡‡ç”¨Encoder-Decoderè¿›è¡Œåˆ†æï¼Œå°†è¾“å…¥çš„åºåˆ—ç»è¿‡Encoderå¤„ç†ï¼Œå‹ç¼©æˆä¸€ä¸ªFixed-length Vectorï¼›åœ¨Decoderé˜¶æ®µï¼Œå°†è¿™ä¸ªå‘é‡çš„ä¿¡æ¯è¿˜åŸæˆä¸€ä¸ªåºåˆ—å®Œæˆç¿»è¯‘ä»»åŠ¡ã€‚åŸºäºRNNçš„Seq2Seqæ¨¡å‹ä¸»è¦ç”±ä¸¤ç¯‡æ–‡ç« ä»‹ç»ï¼Œåªæ˜¯é‡‡ç”¨äº†ä¸åŒçš„RNNæ¨¡å‹ã€‚Ilya Sutskeverç­‰äºº2014å¹´åœ¨è®ºæ–‡ã€ŠSequence to Sequence Learning with Neural Networksã€‹ä¸­ä½¿ç”¨LSTMæ¥æ­å»ºSeq2Seqæ¨¡å‹ã€‚éšåï¼Œ2015å¹´ï¼ŒKyunghyun Choç­‰äººåœ¨è®ºæ–‡ã€ŠLearning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translationã€‹æå‡ºäº†åŸºäºGRUçš„Seq2Seqæ¨¡å‹ã€‚æƒ³è¦è§£å†³çš„ä¸»è¦é—®é¢˜å°±æ˜¯å¦‚ä½•æŠŠæœºå™¨ç¿»è¯‘ä¸­ï¼Œå˜é•¿çš„è¾“å…¥Xæ˜ å°„åˆ°ä¸€ä¸ªå˜é•¿è¾“å‡ºYã€‚è€Œè¿™ç¯‡è®ºæ–‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ä¹Ÿæ˜¯åŸºäº<code>encoder-decoder</code>çš„ï¼Œä¸ä¹‹å‰çš„<code>encoder-decoder</code>æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œæ¯æ¬¡åœ¨ç¿»è¯‘ä¸€ä¸ªå•è¯çš„æ—¶å€™ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨æœå¯»è¯¥å•è¯ä¸æºå¥å­å“ªäº›å•è¯æœ‰å…³è”ï¼Œå¹¶å°†è¿™ç§å…³è”çš„å¼ºåº¦è¿›è¡Œæ•°å­—åŒ–è¡¨ç¤º(åœ¨æ¨¡å‹ä¸­å°±æ˜¯æƒé‡)ï¼Œå¹¶ä¸”è®­ç»ƒå¾—å‡ºè¿™ç§æ–¹æ³•å¯ä»¥è§£å†³å¥å­ç¿»è¯‘ä¸å‡†çš„é—®é¢˜ã€‚</p><h4 id="ä¼ ç»ŸRNN"><a href="#ä¼ ç»ŸRNN" class="headerlink" title="ä¼ ç»ŸRNN"></a>ä¼ ç»ŸRNN</h4><p>â€‹    å¤§éƒ¨åˆ†çš„ç¥ç»æœºå™¨ç¿»è¯‘éƒ½æ˜¯åŸºäº<code>encoder-decoder</code>æ¡†æ¶çš„å¹¶ä¸”éƒ½ä¼šå°†æºè¯­è¨€å¥å­åºåˆ—å‹ç¼©æˆä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼Œç„¶åä¼ é€’ç»™decoderã€‚ä¼ ç»Ÿçš„RNN Encoder-Decoderæ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µæ—¶å€™ï¼Œä¼šä½¿æ¨¡å‹å»æœ€å¤§åŒ–æºè¯­è¨€ç¿»è¯‘æˆç›®æ ‡è¯­è¨€çš„æ¡ä»¶æ¦‚ç‡ã€‚å½“æ¨¡å‹è®­ç»ƒå¥½ä¹‹åï¼Œå½“ä»£ç¿»è¯‘çš„æºè¯­è¨€å¥å­æ”¾å…¥åˆ°æ¨¡å‹ä¸­åï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è®¡ç®—æœ€å¤§ç›®æ ‡å¥å­çš„æ¦‚ç‡å¹¶ä¸”å°†è¿™ä¸ªå¥å­å½“ä½œæ˜¯ç¿»è¯‘åçš„å¥å­ã€‚ç®€å•ä»‹ç»ä»¥ä¸‹ä¼ ç»Ÿçš„RNN:</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20210920180508294.png" alt="image-20210920180508294"></p><p>ä¸Šå›¾ä¸­<code>C</code>çš„å·¦ä¾§æ˜¯<code>Encoder</code>,å³ä¾§æ˜¯<code>Decoder</code>,â€Câ€æ˜¯å¾…ç¿»è¯‘è¯­å¥çš„è¯­ä¹‰ä¿¡æ¯ï¼›è¾“å…¥ä¸€ä¸ªå¥å­çš„æ—¶å€™ä¼šç»è¿‡Encoderï¼ŒEncoderè®²è¿™å¥è¯è¿›è¡Œç¼–ç ï¼ŒEncoderç”¨åˆ°çš„æ¨¡å‹æ˜¯RNNï¼Œç¼–ç ç»“æŸä»¥åå°†æœ€åä¸€ä¸ªæ—¶åˆ»RNNçš„éšå±‚çš„è¾“å‡ºå½“ä½œè¾“å…¥çš„è¿™å¥è¯çš„â€è¯­ä¹‰å‹ç¼©â€ã€‚ç„¶åè§£ç å™¨æ¯äº§ç”Ÿä¸€ä¸ªç¿»è¯‘åçš„è‹±æ–‡å•è¯çš„æ—¶å€™ï¼Œéƒ½ä¼šåˆ©ç”¨<strong>C</strong>å¹¶ä¸”è¿˜ä¼šæ¥å—è¾“å…¥tæ—¶åˆ»çš„ä¸Šä¸€ä¸ªéšè—å‘é‡<strong>s</strong>ã€‚è¿™ä¸ªæ—¶åˆ»çš„è¾“å‡ºç«¯å°±ä¼šäº§ç”Ÿç¬¬ä¸€ä¸ªå•è¯(è¿™é‡Œä½¿ç”¨äº†softmaxå‡½æ•°ï¼Œè¾“å‡ºå±‚æ˜¯ä¸€ä¸ªè¯å…¸å¤§å°ç»´åº¦çš„å‘é‡)ï¼Œå“ªä¸ªç»´åº¦çš„å€¼æœ€å¤§å°±å–å“ªä¸ªç»´åº¦æ‰€å¯¹åº”çš„å•è¯ã€‚å¤§å®¶å¯ä»¥æ˜ç™½çš„æ˜¯è®­ç»ƒé˜¶æ®µï¼ŒEncoderå’ŒDecoderä¸å¯èƒ½ç«‹é©¬äº§ç”Ÿç›®æ ‡å•è¯ï¼Œè€Œæ˜¯äº§ç”Ÿä¸€ä¸ªé¢„æµ‹ç»“æœï¼Œè®­ç»ƒçš„ç›®çš„å°±æ˜¯ä¸æ–­ä¼˜åŒ–å‚æ•°ã€‚</p><h4 id="Attentionæœºåˆ¶åŠ å…¥"><a href="#Attentionæœºåˆ¶åŠ å…¥" class="headerlink" title="Attentionæœºåˆ¶åŠ å…¥"></a>Attentionæœºåˆ¶åŠ å…¥</h4><p>æœ¬paperæå‡ºçš„æ¨¡å‹å«åš<strong>RNNsearch</strong>ï¼š</p><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20210920183014609.png" alt="image-20210920183014609"></p><p>â€‹    å›¾ä¸­çš„å³åŠéƒ¨åˆ†æ˜¯encoderï¼Œè¿™ä¸€éƒ¨åˆ†å’ŒRNNencæ¨¡å‹ä¸€æ ·ï¼Œé‡ç‚¹åœ¨decoderéƒ¨åˆ†å’Œä¼ ç»Ÿçš„ä¼šæœ‰å·¨å¤§çš„å·®åˆ«ï¼›åœ¨t=0æ—¶åˆ»ï¼Œdecoderçš„BiLSTMæ¥å—ä¸‰ä¸ªè¾“å…¥ï¼Œç¬¬ä¸€ä¸ªæ˜¯åˆå§‹çŠ¶æ€s0(è¿™ä¸ªæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œæ— è®ºæ˜¯è®­ç»ƒé˜¶æ®µè¿˜æ˜¯é¢„æµ‹é˜¶æ®µéƒ½æ˜¯éšæœº)ï¼›ç¬¬äºŒä¸ªè¾“å…¥æ¥æºäºemdeddingåçš„å‘é‡ï¼›ç¬¬ä¸‰ä¸ªè¾“å…¥æ¯”è¾ƒå¤æ‚ï¼Œä¹Ÿæ˜¯æ–°æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°ç‚¹</p><p>â€‹    é¦–å…ˆï¼Œéšæœºåˆ ç®—(è®¡ç®—æ–¹å¼æœ‰å¾ˆå¤šç§å¯ä»¥è‡ªå·±å®šä¹‰)ï¼Œå„è‡ªå¾—åˆ°ä¸€ä¸ªe1 ~ e6çš„å€¼ï¼Œå¯¹è¿™ä¸ª6ä¸ªå€¼è¿›è¡Œä¸€æ¬¡softmaxå¾—åˆ°Î±1 ~ Î±6ï¼Œå’Œæ˜¯1ï¼›å°†Î±1ï¼ŒÎ±2ï¼ŒÎ±3ï¼ŒÎ±4ï¼ŒÎ±5ï¼ŒÎ±6çœ‹ä½œæ˜¯s0å’Œh1 ~ h6çš„ç›¸ä¼¼åº¦ã€‚ç„¶åÎ±å’Œhå‘é‡åšä¸€æ¬¡å…ƒç´ ä¹˜ç§¯ï¼Œå¾—åˆ°çš„6ä¸ªå‘é‡åšä¸€æ¬¡å…ƒç´ çš„ç›¸åŠ å¾—åˆ°æœ€ç»ˆçš„å‘é‡ã€‚å°†è¿™ä¸ªå‘é‡å½“ä½œ0æ—¶åˆ»BiLSTMçš„ç¬¬ä¸‰ä¸ªè¾“å…¥ã€‚æ—¶åˆ»0ï¼ŒBiLSTMå°±ä¼šæœ‰ä¸€ä¸ªè¾“å‡ºï¼Œæ—¶åˆ»å˜ä¸º1ï¼Œæ¥ä¸‹æ¥çš„è¿‡ç¨‹ç»§ç»­å‘åè¿›è¡Œã€‚</p><p><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a>å®ç°Seq2Seq(Attention)åçš„æ¨¡å‹ï¼ŒåŸºæœ¬å®ç°äº†æ­¤ç¯‡è®ºæ–‡çš„åˆ›æ–°ç‚¹ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> è®ºæ–‡ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> æ³¨æ„åŠ›æœºåˆ¶ </tag>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>åŸºç¡€ç®—æ³•è€Œå·²</title>
      <link href="/year/09/17/003/"/>
      <url>/year/09/17/003/</url>
      
        <content type="html"><![CDATA[<p>ç®—æ³•å…¥é—¨ï¼šå•Šå“ˆç®—æ³• ç®—æ³•å›¾è§£ å¤§è¯æ•°æ®ç»“æ„</p><p>ç®—æ³•è¿›é˜¶ï¼šcf ç™½ä¹¦ ç´«ä¹¦ è“ä¹¦</p><h2 id="ç¬¬ä¸€ç« -åŸºç¡€ç®—æ³•"><a href="#ç¬¬ä¸€ç« -åŸºç¡€ç®—æ³•" class="headerlink" title="ç¬¬ä¸€ç«  åŸºç¡€ç®—æ³•"></a>ç¬¬ä¸€ç«  åŸºç¡€ç®—æ³•</h2><h3 id="åŸºç¡€ç®—æ³•-ä¸€"><a href="#åŸºç¡€ç®—æ³•-ä¸€" class="headerlink" title="åŸºç¡€ç®—æ³•(ä¸€)"></a>åŸºç¡€ç®—æ³•(ä¸€)</h3><h4 id="æ’åº"><a href="#æ’åº" class="headerlink" title="æ’åº"></a>æ’åº</h4><ul><li>å«ä¹‰:æ’åºæ˜¯æŒ‡å°†ä¸€ä¸ªæ— åºåºåˆ—æŒ‰ç…§æŸä¸ªè§„åˆ™è¿›è¡Œæœ‰åºæ’åˆ—(ä»¥ä¸‹æ’åºå‡å®ç°çš„æ˜¯ä»å°åˆ°å¤§æ’åº)</li></ul><h5 id="ç®€å•æ’åº"><a href="#ç®€å•æ’åº" class="headerlink" title="ç®€å•æ’åº"></a>ç®€å•æ’åº</h5><ul><li><p>å†’æ³¡æ’åºçš„æœ¬è´¨åœ¨äº==äº¤æ¢== ï¼Œå³æ¯æ¬¡é€šè¿‡äº¤æ¢çš„æ–¹å¼æŠŠå½“å‰å‰©ä½™å…ƒç´ çš„æœ€å¤§å€¼ç§»åŠ¨åˆ°ä¸€ç«¯</p><pre class="line-numbers language-c++"><code class="language-c++"># å†’æ³¡æ’åº(ä»¥ä¸‹å®ç°ä»å°åˆ°å¤§æ’åº)int a[n]={......};for(int i=1;i<n;i++){  //è¿›è¡Œn-1èºº//ç¬¬ièººï¼Œä»a[0]-a[n-i-1]æ¯ä¸€ä¸ªæ•°éƒ½è¦ä¸ä¸‹ä¸€ä¸ªæ•°è¿›è¡Œæ¯”è¾ƒï¼Œé‡åˆ°åé¢æ¯”è‡ªå·±è¾ƒå¤§çš„æ•°å°±äº¤æ¢ï¼Œå®ç°æ¯ä¸€è¶Ÿå‰©ä½™çš„æ•°a[0]-a[n-    i]çš„å†’æ³¡æ’åºï¼Œä½¿å½“å‰a[0]~a[n-i]ä¸­çš„æœ€å¤§çš„å…ƒç´ ç§»åŠ¨åˆ°æœ€åé¢çš„,a[n-i+1]-a[i]å·²ç»æ’å¥½åº    for(int j=0;j< n-i;j++){        if(a[j] > a[j+1]){  //å¦‚æœå·¦è¾¹çš„æ•°æ›´å¤§ï¼Œåˆ™a[j]ä¸a[j+1]äº¤æ¢            int temp = a[j];            a[j] = a[j+1];            a[j+1] = temp;        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>(ç®€å•)é€‰æ‹©æ’åºï¼š</p><pre class="line-numbers language-c++"><code class="language-c++"># é€‰æ‹©æ’åº(ä»¥ä¸‹å®ç°ä»å°åˆ°å¤§æ’åº)# ç®€å•é€‰æ‹©æ’åºæ˜¯æŒ‡å¯¹ä¸€ä¸ªåºåˆ—a[n]ä¸­çš„å…ƒç´ a[1]~a[n]ï¼Œä»¤iä»1~nè¿›è¡Œæšä¸¾ï¼Œè¿›è¡Œnè¶Ÿæ“ä½œï¼Œæ¯è¶Ÿä»å¾…æ’åºéƒ¨åˆ†[i,n]å…¶ä¸­é€‰æ‹©æœ€å°çš„å…ƒç´ ï¼Œä»¤å…¶ä¸å¾…æ’éƒ¨åˆ†çš„ç¬¬ä¸€ä¸ªå…ƒç´ a[i]è¿›è¡Œäº¤æ¢ï¼Œè¿™æ ·å…ƒç´ a[i]å°±ä¼šä¸å½“å‰åŒºé—´[1,i-1]å½¢æˆæ–°çš„æœ‰åºåŒºé—´[1,i],nè¶Ÿæ“ä½œä»¥åï¼Œå°±å½¢æˆæœ‰åºåŒºé—´int a[n]={......};void select_sort(){    for(int i=1;i<=n;i++){  //è¿›è¡Œnè¶Ÿæ“ä½œ        int k = i;        for(int j=i;j<=n;j++){  //é€‰å‡º[i,n]ä¸­æœ€å°å…ƒç´ çš„ä¸‹æ ‡ï¼Œå¹¶ä¸”å°†ä¸‹æ ‡è®°ä¸ºk            if(a[j]<a[k]){                k = j;            }        }        int temp = a[i];  //äº¤æ¢a[k]ä¸å½“å‰å¾…æ’åºåºåˆ—[i,n]çš„ç¬¬ä¸€ä¸ªå…ƒç´ a[i]        a[i] = a[k];        a[j] = temp;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>(ç›´æ¥)æ’å…¥æ’åºï¼š</p><pre class="line-numbers language-c++"><code class="language-c++"># ç›´æ¥æ’å…¥æ’åº# ç›´æ¥æ’å…¥æ’åºæ˜¯æŒ‡å¯¹åºåˆ—a[n]ä¸­çš„å…ƒç´ a[i]~a[n]ï¼Œiä»2~nè¿›è¡Œæšä¸¾ï¼Œè¿›è¡Œn-1è¶Ÿæ“ä½œã€‚å‡è®¾æŸä¸€è¶Ÿï¼Œåºåˆ—a[1]~a[i-1]å·²ç»æœ‰åºï¼Œé‚£ä¹ˆè¿™ä¸€æ¬¡å°±æ˜¯ä»èŒƒå›´[1,i-1]ä¸­å¯»æ‰¾æŸä¸ªä½ç½®j,ä½¿å¾—a[i]æ’å…¥åˆ°è¿™ä¸ªä½ç½®jåï¼Œæ­¤æ—¶a[j]~a[i-1]ä¼šè‡ªåŠ¨å‘åç§»åŠ¨ä¸€ä½åˆ°a[j+1]~a[i],èŒƒå›´a[1,i]æœ‰åºint a[n]={......};  //nä¸ºå…ƒç´ ä¸ªæ•°ï¼Œæ•°ç»„ä¸‹æ ‡ä¸º1~nvoid insert_sort(){    for(int i=2;i<=n;i++){  //è¿›è¡Œn-1è¶Ÿæ’åº        int temp = a[i],j = i; //tempä¸´æ—¶å­˜æ”¾a[i]        while(j>1 && temp<a[j-1]){            a[j] = a[j-1];            j--;        }        a[j] = temp;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="å¿«æ’"><a href="#å¿«æ’" class="headerlink" title="å¿«æ’"></a>å¿«æ’</h5><ul><li><p>å¿«æ’çš„ä¸»è¦æ€æƒ³æ˜¯åˆ†æ²»</p><pre class="line-numbers language-c++"><code class="language-c++">//å¿«æ’çš„æ—¶é—´å¤æ‚åº¦æ˜¯nlogn(è¿™é‡Œæ‰€æŒ‡çš„æ˜¯å¹³å‡å¤æ‚åº¦)#include <iostream>acwing 785å¿«é€Ÿæ’åºusing namespace std;const int N = 1e6+10;int n;int q[N];void quick_sort(int q[], int l, int r){    if (l >= r) return;    int i = l - 1, j = r + 1, x = q[l + r >> 1];  //xçš„å–å€¼å¯ä»¥å–åŒºé—´é‡Œé¢ä»»æ„ä¸€ä¸ª    while (i < j)    {        do i ++ ; while (q[i] < x);        do j -- ; while (q[j] > x);        if (i < j) swap(q[i], q[j]);    }    quick_sort(q, l, j); //å¯¹å·¦è¾¹çš„è¿›è¡Œå¿«æ’    quick_sort(q, j + 1, r); //å¯¹å³è¾¹è¿›è¡Œå¿«æ’}int main(){    scanf("%d",&n);    for(int i=0;i<n;i++){        scanf("%d",&q[i]);    }    quick_sort(q,0,n-1);        for(int i=0;i<n;i++){        printf("%d ",q[i]);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="å½’å¹¶æ’åº"><a href="#å½’å¹¶æ’åº" class="headerlink" title="å½’å¹¶æ’åº"></a>å½’å¹¶æ’åº</h5><ul><li><p>å½’å¹¶çš„ä¸»è¦æ€æƒ³ä¹Ÿæ˜¯åˆ†æ²»</p><pre class="line-numbers language-c++"><code class="language-c++">acwing787 å½’å¹¶æ’åº#include <iostream>using namespace std;const int N = 1e6+10;int n;int q[N];int tmp[N];void merge_sort(int q[],int l,int r){    if(l>=r) return ;        int mid = l+r >> 1;  //1ï¼šç¡®å®šåˆ†ç•Œç‚¹        merge_sort(q,l,mid);   //å¯¹å·¦å³ä¸¤è¾¹åˆ†åˆ«è¿›è¡Œå½’å¹¶æ’åº    merge_sort(q,mid+1,r);        // å°†å·¦å³ä¸¤è¾¹è¿›è¡Œå½’å¹¶æ’åºï¼ŒæŠŠä¸¤ä¸ªæœ‰åºçš„åºåˆ—æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œæ‹¼æ¥çš„æ–¹æ³•å°±æ˜¯å½’å¹¶    int k=0,i=l,j=mid+1;     while(i<=mid && j<= r){        if(q[i]<=q[j]) tmp[k++] = q[i++];        else tmp[k++] = q[j++];    }    while(i<=mid) tmp[k++]=q[i++];  //å¯¹äºq[l]~[mid]å’Œq[mid+1~r]ä¸¤ä¸ªåºåˆ—ï¼Œå¦‚æœå­˜åœ¨åºåˆ—æ²¡æœ‰å¾ªç¯ç»“æŸçš„è¯å°±ç›´æ¥                                åˆ°tmpåºåˆ—åé¢å³å¯    while(j<=r) tmp[k++]=q[j++];        for(i=l,j=0;i <= r;i++,j++) q[i] = tmp[j];}int main(){    scanf("%d",&n);    for(int i=0;i<n;i++) scanf("%d",&q[i]);        merge_sort(q,0,n-1);        for(int i=0;i<n;i++) printf("%d ",q[i]);        return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="äºŒåˆ†"><a href="#äºŒåˆ†" class="headerlink" title="äºŒåˆ†"></a>äºŒåˆ†</h4><h5 id="æ•´æ•°"><a href="#æ•´æ•°" class="headerlink" title="æ•´æ•°"></a>æ•´æ•°</h5><ul><li><p>æ•´æ•°äºŒåˆ†çš„æœ¬è´¨:æœ‰å•è°ƒæ€§çš„è¯ä¸€å®šå¯ä»¥äºŒåˆ†ï¼›ä½†æ˜¯èƒ½äºŒåˆ†çš„ä¸ä¸€å®šå…·æœ‰å•è°ƒæ€§<br>äºŒåˆ†çš„æœ¬è´¨æ˜¯å¯¹äºä¸€ä¸ªæ•´æ•°åŒºé—´ï¼Œæˆ‘ä»¬å…ˆå®šä¹‰ä¸€ä¸ªæ€§è´¨ï¼Œè¦æ‰¾åˆ°ä¸€ä¸ªä¸­é—´ç‚¹ï¼Œæ˜¯çš„åœ¨è¿™ä¸ªç‚¹çš„å³åŠè¾¹æ»¡è¶³è¿™ä¸ªæ€§è´¨ï¼Œå·¦åŠè¾¹ä¸æ»¡è¶³è¿™ä¸ªæ€§è´¨ï¼Œè¿™æ ·å°±å¯ä»¥æŠŠä¸€ä¸ªåŒºé—´ä¸€åˆ†ä¸ºäºŒï¼Œæ‰¾åˆ°è¿™ä¸ªè¾¹ç•Œ</p><pre class="line-numbers language-c++"><code class="language-c++">#1.æ‰¾åˆ°ä¸€ä¸ªä¸­é—´å€¼mid# if(check(mid)) true:midæ»¡è¶³è¿™ä¸ªæ€§è´¨  false:midä¸æ»¡è¶³è¿™ä¸ªæ€§è´¨# äºŒåˆ†çš„æ—¶å€™ä¸€å®šè¦ä¿è¯è¦å¯»æ‰¾çš„å€¼ä¸€å®šåœ¨ä¸æ–­ç¼©å°çš„é‚£ä¸ªåŒºé—´é‡Œé¢ï¼Œå½“åŒºé—´çš„é•¿åº¦ä¸º1çš„æ—¶å€™å°±ä»£è¡¨æ‰¾åˆ°ç­”æ¡ˆ#acwing789:æ•°çš„èŒƒå›´#include <iostream>#include <algorithm>#include <cstring>using namespace std;const int N=100010;int a,b;int q[N];int main(){    scanf("%d %d",&a,&b);    for(int i=0;i<a;i++) scanf("%d",&q[i]);        while(b--){        int x;        scanf("%d",&x);                int l=0,r=a-1;        while(l<r){            int mid= l+r >> 1;            if(q[mid]>=x) r=mid;            else l=mid+1;        }                if(q[l]!=x) cout<<"-1 -1"<<endl;  // è¿™ä¸ªè¡¨ç¤ºè¦å¯»æ‰¾çš„é‚£ä¸ªå€¼ä¸åœ¨åŒºé—´é‡Œé¢ï¼Œæ­¤æ—¶q[l]çš„å€¼æ˜¯ç¬¬ä¸€ä¸ªæ»¡è¶³å¤§äºxçš„æ•°        else{            cout<<l<<' ';                        int l=0,r=a-1;            while(l<r){                int mid= l+r+1 >> 1;                if(q[mid]<=x) l=mid;                else r=mid-1;            }            cout<<l        }    }    return 0;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h5 id="æµ®ç‚¹æ•°"><a href="#æµ®ç‚¹æ•°" class="headerlink" title="æµ®ç‚¹æ•°"></a>æµ®ç‚¹æ•°</h5><ul><li><p>æµ®ç‚¹æ•°äºŒåˆ†:æœ¬è´¨ä¸Šä¹Ÿæ˜¯å¯»æ‰¾è¾¹ç•Œï¼Œæ»¡è¶³å·¦åŠè¾¹æ»¡è¶³æ€§è´¨ï¼Œå³åŠè¾¹ä¸æ»¡è¶³æ€§è´¨ï¼ŒçŸ¥é“</p></li><li><pre class="line-numbers language-c++"><code class="language-c++"># ä¾‹å­:ç®—å¹³æ–¹æ ¹#include <iostream>using namespace std;int mian(){    double x;    cin>>x;        double l=0,r=x;    double mid = (l+r)/2;    while(r-l > 1e-8){        if(mid*mid>=x)  r=mid;        else l=mid;    }        printf("%lf",&l);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> ç®—æ³• </category>
          
      </categories>
      
      
        <tags>
            
            <tag> æ’åºç®—æ³• </tag>
            
            <tag> acwing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ç¬¬ä¸€ç¯‡ï¼Œè¡¨è¾¾ç‚¹çœ‹æ³•å§</title>
      <link href="/year/09/15/001/"/>
      <url>/year/09/15/001/</url>
      
        <content type="html"><![CDATA[<p><span class="github-emoji"><span>ğŸ˜„</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>ğŸ˜†</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>ğŸ˜†</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>å€¼å¾—è®°å½•ä¸€ä¸‹</p><p>è¿™æ˜¯æˆ‘çš„ç¬¬ä¸€ç¯‡åšæ–‡ï¼ŒèŠ±äº†å¾ˆé•¿æ—¶é—´æ¥è¿›è¡Œæ“ä½œï¼Œåœ¨ç½‘ä¸Šæœç´¢çš„æ•™ç¨‹å‚å·®ä¸é½ï¼Œä¹ŸåŒæ ·ä¼šå‡ºç°å„ç§å„æ ·çš„é—®é¢˜ï¼Œæ¯”å¦‚nodejsç‰ˆæœ¬è¿‡é«˜ä¸hexoä¸å…¼å®¹é—®é¢˜ï¼Œæˆ‘è§‰å¾—è¿˜æ˜¯æœ‰é—®é¢˜è¿˜æ˜¯è¦å¤šå’Œå…¶ä»–äººæ²Ÿé€šï¼Œå¦å¤–å¯¹ä¸»é¢˜çš„è®¾ç½®å¯ä»¥æŒ‰ç…§è‡ªå·±çš„é£æ ¼æ¥ï¼Œä½†æ˜¯è¿™å°±éœ€è¦å¯¹webçŸ¥è¯†æœ‰ä¸€å®šçš„äº†è§£ï¼Œå¯¹æ’ç‰ˆæœ‰è‡ªå·±çš„ç†è§£æ‰å¯ä»¥ã€‚<br>åœ¨æˆ‘çœ‹æ¥ï¼Œåšå®¢æ›´åŠ æ³¨é‡çš„åº”è¯¥æ˜¯å†…å®¹ï¼Œä»¥åŠå…»æˆè®°å½•æ—¥è®°çš„ä¹ æƒ¯ï¼Œå¯¹è‡ªå·±æ¯ä¸ªé˜¶æ®µçš„å­¦ä¹ æœ‰ä¸€ä¸ªé€‚å½“çš„æ€»ç»“ï¼Œå¯ä»¥è®©è‡ªå·±è®¡åˆ’æ›´åŠ æ˜ç¡®ã€‚<br>æ‰€ä»¥æˆ‘å°±ç®€å•ä»‹ç»hexo+github.ioæ­å»ºåšå®¢è¿‡ç¨‹ä¸­é‡è¦çš„ç‚¹å§(æˆ‘æ˜¯ç”¨çš„ä¸»é¢˜æ˜¯matery)</p><h4 id="æœ¬åœ°é…ç½®æ–‡ä»¶"><a href="#æœ¬åœ°é…ç½®æ–‡ä»¶" class="headerlink" title="æœ¬åœ°é…ç½®æ–‡ä»¶"></a>æœ¬åœ°é…ç½®æ–‡ä»¶</h4><p><img src="C:\Users\VrShadow\AppData\Roaming\Typora\typora-user-images\image-20210916214915453.png" alt="image-20210916214915453"></p><ul><li><p><strong>_config.yml</strong>ï¼š</p><p>ç½‘ç«™<strong>ç«™ç‚¹é…ç½®æ–‡ä»¶</strong>ï¼Œåˆå«æ ¹ç›®å½•ç«™ç‚¹é…ç½®æ–‡ä»¶ï¼Œåœ¨è¿™ä¸ªæ–‡ä»¶é‡Œé¢å¯ä»¥é…ç½®å¤§éƒ¨åˆ†çš„å‚æ•°</p></li><li><p><strong>scaffolds</strong>:</p><p>æ­¤æ–‡ä»¶å¤¹ä¼šæ”¾ä¸€äº›é»˜è®¤çš„æ–‡ä»¶ï¼Œç”¨æ¥å½“ä½œåˆ›å»ºåšæ–‡çš„æ¨¡æ¿mdæ–‡ä»¶ï¼Œhexoä¼šæ ¹æ®scaffoldæ¥å»ºç«‹æ–‡ä»¶ã€‚æ¨¡æ¿æ˜¯æŒ‡æ–°å»ºçš„mdæ–‡ä»¶ä¼šé»˜è®¤æ”¾å…¥æ¨¡æ¿æ–‡ä»¶çš„åˆå§‹å†…å®¹</p></li><li><p><strong>public</strong>ï¼š</p><p>è¿™ä¸ªæ–‡ä»¶çš„å†…å®¹æœ€ç»ˆéƒ½ä¼špushåˆ°githubä»“åº“ä¸­</p></li><li><p><strong>source</strong>:</p><p>è¿™ä¸ªæ–‡ä»¶å¤¹æ˜¯å­˜æ”¾ç”¨æˆ·èµ„æºçš„åœ°æ–¹ï¼Œé™¤äº†<code>_posts</code>æ–‡ä»¶å¤¹ä¹‹å¤–ï¼Œå¼€å¤´å‘½åä¸º_(ä¸‹åˆ’çº¿çš„æ–‡ä»¶/æ–‡ä»¶å¤¹ä»¥åŠéšè—çš„æ–‡ä»¶éƒ½ä¼šè¢«å¿½ç•¥)ã€‚markdownå’Œhtmlæ–‡ä»¶éƒ½ä¼šè¢«è§£æå¹¶æ”¾åˆ°<strong>public</strong>æ–‡ä»¶å¤¹é‡Œé¢ï¼Œè€Œå…¶ä»–æ–‡ä»¶ä¼šè¢«æ‹·è´åˆ°publicæ–‡ä»¶å¤¹ã€‚</p></li><li><p>**ä¸ºgithubä»“åº“æ·»åŠ readme</p><p>æ—¢ç„¶<code>source</code>æ–‡ä»¶å¤¹ä¸­çš„å†…å®¹ä¼šè¢«å…¨éƒ¨æ¨é€åˆ°publicæ–‡ä»¶å¤¹ï¼Œpublicæ–‡ä»¶å¤¹ä¸­çš„å†…å®¹æœ€ç»ˆåˆä¼šè¢«pushåˆ°githubä»“åº“ï¼Œæ‰€ä»¥å¦‚æœæƒ³è¦ä¸ºgithubä»“åº“æ·»åŠ readme.mdï¼Œåªè¦åœ¨sourceæ–‡ä»¶å¤¹ä¸­åˆ›å»ºå°±å¥½äº†ã€‚æœ€å<strong>éƒ¨ç½²</strong>åˆ°githubå°±æœ‰readmeäº†ã€‚ä½†æ˜¯ä¼šå‘ç°ï¼ŒREADME.mdæ–‡ä»¶éƒ¨ç½²çš„æ—¶å€™ä¼šè¢«è§£ææˆhtmlæ–‡ä»¶ï¼Œæ˜¾ç¤ºçš„æ˜¯htmlä»£ç ï¼Œä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„æ–‡æ¡£å†…å®¹ã€‚</p><p><strong>è§£å†³åŠæ³•</strong>ï¼šå°†åœ¨sourceæ–‡ä»¶å¤¹æ–°å»ºçš„README.mdé‡å‘½åä¸ºREMADE.MDWNï¼Œåœ¨é‡æ–°éƒ¨ç½²åˆ°githubã€‚(sourceæ–‡ä»¶å¤¹ä¸­ï¼Œ.mdä¼šè¢«è§£æä¸ºhtmlã€‚å¹¶æ”¾åˆ°publicæ–‡ä»¶å¤¹è¢«pushåˆ°githubï¼Œä½†.MDWNä¸ä¼šè¢«è§£æ)</p></li></ul><h4 id="ä¸€äº›å¸¸ç”¨çš„Hexoå‘½ä»¤"><a href="#ä¸€äº›å¸¸ç”¨çš„Hexoå‘½ä»¤" class="headerlink" title="ä¸€äº›å¸¸ç”¨çš„Hexoå‘½ä»¤"></a>ä¸€äº›å¸¸ç”¨çš„Hexoå‘½ä»¤</h4><ul><li><p>å¸¸ç”¨å‘½ä»¤</p><pre><code>hexo new "postName" #æ–°å»ºåšæ–‡hexo generate #ç”Ÿæˆé™æ€é¡µé¢è‡³publicç›®å½•hexo server #å¼€å¯é¢„è§ˆè®¿é—®ç«¯å£ï¼ˆé»˜è®¤ç«¯å£4000ï¼Œâ€™crtl+c'å…³é—­serverï¼‰hexo deploy #éƒ¨ç½²åˆ°githubhexo help #æŸ¥çœ‹å¸®åŠ©hexo version #æŸ¥çœ‹ç‰ˆæœ¬</code></pre></li><li><p>ç¼©å†™</p><pre><code>hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy</code></pre></li><li><p>ç»„åˆå‘½ä»¤</p><pre><code>hexo s -g #ç”Ÿæˆå¹¶æœ¬åœ°é¢„è§ˆhexo d -g #ç”Ÿæˆå¹¶éƒ¨ç½²åˆ°äº‘ç«¯</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> æµ‹è¯•ï¼Œæµ‹è¯•çš„å­åˆ†ç±» </category>
          
      </categories>
      
      
        <tags>
            
            <tag> åšæ–‡ </tag>
            
            <tag> æµ‹è¯• </tag>
            
            <tag> whatever </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>README.md</title>
      <link href="/year/09/14/README/"/>
      <url>/year/09/14/README/</url>
      
        <content type="html"><![CDATA[<h3 id="20201-9-17-ç¬¬ä¸€æ¬¡æ›´æ–°"><a href="#20201-9-17-ç¬¬ä¸€æ¬¡æ›´æ–°" class="headerlink" title="20201.9.17 ç¬¬ä¸€æ¬¡æ›´æ–°"></a>20201.9.17 ç¬¬ä¸€æ¬¡æ›´æ–°</h3><ul><li>å¯¹æ–‡ç« Front-matterä»‹ç»çš„ä¸€äº›åº”ç”¨å°è¯•å¢åŠ <ul><li>æ¯”å¦‚title,date,topï¼Œsummaryç­‰ï¼Œå‰©ä¸‹çš„å¾…æ›´æ–°å°è¯•<span class="github-emoji"><span>ğŸ‘Š</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44a.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>ğŸ’¤</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a4.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></li></ul></li></ul><h3 id="2021-9-14-æ°´ç¬¬ä¸€ç¯‡ï¼Œå•¥åŠŸèƒ½æ²¡æœ‰"><a href="#2021-9-14-æ°´ç¬¬ä¸€ç¯‡ï¼Œå•¥åŠŸèƒ½æ²¡æœ‰" class="headerlink" title="2021.9.14 æ°´ç¬¬ä¸€ç¯‡ï¼Œå•¥åŠŸèƒ½æ²¡æœ‰"></a>2021.9.14 æ°´ç¬¬ä¸€ç¯‡ï¼Œå•¥åŠŸèƒ½æ²¡æœ‰</h3><ul><li>èƒ½æ­£å¸¸éƒ¨ç½²æ–‡ç« å’Œæ¸²æŸ“æ­£å¸¸</li><li>èƒ½å¤Ÿè®¿é—®blog</li></ul><h3 id="2021-11-05"><a href="#2021-11-05" class="headerlink" title="2021.11.05"></a>2021.11.05</h3><ul><li>å»ºç«™åŠŸèƒ½</li><li>ä¸è’œå­åˆå§‹åŒ–è®¡æ•°</li></ul><h3 id="2021-11-07"><a href="#2021-11-07" class="headerlink" title="2021.11.07"></a>2021.11.07</h3><ul><li>å…¨å±€æœç´¢</li><li>ä»£ç é«˜äº®</li></ul>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
