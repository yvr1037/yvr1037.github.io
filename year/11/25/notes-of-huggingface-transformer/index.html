<!DOCTYPE HTML>
<html lang="zn-en">
    <!-- shw2018 æ´ªå«  modify 2019.08.15-->



<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="notes of huggingface transformer, æœºå™¨å­¦ä¹ ,æ·±åº¦å­¦ä¹ ">
    <meta name="baidu-site-verification" content="fmlEuI34ir" />
    <meta name="google-site-verification" content="KeoTn_OFy4ndJwXNmm2gMeQfPhd7alqE9vQDwI32KCY" />
    <meta name="description" content="PerfaceHuggingFace-Transformersæ‰‹å†Œæ˜¯å¼€æºå…¬å¸HuggingFaceå¼€å‘çš„æ¶µç›–å¾ˆå¤šæ¨¡å‹çš„æ¡†æ¶ã€‚
Transformers(å‰èº«æ˜¯ç§°ä¸ºpytorch Transformerså’Œpytorch pretrained">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>notes of huggingface transformer | yvr</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.bootcss.com/materialize/1.0.0/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.bootcss.com/aos/3.0.0-beta.6/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.bootcss.com/lightgallery/1.6.12/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    
    <style type="text/css">
        
            
            code[class*="language-"],
            pre[class*="language-"] {
                white-space: pre !important;
            }

        
    </style>

    <script src="https://libs.baidu.com/jquery/2.1.4/jquery.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?46e79e71af0709a5b9106bf20cecc493";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

    
    
        <script>
            (function(){
                var bp = document.createElement('script');
                var curProtocol = window.location.protocol.split(':')[0];
                if (curProtocol === 'https') {
                    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
                }
                else {
                    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                }
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(bp, s);
            })();
        </script>
    

<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

    <body>

        <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/team.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">yvr</span>
                </a>
            </div>
            


<!-- <a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>Index</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>Tags</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>Categories</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>Archives</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/galleries" class="waves-effect waves-light">
            
            <i class="fa fa-photo"></i>
            
            <span>Galleries</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>About</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/contact" class="waves-effect waves-light">
            
            <i class="fa fa-envelope"></i>
            
            <span>Contact</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>Friends</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="Search"></i>
        </a>
    </li>
</ul> -->

<!-- æ”¯æŒäºŒçº§èœå•ç‰¹æ€§ æ´ªå« shw2018 modify 2019.09.17  -->
<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right nav-menu">
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/" class="waves-effect waves-light">
              
                <i class="fa fa-home"></i>
              
              <span>Index</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/tags" class="waves-effect waves-light">
              
                <i class="fa fa-tags"></i>
              
              <span>Tags</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/categories" class="waves-effect waves-light">
              
                <i class="fa fa-bookmark"></i>
              
              <span>Categories</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/archives" class="waves-effect waves-light">
              
                <i class="fa fa-archive"></i>
              
              <span>Archives</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/galleries" class="waves-effect waves-light">
              
                <i class="fa fa-photo"></i>
              
              <span>Galleries</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/about" class="waves-effect waves-light">
              
                <i class="fa fa-user-circle-o"></i>
              
              <span>About</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/contact" class="waves-effect waves-light">
              
                <i class="fa fa-envelope"></i>
              
              <span>Contact</span>
            </a>

            
      </li>
    
      <li class="hide-on-med-and-down nav-item" >

        
            <a href="/friends" class="waves-effect waves-light">
              
                <i class="fa fa-address-book"></i>
              
              <span>Friends</span>
            </a>

            
      </li>
    

    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="Search"></i>
        </a>
    </li>

</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/team.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">yvr</div>
        <div class="logo-desc">
            
            å•¦å•¦å•¦
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                Index
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                Tags
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                Categories
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                Archives
            </a>
        </li>
        
        <li>
            <a href="/galleries" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-photo"></i>
                
                Galleries
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                About
            </a>
        </li>
        
        <li>
            <a href="/contact" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-envelope"></i>
                
                Contact
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                Friends
            </a>
        </li>
        
        
    </ul>

   
   
<!-- æ”¯æŒäºŒçº§èœå•ç‰¹æ€§ æ´ªå« shw2018 modify 2019.09.17  -->
<!-- <ul class="menu-list mobile-menu-list">
    
        <li class="m-nav-item">
                
                    <a href="/" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-home"></i>
                        
                        Index
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/tags" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-tags"></i>
                        
                        Tags
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/categories" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-bookmark"></i>
                        
                        Categories
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/archives" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-archive"></i>
                        
                        Archives
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/galleries" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-photo"></i>
                        
                        Galleries
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/about" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-user-circle-o"></i>
                        
                        About
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/contact" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-envelope"></i>
                        
                        Contact
                    </a>
              
            </li>
        
        <li class="m-nav-item">
                
                    <a href="/friends" class="waves-effect waves-light">
                        
                        <i class="fa fa-fw fa-address-book"></i>
                        
                        Friends
                    </a>
              
            </li>
        

        
    </ul> -->

</div>

        </div>

        
    </nav>

</header>

        
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('è¯·è¾“å…¥è®¿é—®æœ¬æ–‡ç« çš„å¯†ç ')).toString(CryptoJS.enc.Hex)) {
                alert('å¯†ç é”™è¯¯ï¼Œå°†è¿”å›ä¸»é¡µï¼');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/23.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        notes of huggingface transformer
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- shw2018 æ´ªå«  modify 2019.08.15-->
<!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/transformer/" target="_blank">
                                <span class="chip bg-color">transformer</span>
                            </a>
                        
                            <a href="/tags/pytorch/" target="_blank">
                                <span class="chip bg-color">pytorch</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/dialogue/" class="post-category" target="_blank">
                                dialogue
                            </a>
                        
                    </div>
                    
                </div>
            </div>
            
            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2021-11-25
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>Author:&nbsp;&nbsp;
                    
                        yvr1037
                    
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                        6.3k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                        31 Min
                    </div>
                    
                
                
                
                        <span id="busuanzi_container_site_pv" style='display:none'></span>
                        <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv" ></span>
    
                

            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h4 id="Perface"><a href="#Perface" class="headerlink" title="Perface"></a>Perface</h4><p>HuggingFace-Transformersæ‰‹å†Œæ˜¯å¼€æºå…¬å¸HuggingFaceå¼€å‘çš„æ¶µç›–å¾ˆå¤šæ¨¡å‹çš„æ¡†æ¶ã€‚</p>
<p>Transformers(å‰èº«æ˜¯ç§°ä¸ºpytorch Transformerså’Œpytorch pretrained bert)ä¸ºè‡ªç„¶è¯­è¨€ç†è§£(NLU)å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG)æä¾›äº†æœ€å…ˆè¿›çš„é€šç”¨æ¶æ„(bert,GPT-2,RoBERTTa,XLM,DistileBert,XLNet,CTRLâ€¦..),å…¶ä¸­è¶…è¿‡32ä¸ª100å¤šç§è¯­è¨€çš„é¢„è®­ç»ƒæ¨¡å‹å¹¶åŒæ—¶æ”¯æŒTensorflow 2.0å’ŒPytorchä¸¤å¤§æ·±åº¦å­¦ä¹ æ¡†æ¶.</p>
<p>The library was designed with two strong goals in mind:</p>
<ul>
<li><p>Be as easy and fast to use as possible:</p>
<blockquote>
<ul>
<li>We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions, just three standard classes required to use each model: <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/configuration.html">configuration</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/model.html">models</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/tokenizer.html">tokenizer</a>.</li>
<li>All of these classes can be initialized in a simple and unified way from pretrained instances by using a common <code>from_pretrained()</code> instantiation method which will take care of downloading (if needed), caching and loading the related class instance and associated data (configurationsâ€™ hyper-parameters, tokenizersâ€™ vocabulary, and modelsâ€™ weights) from a pretrained checkpoint provided on <a target="_blank" rel="noopener" href="https://huggingface.co/models">Hugging Face Hub</a> or your own saved checkpoint.</li>
<li>On top of those three base classes, the library provides two APIs: <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/pipelines.html#transformers.pipeline"><code>pipeline()</code></a> for quickly using a model (plus its associated tokenizer and configuration) on a given task and <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer"><code>Trainer()</code></a>/<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TFTrainer"><code>TFTrainer()</code></a> to quickly train or fine-tune a given model.</li>
<li>As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to extend/build-upon the library, just use regular Python/PyTorch/TensorFlow/Keras modules and inherit from the base classes of the library to reuse functionalities like model loading/saving.</li>
</ul>
</blockquote>
</li>
<li><p>Provide state-of-the-art models with performances as close as possible to the original models:</p>
<blockquote>
<ul>
<li>We provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture.</li>
<li>The code is usually as close to the original code base as possible which means some PyTorch code may be not as <em>pytorchic</em> as it could be as a result of being converted TensorFlow code and vice versa.</li>
</ul>
</blockquote>
</li>
</ul>
<p>è¿™æ˜¯<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/philosophy.html%E5%AE%98%E7%BD%91%E7%BB%99%E5%87%BA%E7%9A%84%E8%A7%A3%E9%87%8A%EF%BC%9A">https://huggingface.co/transformers/philosophy.htmlå®˜ç½‘ç»™å‡ºçš„è§£é‡Šï¼š</a></p>
<ul>
<li>æ¶æ„<ul>
<li>ä½¿ç”¨æ¯ä¸ªæ¨¡å‹éƒ½éœ€è¦ä¸‰ä¸ªæ ‡å‡†ç±»:<strong>configuration</strong>,<strong>models</strong>,<strong>tokenizer</strong>.modelç”¨äºæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹,ä¾‹å¦‚modelä¸ºbertï¼Œé‚£ä¹ˆç›¸åº”çš„ç½‘ç»œç»“æ„æ˜¯bertçš„ç½‘ç»œç»“æ„ï¼›configurationæ˜¯æ¨¡å‹å…·ä½“çš„æ¶æ„é…ç½®ï¼Œä¾‹å¦‚å¯ä»¥é…ç½®å¤šå¤´çš„æ•°é‡ç­‰ç­‰,è¿™é‡Œé…ç½®éœ€è¦æ³¨æ„çš„åœ°æ–¹å°±æ˜¯ï¼Œå¦‚æœè‡ªå®šä¹‰é…ç½®ä¸æ”¹å˜æ ¸å¿ƒç½‘ç»œç»“æ„çš„åˆ™ä»æ—§å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œå¦‚æœé…ç½®æ¶‰åŠåˆ°æ ¸å¿ƒç»“æ„çš„ä¿®æ”¹ï¼Œä¾‹å¦‚å‰é¦ˆç½‘ç»œçš„éšå±‚ç¥ç»å…ƒçš„ä¸ªæ•°ï¼Œåˆ™æ— æ³•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œè¿™ä¸ªæ—¶å€™transformersä¼šé»˜è®¤ä½ è¦é‡æ–°è‡ªå·±é¢„è®­ç»ƒä¸€ä¸ªæ¨¡å‹ä»è€Œéšæœºåˆå§‹åŒ–æ•´ä¸ªæ¨¡å‹çš„æƒé‡ï¼Œè¿™æ˜¯æ˜¯ä¸€ç§åŠçµæ´»æ€§çš„è®¾è®¡.</li>
<li>æ‰€æœ‰è¿™äº›ç±»éƒ½å¯ä»¥ä½¿ç”¨é€šç”¨çš„from_pretrained()å®ä¾‹åŒ–æ–¹æ³•ï¼Œä»¥ç®€å•ç»Ÿä¸€çš„æ–¹å¼ä»å—è¿‡è®­ç»ƒçš„å®ä¾‹ä¸­åˆå§‹åŒ–ï¼Œè¯¥æ–¹æ³•å°†è´Ÿè´£ä¸‹è½½ï¼ˆå¦‚æœéœ€è¦ï¼‰ï¼Œç¼“å­˜å’ŒåŠ è½½ç›¸å…³çš„ç±»å®ä¾‹ä»¥åŠç›¸å…³çš„æ•°æ®(configçš„çš„è¶…å‚æ•°ï¼Œtokenizerç”Ÿæˆå™¨çš„è¯æ±‡è¡¨å’Œæ¨¡å‹çš„æƒé‡)åœ¨ <a href="https://link.zhihu.com/?target=https://huggingface.co/models">Hugging Face Hub</a> ä¸Šæä¾›çš„é¢„å…ˆè®­ç»ƒçš„æ£€æŸ¥ç‚¹æˆ–æ‚¨è‡ªå·±ä¿å­˜çš„æ£€æŸ¥ç‚¹</li>
<li>åœ¨è¿™ä¸‰ä¸ªåŸºæœ¬ç±»çš„åŸºç¡€ä¸Šï¼Œè¯¥åº“æä¾›äº†ä¸¤ä¸ªAPIï¼š<ul>
<li><a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/main_classes/pipelines.html%23transformers.pipeline">pipeline()</a>ç”¨äºåœ¨ç»™å®šä»»åŠ¡ä¸Šå¿«é€Ÿä½¿ç”¨æ¨¡å‹ï¼ˆåŠå…¶å…³è”çš„tokenizerå’Œconfigurationï¼‰å’Œ </li>
<li>Traineræˆ–è€…<a href="https://link.zhihu.com/?target=https://huggingface.co/transformers/main_classes/trainer.html%23transformers.TFTrainer">TF</a>trainer å¿«é€Ÿè®­ç»ƒæˆ–å¾®è°ƒç»™å®šæ¨¡å‹</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>å› æ­¤<strong>Transformers</strong>ä¸æ˜¯ç¥ç»ç½‘ç»œæ„å»ºæ¨¡å—åŒ–çš„æ¨¡å—å·¥å…·ç®±ã€‚å¦‚æœè¦æ‰©å±•/æ„å»ºåº“ï¼Œåªéœ€ä½¿ç”¨å¸¸è§„çš„Python / PyTorch / TensorFlow / Kerasæ¨¡å—å¹¶ä»åº“çš„åŸºç±»ç»§æ‰¿å³å¯é‡ç”¨æ¨¡å‹åŠ è½½/ä¿å­˜ä¹‹ç±»çš„åŠŸèƒ½ã€‚</p>
<p>ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æ•´ä½“ä¸Šéƒ½å±äºä¸‹é¢çš„äº”ä¸ªç±»åˆ«ï¼š</p>
<h5 id="Decoders-or-autoregressive-models"><a href="#Decoders-or-autoregressive-models" class="headerlink" title="Decoders or autoregressive models"></a>Decoders or autoregressive models</h5><p>è‡ªå›å½’æ¨¡å‹åœ¨ç»å…¸è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼šçŒœæµ‹ä¸‹ä¸€ä¸ªå·²è¯»å®Œæ‰€æœ‰å…ˆå‰tokençš„tokenã€‚å®ƒä»¬å¯¹åº”äºtransformeræ¨¡å‹çš„è§£ç å™¨éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªå¥å­çš„é¡¶éƒ¨ä½¿ç”¨äº†ä¸€ä¸ªæ©ç ï¼Œä»¥ä¾¿æ³¨æ„å¤´åªèƒ½çœ‹åˆ°æ–‡æœ¬ä¸­çš„ä¹‹å‰å†…å®¹ï¼Œè€Œä¸èƒ½çœ‹åˆ°å…¶åçš„å†…å®¹ã€‚å°½ç®¡å¯ä»¥å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶åœ¨è®¸å¤šä»»åŠ¡ä¸Šå–å¾—å‡ºè‰²çš„ç»“æœï¼Œä½†å…¶æœ€è‡ªç„¶çš„åº”ç”¨æ˜¯æ–‡æœ¬ç”Ÿæˆã€‚æ­¤ç±»æ¨¡å‹çš„å…¸å‹ä¾‹å­æ˜¯GPT</p>
<h5 id="Encoders-or-autoencoding-models"><a href="#Encoders-or-autoencoding-models" class="headerlink" title="Encoders or autoencoding models"></a>Encoders or autoencoding models</h5><p>é€šè¿‡ä»¥æŸç§æ–¹å¼ç ´åè¾“å…¥tokenå¹¶å°è¯•é‡å»ºåŸå§‹å¥å­æ¥å¯¹è‡ªç¼–ç æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒä»¬ä¸transformerä¸­çš„çš„ç¼–ç å™¨ç›¸å¯¹åº”ï¼Œå› ä¸ºå®ƒä»¬æ— éœ€ä»»ä½•æ©ç å³å¯è®¿é—®å®Œæ•´çš„è¾“å…¥ã€‚è¿™äº›æ¨¡å‹é€šå¸¸å»ºç«‹æ•´ä¸ªå¥å­çš„åŒå‘è¡¨ç¤ºã€‚å¯ä»¥å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒå¹¶åœ¨è®¸å¤šä»»åŠ¡ï¼ˆä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆï¼‰ä¸Šå–å¾—å‡ºè‰²çš„ç»“æœï¼Œä½†æ˜¯å®ƒä»¬æœ€è‡ªç„¶çš„åº”ç”¨æ˜¯æ–‡æœ¬åˆ†ç±»æˆ–tokenåˆ†ç±»ï¼ˆæ¯”å¦‚è¯æ€§æ ‡æ³¨ï¼‰ã€‚æ­¤ç±»æ¨¡å‹çš„å…¸å‹ä¾‹å­æ˜¯BERT</p>
<p>è‡ªåŠ¨å›å½’æ¨¡å‹å’Œè‡ªåŠ¨ç¼–ç æ¨¡å‹ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«åœ¨äºæ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼ã€‚å› æ­¤ï¼Œç›¸åŒçš„ä½“ç³»ç»“æ„æ—¢å¯ä»¥ç”¨äºè‡ªåŠ¨å›å½’æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç”¨äºè‡ªåŠ¨ç¼–ç æ¨¡å‹.</p>
<h5 id="Sequence-to-Sequence-models"><a href="#Sequence-to-Sequence-models" class="headerlink" title="Sequence-to-Sequence models"></a>Sequence-to-Sequence models</h5><p>åºåˆ—åˆ°åºåˆ—æ¨¡å‹å°†transformersçš„ç¼–ç å™¨å’Œè§£ç å™¨åŒæ—¶ç”¨äºç¿»è¯‘ä»»åŠ¡æˆ–é€šè¿‡å°†å…¶ä»–ä»»åŠ¡è½¬æ¢ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜æ¥è®­ç»ƒå¾—åˆ°çš„ã€‚å¯ä»¥å°†å®ƒä»¬å¾®è°ƒæ¥é€‚åº”è®¸å¤šä»»åŠ¡ï¼ˆè¿™é‡Œåº”è¯¥æ˜¯è¯´æŠŠsequence to sequenceçš„é¢„è®­ç»ƒæ¨¡å‹çš„encoderæˆ–è€…decoderå•ç‹¬æŠ½å–å‡ºæ¥ï¼Œç„¶åç”¨æ³•å°±å’Œä¸Šé¢ä¸¤ç§æ¨¡å‹çš„ç”¨æ³•ä¸€è‡´ï¼‰ï¼Œä½†æœ€è‡ªç„¶çš„åº”ç”¨æ˜¯ç¿»è¯‘ï¼Œæ‘˜è¦å’Œé—®é¢˜è§£ç­”ã€‚T5æ˜¯ä¸€ä¸ªå…¸å‹çš„ä¾‹å­.</p>
<h5 id="Multimodal-models"><a href="#Multimodal-models" class="headerlink" title="Multimodal models"></a>Multimodal models</h5><p>å¤šæ¨¡æ€æ¨¡å‹å°†æ–‡æœ¬è¾“å…¥ä¸å…¶ä»–ç±»å‹çš„è¾“å…¥ï¼ˆä¾‹å¦‚å›¾åƒï¼‰æ··åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æ›´ç‰¹å®šäºç»™å®šä»»åŠ¡.</p>
<p><img src="https://s2.loli.net/2021/12/23/zu7FbaZfjXMVUyG.png" alt="image-20211125193439861.png"></p>
<p>è¿™ç§æ¨¡å‹æ²¡æœ‰æä¾›ä»»ä½•é¢„è®­ç»ƒæƒé‡åªæ˜¯å®šä¹‰äº†æ¨¡å‹çš„ç»“æ„.</p>
<h5 id="Retrieval-based-models"><a href="#Retrieval-based-models" class="headerlink" title="Retrieval-based models"></a>Retrieval-based models</h5><p><img src="https://s2.loli.net/2021/12/23/aqH4X5ikoGsVjDp.png" alt="image-20211125193530708.png"></p>
<h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p>  The most basic object in the ğŸ¤— Transformers library is the <code>pipeline()</code> function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:</p>
<p>  There are three main steps involved when you pass some text to a pipeline:</p>
<ol>
<li>The text is preprocessed into a format the model can understand.</li>
<li>The preprocessed inputs are passde to the model .</li>
<li>The predictions of the model are post-processed,so you can make sense of them.</li>
</ol>
<p>Some of the currently <strong>available pipelines</strong> are:</p>
<ul>
<li>feature-extraction(get the vector representation of a text)</li>
<li>file-mask</li>
<li>ner(named entity recogniton)</li>
<li>question-answering</li>
<li>sentiment-analysis</li>
<li>summarization</li>
<li>text-generation</li>
<li>translation</li>
<li>zero-shot-classification</li>
</ul>
<h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><h5 id="Transformer-history"><a href="#Transformer-history" class="headerlink" title="Transformer history"></a>Transformer history</h5><p><img src="https://s6.jpg.cm/2021/12/23/Lbta92.png" alt="Lbta92.png"></p>
<p>The <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Transformer architecture</a> was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:</p>
<ul>
<li><strong>June 2018</strong>: <a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results</li>
<li><strong>October 2018</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT</a>, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)</li>
<li><strong>February 2019</strong>: <a target="_blank" rel="noopener" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns</li>
<li><strong>October 2019</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERTâ€™s performance</li>
<li><strong>October 2019</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.13461">BART</a> and <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.10683">T5</a>, two large pretrained models using the same architecture as the original Transformer model (the first to do so)</li>
<li><strong>May 2020</strong>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT-3</a>, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called <em>zero-shot learning</em>)</li>
</ul>
<p>This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:</p>
<ul>
<li>GPT-like (also called <em>auto-regressive</em> Transformer models)</li>
<li>BERT-like (also called <em>auto-encoding</em> Transformer models)</li>
<li>BART/T5-like (also called <em>sequence-to-sequence</em> Transformer models)</li>
</ul>
<p>We will dive into these families in more depth later on.</p>
<h5 id="Transformers-are-language-models"><a href="#Transformers-are-language-models" class="headerlink" title="Transformers are language models"></a>Transformers are language models</h5><p>All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as <em>language models</em>. This means they have been trained on large amounts of raw text in a self-supervised fashion. <strong>Self-supervised</strong> learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!</p>
<p>æ‰€æœ‰ä¸Šè¿°æåˆ°çš„æ¨¡å‹éƒ½å·²ç»è¢«è®­ç»ƒæˆäº†å¯¹åº”çš„è¯­è¨€æ¨¡å‹ã€‚è¿™ä¹Ÿå°±æ˜¯è¯´è¿™äº›æ¨¡å‹ä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼æ¥å—äº†å¤§é‡åŸå§‹æ–‡æœ¬çš„è®­ç»ƒã€‚è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§è®­ç»ƒç±»å‹ï¼Œç›®æ ‡æ˜¯æ ¹æ®æ¨¡å‹çš„è¾“å…¥è‡ªåŠ¨è®¡ç®—çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ä¸éœ€è¦äººç±»æ‰‹åŠ¨æ ‡è®°æ•°æ®ã€‚</p>
<hr>
<p>This type of model develops a statistical understanding of the language it has been trained on, but itâ€™s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called <em><strong>transfer learning</strong></em>. During this process, the model is fine-tuned in a supervised way â€” that is, using human-annotated labels â€” on a given task</p>
<p>è¿™ç§ç±»å‹çš„æ¨¡å‹å¯¹å…¶æ‰€è®­ç»ƒçš„è¯­è¨€æœ‰ç»Ÿè®¡ç†è§£ä½†å¯¹äºç‰¹å®šçš„å®é™…ä»»åŠ¡ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚å› æ­¤é€šç”¨çš„é¢„è®­ç»ƒæ¨¡å‹éƒ½ä¼šç»å†ä¸€ä¸ªç§°ä¸º<strong>è¿ç§»å­¦ä¹ </strong>çš„è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨ç»™å®šçš„ä»»åŠ¡ä¸Šä»¥æœ‰ç›‘ç£çš„æ–¹å¼è¿›è¡Œå¾®è°ƒâ€”â€”å³ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ•°æ®æ ‡ç­¾ã€‚</p>
<hr>
<p>An example of a task is predicting the next word in a sentence having read the <em>n</em> previous words. This is called *<strong>causal language modeling</strong> because the output depends on the past and present inputs, but not the future ones.  </p>
<p>ä»»åŠ¡çš„ä¸€ä¸ªå®ä¾‹å°±æ˜¯é¢„æµ‹å·²ç»é˜…è¯»çš„å‰nä¸ªå•è¯çš„å¥å­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è¿™ä¹Ÿè¢«ç§°ä¸º<strong>å› æœè¯­è¨€å»ºæ¨¡</strong>ï¼Œå› ä¸ºè¾“å‡ºå–å†³äºè¿‡å»å’Œç°åœ¨çš„è¾“å…¥è€Œä¸æ˜¯æœªæ¥çš„è¾“å…¥ã€‚</p>
<p><img src="https://s6.jpg.cm/2021/12/23/LbtxQH.png" alt="LbtxQH.png"></p>
<p>Another example is <em>masked language modeling</em>, in which the model predicts a masked word in the sentence.</p>
<p>å¦ä¸€ä¸ªä¾‹å­æ˜¯æ©ç è¯­è¨€å»ºæ¨¡ï¼Œå…¶ä¸­æ¨¡å‹é¢„æµ‹å¥å­çš„æ©ç è¯ã€‚</p>
<h5 id="Transformer-are-big-models"><a href="#Transformer-are-big-models" class="headerlink" title="Transformer are big models"></a>Transformer are big models</h5><p>Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the modelsâ€™ sizes as well as the amount of data they are pretrained on.</p>
<p>é™¤äº†ä¸€äº›ç‰¹æ®Š(å¦‚ DistilBERT)å¤–ï¼Œå®ç°æ›´å¥½æ€§èƒ½çš„ä¸€èˆ¬ç­–ç•¥æ˜¯å¢åŠ æ¨¡å‹çš„å¤§å°ä»¥åŠé¢„è®­ç»ƒçš„æ•°æ®é‡ã€‚</p>
<p><img src="https://s6.jpg.cm/2021/12/23/LbtDYL.png" alt="LbtDYL.png"></p>
<p>Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph.</p>
<p><img src="https://s6.jpg.cm/2021/12/23/LbtZRU.png" alt="LbtZRU.png"></p>
<h5 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h5><p><em>Pretraining</em> is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.</p>
<p>é¢„è®­ç»ƒæ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„è¡Œä¸ºï¼šæƒé‡éšæœºåˆå§‹åŒ–,è®­ç»ƒåœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å¼€å§‹.</p>
<p><img src="https://s6.jpg.cm/2021/12/23/LbNGuO.png" alt="LbNGuO.png"></p>
<p>This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.</p>
<p><em>Fine-tuning</em>, on the other hand, is the training done <strong>after</strong> a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task.</p>
<p>å¾®è°ƒæ˜¯åœ¨æ¨¡å‹é¢„è®­ç»ƒåè¿›è¡Œçš„è®­ç»ƒã€‚è¦è¿›è¡Œå¾®è°ƒï¼Œé¦–å…ˆéœ€è¦è·å¾—ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç„¶åä½¿ç”¨ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œé¢å¤–çš„è®­ç»ƒã€‚</p>
<ul>
<li>The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).</li>
<li>Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.</li>
<li>For the same reason, the amount of time and resources needed to get good results are much lower</li>
</ul>
<p>For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is â€œtransferred,â€ hence the term <em>transfer learning</em>.</p>
<p><img src="https://s6.jpg.cm/2021/12/23/LbNIBw.png" alt="LbNIBw.png"></p>
<p>Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.</p>
<p>å¾®è°ƒæ¨¡å‹å…·æœ‰æ›´ä½çš„æ—¶é—´,æ•°æ®,ç»æµå’Œç¯å¢ƒæˆæœ¬ã€‚è¿­ä»£ä¸åŒçš„å¾®è°ƒæ–¹æ¡ˆä¹Ÿæ›´å¿«æ›´å®¹æ˜“ï¼Œå› ä¸ºè®­ç»ƒæ¯”å®Œå…¨é¢„è®­ç»ƒçš„çº¦æŸæ›´å°‘ã€‚</p>
<p>This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model â€” one as close as possible to the task you have at hand â€” and fine-tune it.</p>
<h5 id="General-Transformer-Architecture"><a href="#General-Transformer-Architecture" class="headerlink" title="General Transformer Architecture"></a>General Transformer Architecture</h5><p><img src="https://s6.jpg.cm/2021/12/23/LbNLO8.png" alt="LbNLO8.png"></p>
<p>The transformer is based on the attention mechanism.</p>
<p><img src="https://s6.jpg.cm/2021/12/23/LbNR8i.png" alt="LbNR8i.png"></p>
<p>The combination of the two parts is known as an encoder-decoder or a sequence-to-sequence transformer.</p>
<p>The model is primarily composed of two blocks:</p>
<ul>
<li><strong>Encoder (left)</strong>: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.</li>
</ul>
<p>ç¼–ç å™¨æ¥å—è¾“å…¥å¹¶æ„å»ºå®ƒçš„è¡¨ç¤º(å…¶ç‰¹å¾)ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ç»è¿‡ä¼˜åŒ–ä»¥ä»è¾“å…¥ä¸­è·å–ç†è§£ã€‚</p>
<ul>
<li><strong>Decoder (right)</strong>: The decoder uses the encoderâ€™s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.</li>
</ul>
<p>è§£ç å™¨ä½¿ç”¨ç¼–ç å™¨çš„è¡¨ç¤º(ç‰¹å¾)å’Œå…¶ä»–è¾“å…¥ç”Ÿæˆç›®æ ‡åºåˆ—ï¼Œè¿™æ„å‘³ç€æ¨¡å‹é’ˆå¯¹ç”Ÿæˆè¾“å‡ºè¿›è¡Œä¼˜åŒ–ã€‚</p>
<p>Each of these parts can be used independently, depending on the task:</p>
<ul>
<li><p><strong>Encoder-only models</strong>: Good for tasks that require understanding of the input, such as <strong>sentence classification and named entity recognition.</strong></p>
<p>é€‚ç”¨äº<strong>éœ€è¦ç†è§£è¾“å…¥çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¥å­åˆ†ç±»å’Œå‘½åå®ä½“è¯†åˆ«ã€‚</strong></p>
</li>
<li><p><strong>Decoder-only models</strong>: Good for generative tasks such as <strong>text generation</strong>.</p>
<p>é€‚ç”¨äº<strong>ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆ</strong></p>
</li>
<li><p><strong>Encoder-decoder models</strong> or <strong>sequence-to-sequence models</strong>: Good for generative tasks that require an input, such as <strong>translation or summarization</strong>.</p>
<p>é€‚ç”¨äº<strong>éœ€è¦è¾“å…¥çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘æˆ–è€…æ‘˜è¦ã€‚</strong> </p>
</li>
</ul>
<h5 id="Atention-layers"><a href="#Atention-layers" class="headerlink" title="Atention layers"></a>Atention layers</h5><p>A key feature of Transformer models is that they are built with special layers called <em>attention layers</em>. In fact, the title of the paper introducing the Transformer architecture was <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">â€œAttention Is All You Needâ€</a>! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.</p>
<p>Transformeræ¨¡å‹çš„å…³é”®ç‚¹å°±æ˜¯ä»–ä»¬ç”±ç§°ä¸ºæ³¨æ„åŠ›å±‚çš„ç‰¹æ®Šå±‚æ„å»ºè€Œæˆã€‚äº‹å®ä¸Šï¼Œæå‡ºTransformeræ¶æ„çš„è®ºæ–‡æ˜¯â€Attention is all your needâ€ã€‚åé¢ä¼šè¯¦ç»†æ¢ç©¶Attenton layerçš„ç»†èŠ‚ã€‚</p>
<p>To put this into context, consider the task of translating text from English to French. Given the input â€œYou like this courseâ€, a translation model will need to also attend to the adjacent word â€œYouâ€ to get the proper translation for the word â€œlikeâ€, because in French the verb â€œlikeâ€ is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating â€œthisâ€ the model will also need to pay attention to the word â€œcourseâ€, because â€œthisâ€ translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of â€œthisâ€. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.</p>
<h5 id="The-original-architecture"><a href="#The-original-architecture" class="headerlink" title="The original architecture"></a>The original architecture</h5><p>The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language</p>
<p>Transformeræ¶æ„æœ€åˆæ˜¯ä¸ºäº†ç¿»è¯‘è€Œè®¾è®¡çš„,åœ¨è®­ç»ƒæœŸé—´ï¼Œç¼–ç å™¨æ¥å—æŸç§è¯­è¨€çš„è¾“å…¥å¥å­ï¼Œè€Œè§£ç å™¨æ¥å—æ‰€éœ€ç›®æ ‡è¯­è¨€çš„ç›¸åŒå¥å­ã€‚</p>
<ul>
<li><p>In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence).</p>
<p>åœ¨ç¼–ç å™¨ä¸­ï¼Œæ³¨æ„åŠ›å±‚å¯ä»¥ä½¿ç”¨å¥å­ä¸­çš„æ‰€æœ‰å•è¯(ç»™å®šå•è¯çš„ç¿»è¯‘å¯ä»¥ä¾èµ–äºå¥å­ä¸­å®ƒä¹‹åå’Œä¹‹å‰çš„å†…å®¹)</p>
</li>
<li><p>The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.</p>
<p>è§£ç å™¨æ˜¯æŒ‰ç…§é¡ºåºå·¥ä½œï¼Œåªèƒ½å…³æ³¨å·²ç»ç¿»è¯‘çš„å¥å­ä¸­å•è¯ã€‚æ¯”å¦‚ï¼Œå½“æˆ‘ä»¬é¢„æµ‹ç¿»è¯‘ç›®æ ‡ä¸­çš„å‰ä¸‰ä¸ªå•è¯æ—¶ï¼Œæˆ‘ä»¬å°†ä»–ä»¬æä¾›ç»™è§£ç å™¨ç„¶åè§£ç å™¨ä½¿ç”¨ç¼–ç å™¨çš„æ‰€æœ‰è¾“å…¥å°è¯•é¢„æµ‹ç¬¬å››ä¸ªå•è¯ã€‚</p>
</li>
</ul>
<p>Note that the the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.</p>
<p>æ³¨æ„ï¼Œè§£ç å™¨çš„ç¬¬ä¸€ä¸ªæ³¨æ„å±‚å…³æ³¨è§£ç å™¨æ‰€æœ‰è¿‡å»çš„è¾“å…¥ï¼Œä½†ç¬¬äºŒä¸ªæ³¨æ„å±‚ä½¿ç”¨ç¼–ç å™¨çš„è¾“å‡ºã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥è®¿é—®æ•´ä¸ªè¾“å…¥å¥å­ä»¥æœ€å¥½çš„é¢„æµ‹å½“å‰çš„å•è¯ï¼Œè¿™æ˜¯å¾ˆæœ‰ç”¨çš„å› ä¸ºä¸åŒçš„è¯­è¨€æœ‰ä¸åŒçš„è¯­æ³•è§„åˆ™ï¼ŒæŠŠå•è¯æ”¾åœ¨ä¸åŒçš„é¡ºåºæˆ–è€…å¥å­åé¢æä¾›çš„ä¸Šä¸‹æ–‡å¯èƒ½æœ‰åŠ©äºç¡®å®šä¸€ä¸ªç»™å®šå•è¯çš„æœ€ä½³ç¿»è¯‘ã€‚</p>
<p>The <em>attention mask</em> can also be used in the encoder/decoder to prevent the model from paying attention to some special words â€” for instance, the special padding word used to make all the inputs the same length when batching together sentences.</p>
<p><strong>attention mask</strong>ä¹Ÿå¯ä»¥è¿ç”¨åœ¨ç¼–ç /è§£ç ä¸­ï¼Œé˜²æ­¢æ¨¡å‹æ³¨æ„åˆ°æŸäº›ç‰¹æ®Šçš„å•è¯</p>
<h5 id="Architecture-amp-amp-Checkpoints"><a href="#Architecture-amp-amp-Checkpoints" class="headerlink" title="Architecture &amp;&amp; Checkpoints"></a>Architecture &amp;&amp; Checkpoints</h5><p>As we dive into Transformer models in this course, youâ€™ll see mentions of <em>architectures</em> and <em>checkpoints</em> as well as <em>models</em>. These terms all have slightly different meanings:</p>
<ul>
<li><strong>Architecture</strong>: This is the skeleton of the model â€” the definition of each layer and each operation that happens within the model.</li>
<li><strong>Checkpoints</strong>: These are the weights that will be loaded in a given architecture.</li>
<li><strong>Model</strong>: This is an umbrella term that isnâ€™t as precise as â€œarchitectureâ€ or â€œcheckpointâ€: it can mean both. This course will specify <em>architecture</em> or <em>checkpoint</em> when it matters to reduce ambiguity.</li>
</ul>
<p>For example, BERT is an architecture while <code>bert-base-cased</code>, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say â€œthe BERT modelâ€ and â€œthe <code>bert-base-cased</code> model.â€</p>
<h4 id="Encoder-models"><a href="#Encoder-models" class="headerlink" title="Encoder models"></a>Encoder models</h4><p>Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having â€œbi-directionalâ€ attention, and are often called <em>auto-encoding models</em>.</p>
<p>Encoder modelåªä½¿ç”¨the transformer modelçš„encoderéƒ¨åˆ†ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œattention layerséƒ½å¯ä»¥è®¿é—®åˆå§‹å¥å­çš„æ‰€æœ‰è¯ã€‚è¿™äº›æ¨¡å‹åŒvè¡Œè¢«æè¿°ä¸ºå…·æœ‰â€bi-directionalâ€ï¼Œé€šå¸¸è¢«ç§°ä¸ºè‡ªç¼–ç æ¨¡å‹ã€‚</p>
<p>The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.</p>
<p>è¿™äº›æ¨¡å‹çš„é¢„è®­ç»ƒé€šå¸¸å›´ç»•æŸç§æ–¹å¼ç ´åç»™å®šçš„å¥å­(ä¾‹å¦‚ï¼Œé€šè¿‡å±è”½å…¶ä¸­çš„éšæœºè¯)ï¼Œå¹¶è®©æ¨¡å‹æŸ¥æ‰¾æˆ–é‡æ„åˆå§‹å¥å­ã€‚</p>
<p>Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.</p>
<p>ç¼–ç å™¨æ¨¡å‹æœ€é€‚åˆéœ€è¦ç†è§£å®Œæ•´å¥å­çš„ä»»åŠ¡ï¼Œä¾‹å¦‚sentence classification,ner(å‘½åå®ä½“è¯†åˆ«)(ä»¥åŠæ›´åŠ ä¸€èˆ¬çš„å•è¯åˆ†ç±»)å’Œeqaæå–å¼å›ç­”.</p>
<h5 id="Representatives-of-this-family-of-models-include"><a href="#Representatives-of-this-family-of-models-include" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/albert.html">ALBERT</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/bert.html">BERT</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/electra.html">ELECTRA</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/roberta.html">RoBERTa</a></li>
</ul>
<h4 id="Decoder-models"><a href="#Decoder-models" class="headerlink" title="Decoder models"></a>Decoder models</h4><p>Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called <em>auto-regressive models</em>.</p>
<p>è§£ç å™¨æ¨¡å‹ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„è§£ç å™¨ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¯¹äºç»™å®šçš„å•è¯ï¼Œæ³¨æ„åŠ›å±‚åªèƒ½è®¿é—®ä½äºå¥å­ä¹‹å‰çš„å•è¯ã€‚è¿™äº›æ¨¡å‹é€šå¸¸è¢«ç§°ä¸ºè‡ªå›å½’æ¨¡å‹ã€‚</p>
<p>The pretraining of decoder models usually revolves around predicting the next word in the sentence.</p>
<p>è§£ç å™¨æ¨¡å‹çš„é¢„è®­ç»ƒé€šå¸¸å›´ç»•é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚</p>
<p>These models are best suited for tasks involving text generation.</p>
<p>è¿™äº›æ¨¡å‹æœ€é€‚åˆè®¾è®¡æ–‡æœ¬ç”Ÿæˆçš„ä»»åŠ¡.</p>
<h5 id="Representatives-of-this-family-of-models-include-1"><a href="#Representatives-of-this-family-of-models-include-1" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/ctrl.html">CTRL</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/gpt.html">GPT</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/gpt2.html">GPT-2</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/transformerxl.html">Transformer XL</a></li>
</ul>
<h4 id="Seq-to-Seq-models"><a href="#Seq-to-Seq-models" class="headerlink" title="Seq-to-Seq models"></a>Seq-to-Seq models</h4><p>Encoder-decoder models (also called <em>sequence-to-sequence models</em>) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.</p>
<p>encoder-decoder models(ä¹Ÿç§°ä¸ºSeqtoSeq models)ä½¿ç”¨Transformerä½“ç³»ç»“æ„çš„æ‰€æœ‰éƒ¨åˆ†ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œç¼–ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®¿é—®åˆå§‹å¥å­çš„æ¯ä¸ªå•è¯,è€Œè§£ç å™¨çš„æ³¨æ„åŠ›å±‚åªèƒ½è®¿é—®è¾“å…¥ä¸­æŸä¸ªå•è¯å‰é¢çš„å•è¯ã€‚</p>
<p>The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, <a target="_blank" rel="noopener" href="https://huggingface.co/t5-base">T5</a> is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.</p>
<p>è¿™äº›æ¨¡å‹çš„é¢„è®­ç»ƒå¯ä»¥ä½¿ç”¨ç¼–ç å™¨/è§£ç å™¨æ¨¡å‹çš„ç›®æ ‡æ¥å®Œæˆ,ä½†é€šå¸¸æ¶‰åŠä¸€äº›æ›´å¤æ‚çš„ä¸œè¥¿ã€‚ä¾‹å¦‚ï¼ŒT5æ˜¯é€šè¿‡ä¸€ä¸ªæ©ç ç‰¹æ®Šè¯å–ä»£éšæœºæ–‡æœ¬è·¨åº¦(å¯ä»¥åŒ…å«å¤šä¸ªå•è¯)è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œç„¶åç›®æ ‡æ˜¯é¢„æµ‹è¿™ä¸ªæ©ç è¯å–ä»£çš„æ–‡æœ¬ã€‚</p>
<p>Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.</p>
<p>Seq-to-Seqæ¨¡å‹æœ€é€‚åˆæ ¹æ®ç»™å®šçš„è¾“å…¥ç”Ÿæˆæ–°å¥å­çš„ä»»åŠ¡ï¼Œæ¯”å¦‚æ‘˜è¦ï¼Œç¿»è¯‘æˆ–è€…ç”Ÿæˆå¼é—®é¢˜å›ç­”ã€‚</p>
<h5 id="Representatives-of-this-family-of-models-include-2"><a href="#Representatives-of-this-family-of-models-include-2" class="headerlink" title="Representatives of this family of models include:"></a>Representatives of this family of models include:</h5><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/bart.html">BART</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/mbart.html">mBART</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/marian.html">Marian</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/t5.html">T5</a></li>
</ul>
<h4 id="Bias-and-limitations"><a href="#Bias-and-limitations" class="headerlink" title="Bias and limitations"></a>Bias and limitations</h4><p>If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.</p>
<p>å¦‚æœæ‰“ç®—åœ¨productionä¸­ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹æˆ–è€…ç»è¿‡å¾®è°ƒçš„ç‰ˆæœ¬ï¼Œè¯·æ³¨æ„ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹æ˜¯æœ€å¼ºå¤§çš„å·¥å…·ä½†æ˜¯ä»–ä»¬ä¹Ÿæœ‰å±€é™æ€§ã€‚å…¶ä¸­æœ€å¤§çš„é—®é¢˜æ˜¯æ˜¯ä¸ºäº†èƒ½å¤Ÿå¯¹å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç ”ç©¶äººå‘˜ç»å¸¸æœé›†ä»–ä»¬èƒ½å¤Ÿæ‰¾åˆ°çš„æ‰€æœ‰å†…å®¹ï¼Œå¹¶ä¸”ä»äº’è”ç½‘ä¸Šå¯è·å¾—çš„ä¿¡æ¯ä¸­æŒ‘é€‰å‡ºæœ€å¥½çš„å’Œæœ€å·®çš„ã€‚</p>
<p>Exampleï¼š<strong>pipeli    ne:fill-mask model:bert-base-uncased</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

unmasker <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"fill-mask"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">"This man works as a [MASK]."</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">"token_str"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>

result <span class="token operator">=</span> unmasker<span class="token punctuation">(</span><span class="token string">"This woman works as a [MASK]."</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>r<span class="token punctuation">[</span><span class="token string">"token_str"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> result<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python">Some weights of the model checkpoint at bert<span class="token operator">-</span>base<span class="token operator">-</span>uncased were <span class="token operator">not</span> used when initializing BertForMaskedLM<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'cls.seq_relationship.bias'</span><span class="token punctuation">,</span> <span class="token string">'cls.seq_relationship.weight'</span><span class="token punctuation">]</span>
<span class="token operator">-</span> This IS expected <span class="token keyword">if</span> you are initializing BertForMaskedLM <span class="token keyword">from</span> the checkpoint of a model trained on another task <span class="token operator">or</span> <span class="token keyword">with</span> another architecture <span class="token punctuation">(</span>e<span class="token punctuation">.</span>g<span class="token punctuation">.</span> initializing a BertForSequenceClassification model <span class="token keyword">from</span> a BertForPreTraining model<span class="token punctuation">)</span><span class="token punctuation">.</span>
<span class="token operator">-</span> This IS NOT expected <span class="token keyword">if</span> you are initializing BertForMaskedLM <span class="token keyword">from</span> the checkpoint of a model that you expect to be exactly identical <span class="token punctuation">(</span>initializing a BertForSequenceClassification model <span class="token keyword">from</span> a BertForSequenceClassification model<span class="token punctuation">)</span><span class="token punctuation">.</span>
<span class="token punctuation">[</span><span class="token string">'carpenter'</span><span class="token punctuation">,</span> <span class="token string">'lawyer'</span><span class="token punctuation">,</span> <span class="token string">'farmer'</span><span class="token punctuation">,</span> <span class="token string">'businessman'</span><span class="token punctuation">,</span> <span class="token string">'doctor'</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token string">'nurse'</span><span class="token punctuation">,</span> <span class="token string">'maid'</span><span class="token punctuation">,</span> <span class="token string">'teacher'</span><span class="token punctuation">,</span> <span class="token string">'waitress'</span><span class="token punctuation">,</span> <span class="token string">'prostitute'</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender â€” and yes, prostitute ended up in the top 5 possibilities the model associates with â€œwomanâ€ and â€œwork.â€ This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (itâ€™s trained on the <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wikipedia">English Wikipedia</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/bookcorpus">BookCorpus</a> datasets).</p>
<p>å¯ä»¥é€šè¿‡ä¾‹å­å‘ç°,å½“éœ€è¦å¡«å†™è¿™ä¸¤å¥è¯ä¸­è¢«å±è”½çš„å•è¯æ—¶,æ¨¡å‹åªç»™å‡ºäº†ä¸€ä¸ªä¸åˆ†æ€§åˆ«çš„ç­”æ¡ˆå¹¶æŒ‰ç…§â€˜manâ€™å’Œâ€˜workâ€™ç›¸å…³è”ï¼Œâ€˜womanâ€™å’Œâ€˜workâ€™ç›¸å…³è”å¯èƒ½æ€§æœ€å¤§çš„å‰5ç§å¯èƒ½æ€§ä¸­ã€‚</p>
<p>When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data wonâ€™t make this intrinsic bias disappear.</p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>In this chapter, you saw how to approach different NLP tasks using the high-level <code>pipeline()</code> function from ğŸ¤— Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser.</p>
<p>We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:</p>
<p>å…³é”®ç‚¹åœ¨äºï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´çš„transformeræ¶æ„ä¹Ÿå¯ä»¥ä½¿ç”¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œå…·ä½“å´å†³äºä½ è¦è§£å†³çš„taskç‰¹ç‚¹ï¼Œä¸‹è¡¨è¿›è¡Œç®€å•æ€»ç»“ï¼š</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Examples</th>
<th>Tasks</th>
</tr>
</thead>
<tbody><tr>
<td>Encoder</td>
<td>ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa</td>
<td>Sentence classification, named entity recognition, extractive question answering</td>
</tr>
<tr>
<td>Decoder</td>
<td>CTRL, GPT, GPT-2, Transformer XL</td>
<td>Text generation</td>
</tr>
<tr>
<td>Encoder-decoder</td>
<td>BART, T5, Marian, mBART</td>
<td>Summarization, translation, generative question answering</td>
</tr>
</tbody></table>
<h4 id="Using-Transformers"><a href="#Using-Transformers" class="headerlink" title="Using Transformers"></a>Using Transformers</h4><ul>
<li><p><strong>Ease of use</strong>: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.</p>
<p>ä¸‹è½½ï¼ŒåŠ è½½å’Œä½¿ç”¨SOTAçš„NLPæ¨¡å‹è¿›è¡Œæ¨ç†å¾ˆç®€å•çš„APIè°ƒç”¨å³å¯</p>
</li>
<li><p><strong>Flexibility</strong>: At their core, all models are simple PyTorch <code>nn.Module</code> or TensorFlow <code>tf.keras.Model</code> classes and can be handled like any other models in their respective machine learning (ML) frameworks.</p>
<p>å…¶å®æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯Pytorch <strong>nn.module</strong>å’ŒTensorflow <strong>tf.keras.model</strong>ç±»ï¼Œå¹¶ä¸”å¯ä»¥åƒå„è‡ªæœºå™¨å­¦ä¹ æ¡†æ¶çš„ä»»ä½•å…¶ä»–æ¨¡å‹ä¸€æ ·è¿›è¡Œå¤„ç†ã€‚</p>
</li>
<li><p><strong>Simplicity</strong>: Hardly any abstractions are made across the library. The â€œAll in one fileâ€ is a core concept: a modelâ€™s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.</p>
</li>
</ul>
<p>This last feature makes ğŸ¤— Transformers quite different from other ML libraries. The models are not built on modules that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.</p>
<p>è¿™äº›æ¨¡å‹ä¸æ˜¯å»ºç«‹åœ¨è·¨æ–‡ä»¶å…±äº«çš„æ¨¡å—ä¸Š.</p>
<p>This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the <code>pipeline()</code> function introduced in <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1">Chapter 1</a>. Next, weâ€™ll discuss the model API: weâ€™ll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions.</p>
<p>æ¥ä¸‹æ¥ä»ä¸€ä¸ªç«¯åˆ°ç«¯çš„ä¾‹å­å¼€å§‹ï¼Œä½¿ç”¨ä¸€ä¸ªæ¨¡å‹å’Œåˆ†è¯å™¨æ¥æ·±å…¥ç†è§£pipelineå‡½æ•°ï¼›ç¨åï¼Œæ·±å…¥è®¨è®ºæ¨¡å‹APIï¼šæ·±å…¥ç ”ç©¶æ¨¡å‹å’Œé…ç½®ç±»ï¼Œå¹¶å±•å¼€å¦‚å¯åŠ è½½æ¨¡å‹ä»¥åŠå¦‚ä½•å¤„ç†æ•°å€¼è¾“å…¥ä»¥ä¾¿è¾“å‡ºé¢„æµ‹ã€‚</p>
<p>Theneâ€™ll look at the tokenizer API, which is the other main component of the <code>pipeline()</code> function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, weâ€™ll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level <code>tokenizer()</code> function.    </p>
<h5 id="Behind-the-pipeline"><a href="#Behind-the-pipeline" class="headerlink" title="Behind the pipeline"></a>Behind the pipeline</h5><p>take a look at the example:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"sentiment-analysis"</span><span class="token punctuation">)</span>
classifier<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        <span class="token string">"I've been waiting for a HuggingFace course my whole life."</span><span class="token punctuation">,</span>
        <span class="token string">"I hate this so much!"</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>and obtained:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token string">'POSITIVE'</span><span class="token punctuation">,</span> <span class="token string">'score'</span><span class="token punctuation">:</span> <span class="token number">0.9598047137260437</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
 <span class="token punctuation">{</span><span class="token string">'label'</span><span class="token punctuation">:</span> <span class="token string">'NEGATIVE'</span><span class="token punctuation">,</span> <span class="token string">'score'</span><span class="token punctuation">:</span> <span class="token number">0.9994558095932007</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>As we can see,this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:</p>
<p><img src="https://s6.jpg.cm/2021/12/23/Lbo6gW.png" alt="Lbo6gW.png"></p>
<h6 id="Preprocessing-with-a-tokenizer"><a href="#Preprocessing-with-a-tokenizer" class="headerlink" title="Preprocessing with a tokenizer"></a>Preprocessing with a tokenizer</h6><p>Like other neural networks, Transformer models canâ€™t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a <em>tokenizer</em>, which will be responsible for:</p>
<ul>
<li>Splitting the input into words, subwords, or symbols (like punctuation) that are called <em>tokens</em></li>
<li>Mapping each token to an integer</li>
<li>Adding additional inputs that may be useful to the model</li>
</ul>
<p>All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the <a target="_blank" rel="noopener" href="https://huggingface.co/models">Model Hub</a>. To do this, we use the <code>AutoTokenizer</code> class and its <code>from_pretrained()</code> method. Using the checkpoint name of our model, it will automatically fetch the data associated with the modelâ€™s tokenizer and cache it (so itâ€™s only downloaded the first time you run the code below).</p>
<p>â€‹    </p>

            </div>
            <hr/>

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone, qq, weibo, douban"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            
            
            
            <div class="reprint1">
                <p>
                    <span class="reprint1-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;Reprint policy:
                    </span>
                    <a href="http://yvr1037.github.io" class="b-link-green">yvr</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/year/11/25/notes-of-huggingface-transformer/" class="b-link-green">notes of huggingface transformer</a>
                </p>
            </div>

        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX',
        clientSecret: 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',
        repo: 'yvr1037.github.io',
        owner: 'yvr1037',
        admin: ["yvr"],
        id: 'year/11/25/notes-of-huggingface-transformer/',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/year/12/16/SA-Study/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/7.jpg" class="responsive-img" alt="SA Study">
                        
                        <span class="card-title">SA Study</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            PerfaceSentiment Analysisæ˜¯nlpé¢†åŸŸä¸€ä¸ªé«˜é˜¶çš„taskä¹‹ä¸€ï¼Œè¿™ä¸ªä»»åŠ¡ç›®æ ‡æ˜¯è®©è®¡ç®—æœºç†è§£äººç±»çš„æƒ…æ„Ÿä¸–ç•Œï¼Œè€Œåœ¨æœºå™¨å­¦ä¹ çš„è®¤çŸ¥æ™ºèƒ½é˜¶æ®µ(ä¸‰ä¸ªé˜¶æ®µï¼šè®¡ç®—æ™ºèƒ½ï¼Œæ„ŸçŸ¥æ™ºèƒ½ï¼Œè®¤çŸ¥æ™ºèƒ½)ã€‚
â€‹    è€Œæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¹Ÿç®—æ˜¯ä¸ªåˆ†ç±»ä»»åŠ¡
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2021-12-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/nlp/" class="post-category" target="_blank">
                                    nlp
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Sentiment-Analysis/" target="_blank">
                        <span class="chip bg-color">Sentiment Analysis</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/year/11/21/Dialogue-system/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/14.jpg" class="responsive-img" alt="Dialogue_system">
                        
                        <span class="card-title">Dialogue_system</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NLPé¢†åŸŸæ¯”è¾ƒä¼ ç»Ÿå’Œæ ¸å¿ƒçš„taskæœ‰å¾ˆå¤š
ä¸‹é¢å…ˆä»‹ç»Chinese NLPçš„åŸºæœ¬ä»»åŠ¡:
Co-reference ResolutionBackground

â€‹    Co-reference identifies pieces of te
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2021-11-21
                            </span>
                        <span class="publish-author">
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: yvr<br />'
            + 'Author: yvr1037<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + 'æœ¬æ–‡ç« è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ï¼Œä»»ä½•å½¢å¼çš„è½¬è½½éƒ½è¯·æ³¨æ˜å‡ºå¤„ã€‚';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


        <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            Copyright&copy; 2021 yvr. 

            <br>
            <span id="sitetime"></span><span class="my-face">áƒ¦ã‚â—¡â•¹)ãƒâ™¡</span>
            <br>
            

            
                    
                        <span id="busuanzi_container_site_pv" style="display: none;"></span>
                        æ€»è®¿é—®é‡: <span id="busuanzi_value_site_pv" class="white-color"></span>
                    
        
                    
                        <span id="busuanzi_container_site_uv" style="display: none;"></span>
                        äººæ¬¡&nbsp; | &nbsp;è®¿å®¢äººæ•°: <span id="busuanzi_value_site_uv" class="white-color"></span> äºº
                    
    
            

            
                &nbsp; | &nbsp;å­—æ•°ç»Ÿè®¡:&nbsp;
                <span class="white-color">44.8k</span> å­—
            

            
            <br>

        </div>

        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/yvr1037" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>





    <a href="http://wpa.qq.com/msgrd?v=3&uin=1213917304&site=qq&menu=yes" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„QQç©ºé—´" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>




</div>

    </div>
</footer>

<div class="progress-bar"></div>

<!-- å¢åŠ å»ºç«™æ—¶é—´ -->
<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();

        /* Date.UTC() -- è¿”å›dateå¯¹è±¡è·ä¸–ç•Œæ ‡å‡†æ—¶é—´(UTC)1970å¹´1æœˆ1æ—¥åˆå¤œä¹‹é—´çš„æ¯«ç§’æ•°(æ—¶é—´æˆ³)
        year - ä½œä¸ºdateå¯¹è±¡çš„å¹´ä»½ï¼Œä¸º4ä½å¹´ä»½å€¼
        month - 0-11ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„æœˆä»½
        day - 1-31ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„å¤©æ•°
        hours - 0(åˆå¤œ24ç‚¹)-23ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„å°æ—¶æ•°
        minutes - 0-59ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„åˆ†é’Ÿæ•°
        seconds - 0-59ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„ç§’æ•°
        microseconds - 0-999ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„æ¯«ç§’æ•° */

        var t1 = Date.UTC(2021, 09, 15, 19, 04, 09); //åŒ—äº¬æ—¶é—´2021-09-15 19:04:09 
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes *
            minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "æœ¬ç«™å·²å‹‰å¼ºè¿è¡Œ " + diffYears + " å¹´ " + diffDays + " å¤© " + diffHours +
            " å°æ—¶ " + diffMinutes + " åˆ†é’Ÿ " + diffSeconds + " ç§’" ;
    } /*éœ€è¦çš„å¯ä»¥å–æ¶ˆ*/
    siteTime();
</script>

<!-- ä¿®æ”¹ä¸è’œå­åˆå§‹åŒ–è®¡æ•°
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);  // 50mså‘¨æœŸæ£€æµ‹å‡½æ•°
        var pvcountOffset = 80000;  // åˆå§‹åŒ–é¦–æ¬¡æ•°æ®
        var uvcountOffset = 20000;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset);
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // åŠ ä¸Šåˆå§‹æ•°æ® 
                clearInterval(int); // åœæ­¢æ£€æµ‹
            }
        }
    });
</script> -->





        <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
        <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


        <script src="https://cdn.bootcss.com/materialize/1.0.0/js/materialize.min.js"></script>
        <script src="https://cdn.bootcss.com/masonry/4.2.2/masonry.pkgd.min.js"></script>
        <script src="https://cdn.bootcss.com/aos/3.0.0-beta.6/aos.js"></script>
        <script src="https://cdn.bootcss.com/scrollprogress/3.0.2/scrollProgress.min.js"></script>
        <script src="https://cdn.bootcss.com/lightgallery/1.6.12/js/lightgallery-all.min.js"></script>
        <script src="/js/matery.js"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->



        

        
            <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        

        <!-- æ´ªå« shw2018 add 2019.08.28 -->
        <script type="text/javascript">
            var OriginTitile = document.title,
                st;
            document.addEventListener("visibilitychange", function () {
                document.hidden ? (document.title = "çœ‹ä¸è§æˆ‘ğŸ™ˆ~çœ‹ä¸è§æˆ‘ğŸ™ˆ~", clearTimeout(st)) : (document.title =
                    "(à¹‘â€¢Ì€ã…‚â€¢Ì) âœ§è¢«å‘ç°äº†ï½", st = setTimeout(function () {
                        document.title = OriginTitile
                    }, 3e3))
            })
        </script>

        <!-- é¼ æ ‡ç‚¹å‡»çƒŸèŠ±çˆ†ç‚¸æ•ˆæœ  æ´ªå« shw2018 add 2019.09.09 -->
        
            <canvas class="fireworks" style="position: fixed; left: 0; top: 0; z-index: 1; pointer-events: none;"></canvas>
            <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
            <script type="text/javascript" src="/js/fireworks.js"></script>
        

        <!-- èƒŒæ™¯é›ªèŠ±é£˜è½ç‰¹æ•ˆæ´ªå« shw2018 add 2019.09.10 -->
        
            <script type="text/javascript">
            //åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>');
            }
            </script>
        

        <!-- é¼ æ ‡ç‚¹å‡»æ–‡å­—ç‰¹æ•ˆ æ´ªå« shw2018 add 2019.09.10-->
        
            <script src="/js/wenzi.js" type="text/javascript"></script>
        

        <!-- èƒŒæ™¯é›ªèŠ±é£˜è½ç‰¹æ•ˆ æ´ªå« shw2018 add 2019.09.10 -->
        
            <script type="text/javascript">
                var windowWidth = $(window).width();
                if (windowWidth > 768) {
                    document.write('<script type="text/javascript" src="/js/xuehuapiaoluo.js"><\/script>');
                }
            </script>
        

        <!-- åœ¨çº¿èŠå¤©å·¥å…·  æ´ªå« shw2018 add 2019.09.11 -->
        
            <script src="//code.tidio.co/xxxxxxxxxxxxxxxxxxxxxxxxxxx.js"></script>
            <!--  åœ¨çº¿èŠå¤©ä½ç½®è‡ªå®šä¹‰  æ´ªå« shw2018 add 2019.09.13  -->
            <script> 
                $(document).ready(function () {

                    setInterval(change_Tidio, 50);  
                    function change_Tidio() { 

                        var tidio=$("#tidio-chat iframe");
                        if(tidio.css("display")=="block"&& $(window).width()>977 ){
                            document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" &&$(window).width()>977)>0? "-40px" : ($("div.toc-title").length&&$(window).width()>977)>0?"80px":"20px";   
                            document.getElementById("tidio-chat-iframe").style.right="-15px";   
                            document.getElementById("tidio-chat-iframe").style.height=parseInt(tidio.css("height"))>=520?"520px":tidio.css("height");
                            document.getElementById("tidio-chat-iframe").style.zIndex="997";
                        } 
                        else if(tidio.css("display")=="block"&&$(window).width()>601 &&$(window).width()<992 ){
                            document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && 601< $(window).width()<992)>0? "-40px":"20px" ;   
                            document.getElementById("tidio-chat-iframe").style.right="-15px"; 
                            document.getElementById("tidio-chat-iframe").style.zIndex="997";
                        }
                        else if(tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))<230){
                            document.getElementById("tidio-chat-iframe").style.bottom= ($("div#backTop.top-scroll").css("display")=="none" && $(window).width()<601)>0? "-10px":"45px" ;   
                            document.getElementById("tidio-chat-iframe").style.zIndex="997";
                        }

                        if( tidio.css("display")=="block"&&$(window).width()<601 && parseInt(tidio.css("height"))>=230){
                            document.getElementById("tidio-chat-iframe").style.zIndex="998";
                        }
                    } 
                }); 
            </script>
        

        <!-- èƒŒæ™¯ canvas-nest  æ´ªå« shw 2018  add 2019.09.15-->
        
            <script type="text/javascript">
            var windowWidth = $(window).width();
            if (windowWidth > 992) {
                document.write('<script type="text/javascript" color="0,0,255" pointColor="0,0,255" opacity= "0.8" zIndex="--1" count="150"src="/libs/background/canvas-nest.js"><\/script>');
            }
            </script>
        

        <!-- èƒŒæ™¯é™æ­¢å½©å¸¦  æ´ªå« shw 2018  add 2019.09.15-->
        

        <!-- èƒŒæ™¯åŠ¨æ€å½©å¸¦ æ´ªå« shw 2018  add 2019.09.15-->
        
            <script type="text/javascript">
            var windowWidth = $(window).width();
            if (windowWidth > 992) {
                document.write('<script type="text/javascript" src="/libs/background/ribbon-dynamic.js"><\/script>');
            }
            </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
        

    </body>
</html>